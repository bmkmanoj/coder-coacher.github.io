<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MIMIC - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="MIMIC - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>MIMIC - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HNXejaW3LiU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">here's what I'd like to do you pointed
out some issues that you were concerned
about and I thought that maybe you could
go and look into it a bit more and you
did and so why don't I turn things over
to you so that you can tell us what you
found out okay well thank you Michael
hi again hi thank you Michael
so I did go and I started trying to deal
with these issues so just to to recap a
little bit there were two problems that
I had more or less and they were that we
had all these cool little randomized
optimization algorithms and most of them
seemed to share this property that the
only thing that really happened over
time is you started out with some point
and you ended up with some point which
was supposed to be you know the optimal
point and the only difference between
the first point and the last point that
you did or the one millionth pointer
over how many iterations you have is
that that point might have been closer
to the optimum by some measure and very
little structure was actually being kept
around or communicated only the point
was being communicated now you could
argue that that isn't quite true with
genetic algorithms but really you move
from a single point to just a few points
the other problem that I had is that we
had all of this sort of probability
theory that was underneath what we were
doing all this randomization but somehow
it wasn't at all clear in most of the
cases exactly what probability
distribution we were dealing with now
one thing I really liked about similarly
the Mealing is that you were very clear
about what the probability distribution
was so what I decided to do is to go out
there in the world and see if I could
find maybe a class of algorithms that
took care of these two points for us and
I found something you'll be very happy
to hear Michael yeah I would love to
find out what it is it turns out that I
wrote a paper about this almost 20 years
ago how did you find that I just said
well if I wanted to start looking
someplace I should look at home first
and I stumbled across this paper that I
read I CSV so learning about machine
learning really begins at home that's
exactly right so I had to reread the
paper because you know it is a couple of
decades old and I will point out that a
lot of other work has been done since
this that refines on these ideas but
this is fairly straightforward and
simple so I think I'm just going to
stick with this work and the papers are
available for everyone listening to this
right now we're watching this right now
so you can read it in all of its gory
details but I just want to go over the
the high-level bit here because I really
think it kind of gets at this idea so in
particular the paper that I'm talking
about introduced an algorithm called
mimic which actually stands for
something though I forget what and it
really had a very simple structure to it
the basic idea was to directly model a
probability distribution probability
distribution of what well I'll tell you
and you know like I said Michael I will
define exactly what this probability
distribution is for you for a second and
hopefully you'll heal by that it seems
like a reasonable distribution to model
and given that you have this probability
distribution that you're directly
modeling and the goal is to do this sort
of search through space just like we did
with all the rest of these algorithms
and to successfully refine the estimate
of that distribution and the idea is
that if you can directly model this
distribution and refine it over time
that that will in fact convey structure
structure in particular of what we're
learning about the search space while
we're doing the search exactly and not
just simply the structure of the search
space but the structure of the parts of
the space that represent good points or
points that are more optimal than others
yeah that seems like a really useful
thing so I'm just gonna give you again
it's the simple mimic algorithm that it
sort of captures these basic ideas
because I think it's fairly simple I
mean easy to understand while still
getting some of the underlying issues
but do keep in mind that there's been
literally decades of work since then and
the optimization space where people
really taking these kinds of ideas and
refine them to sort of be much more
mathematically precise but this I think
does get the idea across and I happen to
understand it so I thought that I would
share it with you compare yeah that
sounds really exciting
excellent</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>