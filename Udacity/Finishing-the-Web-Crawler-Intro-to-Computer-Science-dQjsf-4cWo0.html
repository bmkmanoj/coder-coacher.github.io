<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Finishing the Web Crawler - Intro to Computer Science | Coder Coacher - Coaching Coders</title><meta content="Finishing the Web Crawler - Intro to Computer Science - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Finishing the Web Crawler - Intro to Computer Science</b></h2><h5 class="post__date">2012-05-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dQjsf-4cWo0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so let's remember the code we had at the
end of unit 2 for crawling the web so we
used two variables we initialized to
crawl to the seed a list containing just
the seed and we're going to use to crawl
to keep track of the pages to crawl we
initialize crawl to the empty list and
we're keeping track of the pages we
found using crawled then we had a loop
that would continue as long as there
were pages left to crawl we'd pop the
last page off the G crawl list if it's
not already crawled then we'll Union
into the crawl all the links that we can
find on that page and then we'll add
that page to the list of pages we've
already crawled so now we want to figure
out how to change this so instead of
just finding all the URLs we're building
up our index we're looking at the actual
content of the pages and we're adding it
to our index so the first change to make
we're updating the index and we're going
to change the return result so instead
of returning crawled what we want to
return at the end is the index if we
wanted to keep track of all the URLs
crawled we could still return crawled
and return both crawled and indexed but
let's keep things simple and just return
index that's what we really want for
being able to respond to search queries
so now we have one other change to make
and this is the important one we need to
find a way to update the index to
reflect all the words that are found on
the page that we've just crawled I'm
going to make one change before we do
that since both get all links and what
we need to do to add the words to the
index depend on the page let's introduce
a new variable and store the content of
the page in that variable this will save
us from having to call get page twice
get page is fairly expensive it requires
a web request to get the content of the
page makes a lot more sense to store
that in a variable and that'll simplify
this code so now we just need to pass in
content so we have one missing statement
and I'll leave it to you to see if you
can figure out what statement we need
there to finish the web crawler when
it's done the result of crawl web what
we return is index should be an index of
all the content we find starting from
the seat</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>