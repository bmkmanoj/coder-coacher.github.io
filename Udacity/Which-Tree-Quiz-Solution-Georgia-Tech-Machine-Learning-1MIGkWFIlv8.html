<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Which Tree Quiz Solution - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Which Tree Quiz Solution - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Which Tree Quiz Solution - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1MIGkWFIlv8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Michael which of these two decision
trees is smaller the one on the right is
smaller that's exactly right
because it's it's it's easier to
represent it in sort of almost any
obvious way that you can think of it has
fewer nodes so smaller decision trees
trees with fewer nodes less depth
whatever you need to make it smaller
have smaller length then bigger decision
trees so that means that if all we cared
about was the second term here we would
prefer smaller decision trees over
bigger decision trees which we do which
we do now what about this over here the
what does it mean so I think this is
pretty straightforward you got this
right there's the link that well I mean
I guess what's weird is that you're you
kind of moving back and forth between a
notion of a prior which is where the P
of H came from and a notion of well you
know if we're gonna actually have to
describe the hypothesis you have to
write it down in some way and this gives
you a way of measuring how long it takes
to write it down but I guess what this
whole derivation is doing is linking
those two concepts so that you can think
about our bias for shorter decision
trees as actually being the prior or any
actually being the thing that says the
smaller ones are more likely and
vice-versa that when we think about
things that are priors that are
assigning high probability of certain
things it's kind of like giving them a
shorter description right so in fact if
you were to take this example literally
here that we prefer smaller trees to
bigger trees this is kind of a Bayesian
argument for Occam's razor
and pruning and pruning well you often
use razors to prune so it makes perfect
sense ah okay so this is kind of
straightforward right that basically
smaller trees are smaller than bigger
trees it sort of makes sense mm-hmm now
what about this over here what does it
mean to talk about the length of the
data given a particular hypothesis I
could think of one interpretation there
so like if the hypothesis generates the
data really well then you don't really
need the data at all right you just have
you already have the hypothesis the data
is free right but if it deviates a lot
from the hypothesis then you're gonna
have to have a long description of where
the deviations are so maybe it's kind of
capturing this sort of notion of how
well it fits right that's exactly right
so I like that explanation so let me
write it down so here we literally just
mean something like size
of age but over here we're talking about
well sort of error right if the hop out
it's just exactly what you said if the
hypothesis perfectly describes the data
then you don't need any the data but
let's imagine that the hypothesis gets
all of the data labels wrong then when
you send the hypothesis over to this
person this sort of person we're making
up who's trying to understand the data
and hypothesis then you're also gonna
have to send over what all the correct
answers were so what this really is is a
notion of miss classification error or
just error in general if we're thinking
about regression so basically what we're
saying is if we're trying to find the
maximum posterior hypothesis we want to
maximize this expression we wouldn't
find the H that maximizes this
expression that's the same as finding
the H that maximizes the log of that
expression which gives you this which is
the same as minimizing this expression
which is just maximizing this expression
throwing a minus 1 in front but these
terms actually have meanings in
information theory so really the best
hypothesis the hypothesis with the
maximum a posterior probability is the
one that minimizes error and the size of
your hypothesis so you want the simplest
hypothesis that minimizes your error oh
that's pretty much literally Occam's
razor yes now what is important here in
reality is that these things are often
traded off with one another if I give a
more complicated or bigger hypothesis I
can typically drive down my error or I
can have a little bit of error for a
smaller hypothesis but this is a sort of
fundamental trade-off here you want to
find the simplest hypothesis that still
explains your data that is minimizes
your error so this actually has a name
and that is the minimum description link
and they've been many algorithms over
the years that have tried to do this
directly by simply trading off some
notion of error and some notion of size
and finding the trade-off between them
that actually works now if you think
about it for a little while Michael you
realize that yeah this sort of makes
sense that the hand-wavy level at which
I just talked about it but you do have
some real issues here about for example
units so I don't know if the units of
the
size of hypotheses are directly
comparable to the you know counts of
errors or sum of squared errors or
something like that and so you have to
come up with some way of translating
between them and some way of making a
decision whether you would rather
minimize this we would rather minimize
that if you were forced to make a
decision but the basic idea is still the
same here that the best hypothesis is
the one that minimizes error without
paying too much of a price for the
complexity of hypothesis
so I've been sitting here thinking about
so with decision trees this notion of
length feels like you could translate it
directly into bits right like if you
actually had to write it down and
transmit it that makes a lot of sense
but then I was thinking about neural
networks and given a fixed neural
network architecture it's always the
same number of weights and they're just
numbers so you just transmit those
numbers so I thought hmm this isn't
really helping us understand neural nets
at all and then it occurred to me that
those weights if they get really big
you're gonna need more bits to express
those big weights and in fact that's
exactly when we get overfitting with
neural nets if we let the weights get
too big so like this gives a really nice
story for understanding neural nets as
well right so the complexity is not in
the number of parameters directly but
what you need to represent the value of
the parameters Wow
so I could have ten parameters that are
all binary in which case I need ten bits
or they could be arbitrary real numbers
in which case I might need well an
arbitrary number of bits that's really
weird yeah but the point here Michael I
want to wrap this up the point here is
we've now used Bayesian learning to
derive a bunch of different things that
we've actually been using all along and
so again the beauty of Bayesian learning
is that it gives you a sort of handle on
why you might be making some of the
decisions that you're making it seems
like this raises the theory question
that you threw at me in a previous unit
right which is like well so if it
doesn't really tell us anything that we
didn't already know how important is it
well in this case I think it's important
because it told us something that we
were thinking and tells us that in fact
we were right so now we can comfortably
go out in the world minimizing some a
squared error when we're in a world
where there's some kind of Gaussian
transmission noise we can go about
trying to believe Occam's razor because
Bayes told us so thanks to Shannon and
so on and so forth we can do these
things and know that in some sense
they're the right thing to do at least
innovation since new okay good now one
more thing Michael I'm going to show you
which is that everything I've told you
so far is a lie</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>