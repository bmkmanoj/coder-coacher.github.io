<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Kernel - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Kernel - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Kernel - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ov_znn3KHfE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so I'll clean up the screen a
little bit Michael to make this a little
bit clearer now let's look at this X I
transpose XJ and I'm now going to
replace it with something so I've just
replaced it with a function which I'm
going to call a kernel which takes X I
and XJ as parameters and we'll return
some number now again as we've talked
about before we think of the X I
transpose XJ as some notion of
similarity and so this kernel function
is our representation still of
similarity another way of thinking about
that by the way is that this is the
mechanism by which we inject domain
knowledge into the support vector
machine learning algorithm just like we
were injecting domain knowledge when we
were thinking about K nears neighbors
yes everything has domain knowledge and
everything ultimately comes back to K
nearest neighbors I don't know why and I
don't know how but it always seems to so
the K and K nearest neighbors and the K
and kernel really stand for knowledge
well that's pretty good we should write
a paper with that title so the real
neatness here the Nina's here is twofold
one is you can create these kernels and
these kernels can have arbitrary
relationships to one another so what
you're really doing is projecting into
some higher dimensional space where in
that higher dimensional space your
points are in fact linearly separable
but the second bit is because you're
using this kernel function to represent
your domain knowledge you don't actually
have to do the computation of
transforming the points into this higher
dimensional space
I mean fact if you think about it with
the last kernel that we used
computationally there was actually no
more work to be done before we were
doing X transpose Y and now we're still
doing X transpose Y except we're then
squaring it so that's just a constant
bit more work right so and that is a
kernel and another kernel is X transpose
Y yes that's something I would call a
cannon now the other kernel we talked
about was just X transpose Y by itself
that's that's a kernel too is now oh no
no that's right that's right that's
absolutely right so that's a different
kernel you're absolutely right just X
transpose Y is another kernel actually
we can write a general form of both of
these and it's a very typical kernel
it's the polynomial kernel where you
have X transpose Y plus some constant
let's call it C raised to some power P
and as you can see both of those earlier
kernels are in fact just a special case
of this hmm
and I would yeah okay good and that
should look familiar it reminds me of
the regression lecture exactly what
we're doing polynomial regression so now
rather than doing polynomial regression
the way we're thinking about it before
we use a polynomial kernel and that will
allow us to represent polynomial
functions and there lots of other
kernels you could come up with Michael
so here's just a couple I will just sort
of leave them leave them up to you to
think about and and there's there's tons
of them so here's one that I happen to
like so that's a sort of radial basis
kernel does that look familiar to you
well let me make sure I understand that
it's doing the right thing so if x and y
are really close to each other then it's
like e to the minus 0 over something
which is like e to the 0 which is like 1
so their similarity is like 1 if they're
on top of each other if they're very far
apart then it's like their distance is
something very big divided by something
e to the minus something very big is
very close to 0 so it does have that
kind of property kind of like the
sigmoid where it transitions between 0
and 1 but it's not exactly the same
shape as that right in fact it's
symmetric that the square of the the
distance between you and making an
actual distance makes it always a
positive value there or at least a
non-negative value there and so it
becomes symmetric so it looks a lot more
like a like a Gaussian okay with some
kind of width which is represented by
Sigma and there tons and tons of these
actually if you wanted to get something
to look like a sigmoid here's one where
alpha is different from the other alphas
but I couldn't think of a different
Greek letter and this function gives you
something that looks a lot more like a
sigmoid and there are tons and tons of
these you can come up with and there's
lots of a lot of research over the years
and what makes a good kernel function
the most important thing here I think is
that it really captures your domain
knowledge it really captures your notion
of similarity
you might notice Michael that since it's
just an arbitrary function that returns
a number it means that x and y or the
the different data points you have don't
have to actually be points in a
numerical space they could be discrete
variables they could describe whether
you're male or female as long as you
have some notion of similarity to play
around with that you can define that
returns a number then it doesn't matter
it'll always work so can you do things
like I don't know strings or graphs or
images absolutely you could think about
two strings our two strings similar
maybe they're they're similar if they're
edit distance is small the number of
transformations that you have to give in
order to transform one string to another
if there are a few of those then they're
very similar there are a lot of those
then they're very dissimilar you could
talk about words like cat and lion and
decide those are more similar than cat
and mosquito for example because they
because of the ears
yeah mainly because of the ears all
right at the night then I think I
understand okay good so you might be
curious Michael whether there are any
bad kernel functions there is actually
an answer to that while it's not clear
whether there are any bad kernel
functions it is the case that in order
for all the math to go through there is
a specific technical requirement of a
kernel function it has a name and it's
the Mercer condition have you ever heard
of the Mercer condition I've heard the
word I actually used to live near Mercer
County New Jersey you did yeah oh so
then I guess it's the condition of
living near where Michael used to live
now so the emergent condition is a very
technical thing well we'll talk about
this again a little bit of homework
assignment but for your intuition in the
meantime it basically means it acts like
a distance or it acts like a similarity
it's not an arbitrary thing that doesn't
relate the various points together being
positive semi-definite in in this
context means it's a well behaved
distance function gotcha</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>