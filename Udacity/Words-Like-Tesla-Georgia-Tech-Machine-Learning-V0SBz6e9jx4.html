<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Words Like Tesla - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Words Like Tesla - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Words Like Tesla - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/V0SBz6e9jx4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so what we're going to get out of
feature transformation is the ability to
combine these features these words
somehow into some new space where
hopefully we will do a better job of
eliminating our false positives and
false negatives by combining words
together so we're going to leave this
example for now but just to give you an
intuition about how a proper kind of
feature transformation might help you
just let me point out that if i type in
a word like car you would expect since i
know what car kind of means let's say
automobiles in this case i should pick
up documents or score documents higher
if they don't contain the word car but
they do contain the word automobile or
perhaps they contain a word like motor
or racetrack or tesla rang that somehow
words like tesla an automobile and
motorway and anything else you can think
of that has to do with cars probably are
highly correlated or highly related to
cars in some way and so it makes sense
to combine words like automobiles and
car together into new features to pick
up documents that are somehow related
together that way does that make sense
so that's the intuition I want you to
think of for the three algorithms that
we're going to go over next okay yeah I
could definitely see how Sonata me is
going to be a win when we index the
documents if we include well if we've
met them down to a lower dimensional
feature space where one feature is used
for anything sort of car related I'm not
sure how that's going to help with
polysemy but I definitely see how this
feature transformation idea could be a
win with Sonata me the way it's oh
that's right and I think it's much
easier to see how it works with sadhana
me and then with polysemy the way it
will it could in principle help you with
polysemy is that it will combine a bunch
of features that together eliminate the
ambiguity of any particular word so as
you type in more words together it'll
start to pick those things up while also
eliminating Sonata me so we'll see the
way it's going to work in practice
actually is that if you think of that
then we're talking about unsupervised
learning but if you think of this as a
supervised learning problem then you can
imagine how you could ask yourself how
to combine sets of these features these
words together such that they still give
you correct labels and if you can solve
that problem you will end up solving
both polysemy epson on me or at least
minimizing their impact
cool okay so let's go from this specific
example to far more abstract examples
and talk about three specific algorithm</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>