<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Optimizing Weights | Coder Coacher - Coaching Coders</title><meta content="Optimizing Weights - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Optimizing Weights</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mE2N3na1VzI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so one of the things that goes wrong
when you try to actually run gradient
descent on a complex network with a lot
of data is that you can't get stuck in
these local minima and then you start to
wonder boy is there some other way that
I can optimize these weights I'm trying
to find a set of weights for the neural
network that well that what that that
tries to minimize error on the training
set and so gradient descent is one way
to do it and it can get stuck but
there's other kinds of advanced
optimization methods that become very
appropriate here and in fact there's a
lot of people in machine learning who
think of optimization and learning is
kind of being the same thing that what
you're really trying to do in any kind
of learning problem is solve this this
high order very difficult optimization
problem to figure out what the the
learned representation needs to be so I
just want to mention in passing some
various kinds of advanced methods that
that people have brought to bear there's
things like using momentum terms in the
gradient which basically where the idea
in in momentum is as we're doing
gradient descent so imagine this is our
error surface we don't want to get stuck
in this little Bowl here we want to kind
of pass all the way through it to get to
this bowl so maybe we need to just you
know continue in the direction we've
been going so instead of you know think
of it as a kind of a physical analogy
instead of just just going to the bottom
of this hill and getting stuck it can
kind of bounce out and pop over and come
to what might be a lower minima
later there's a lot of work in using
higher-order derivatives to better
optimize things instead of just thinking
about the way that individual weights
change the error function to look at
combinations of weights Hamiltonians and
whatnot there's various ideas from
randomized optimization which we're
going to get to in a sister course that
can be applied to to make things more
robust and sometimes it's worth thinking
you know what we don't really want to
just minimize the error on the training
set we may actually want to have some
kind of penalty for using using a
structure that's too complex I mean it's
just when did we when do we see
something like this before Charles when
we were doing regression and we were
talking about overfitting so right
that's right he came up in regression
but something similar will also happen
in the decision tree section sure we had
a we had a issue with decision trees
where if we had we let the tree grow too
much to explain every little quirk in
the
you'd over fit and so we came up with a
lot of ways of dealing with that like
pruning not going too far deeply into
the tree you can either do that by
filling out the tree and then backing up
so you only have a little bit of small
error or by stopping once you've read
some sort of threshold as you as you
grow the tree out that's really the same
as giving some kind of penalty for
complexity yes exactly right so
complexity in the tree setting has to do
with the size of the tree in regression
it had to do with the order of the
polynomial what do you suppose it would
mean in the neural net setting and and
how would you predict what negative
attributes it might have so what's
what's a more or less complex network
well there's two things you can do with
networks right you can add more and more
nodes and you can add more and more
layers good so right so if we the more
nodes that we put into network the more
complicated the mapping becomes from
input to output the more local minimum
we get the more we get they have the
ability to actually model the noise
which brings up exactly the same
overfitting issues it turns out there's
another one that's actually really
interesting in the neural net setting
which I think didn't occur to people in
the early days but it became clear and
clear over time which is that you can
also have a complex network just because
the numbers the weights are very large
so same number of weights same number of
nodes same number of layers but larger
numbers often leads to more complex
Network and the possibility of
overfitting and so sometimes we want to
penalize a network not just by giving it
fewer nodes or layers but also by
keeping the numbers in a reasonable
range that that make sense makes perfect
sense</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>