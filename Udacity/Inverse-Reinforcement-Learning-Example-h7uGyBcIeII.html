<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Inverse Reinforcement Learning Example | Coder Coacher - Coaching Coders</title><meta content="Inverse Reinforcement Learning Example - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Inverse Reinforcement Learning Example</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/h7uGyBcIeII" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we're in a grid world okay and you have
to learn a reward function you have to
learn what's good or not good to do in
this grid and you're gonna get to learn
it by watching me so I'm gonna I'm gonna
I'm gonna control the agent the agent
his name is Frank okay he's gonna go
like this
and then he stays there okay okay so I
claim that you can actually make some
very good educated guesses about how
rewards work in this environment just
based on that one little demonstration I
think that's probably true so is there
anything that the agent finds positively
rewarding Green Green yes good is there
anything that the agent finds aversive
so I could guess that blue is aversive
because you avoided the blue but that
also happened to be one of the shortest
path to get to the green yeah that's
true that's true so it might have just
been a coincidence but I think you have
some evidence that maybe blue was
something I was trying to avoid right
way what about red so red is not
something you were trying to avoid
because you could have avoided it that's
right I don't know about orange because
you couldn't avoid orange to get to
Green excellent and that was exactly the
inferences that I'm hoping you could
make so again you can learn a tremendous
amount about what's valuable and what's
not valuable by not just the path that
was taken but in a sense by the paths
that were not taken and and and what the
other options are or were and what the
other options weren't right so you
couldn't learn anything about orange it
could be the hates orange not as much he
likes green so Frank was willing to go
through orange and it didn't matter
where but red yeah totally could have
gone around red and didn't bother right
so that you can avoid blue and red just
by going like that but but he didn't do
that so he probably indifferent to red
or something so so yeah so it's actually
a really cool idea and we can actually
learn things about what people want to
do and what they don't want to do how
how tolerant are they of like sudden
stops in a car or a bumpy road that sort
of thing how do you trade that off
against how fast you're trying to get to
your destination you can it's hard to
get people to tell you numbers for that
but it's actually not so hard to have
them demonstrate and then infer these
kinds of values from their behavior so
you know obviously as we're sitting here
we can't really get into the details of
the algorithm but just to give you a
kind of a hint of the form of this
there's there's an algorithm that I
worked on called maximum likelihood
inverse reinforcement learning but
there's a whole bunch of different
algorithms this is this one I happen to
understand the best cuz I helped make it
up
and the basic game is here guess a
reward function in this case we were
guessing reward functions that mapped
colors to values we then compute an
optimal policy with that reward function
in mind measure the probability of the
data the behavioral trajectories that we
saw given that policy and then compute a
gradient on how how can the value of the
rewards change in a way that would make
that the data more likely and then we
just go back and do that again until we
find something that is a local maximum
no that was that working practice it
seems to work pretty well it can be slow
and it can get stuck in local minima but
we haven't really had significant
problems with that it actually pretty
consistently comes up with a reward
function that's pretty solid
there's tricky things like keeping in
mind the fact that when I showed you the
trajectory I went through green and then
I stayed there so how do we represent
that is this an absorbing State and if
so do I just include it once in my
trajectory in which case it just saw one
little green in which case it's not so
excited about green or is it really
important and it kind of sat there
forever in which case it really didn't
see much of anything else
so getting getting those terminal states
right can be a bit of a trick that make
sense okay I like that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>