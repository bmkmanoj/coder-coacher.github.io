<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>27-1_l_Remax Analysis Part Three | Coder Coacher - Coaching Coders</title><meta content="27-1_l_Remax Analysis Part Three - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>27-1_l_Remax Analysis Part Three</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4pJ2IvkbDhs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so what I'm claiming in step 2 here is
that if we end up choosing a policy that
is not based on knowing all the
transitions like there's still some
unknown transitions out there but we end
up taking a policy that puts us into a
loop where we never visit any of those
then that still has to be an optimal
loop we're not making any mistakes right
it could be that we could do as well by
uncovering those other values but we
can't do any better because we are
assuming in step 2 that anything that we
don't know has the largest possible
value and so if it has the largest
possible value and we still don't want
to go there then we're doing at least as
well as optimal yeah that's kind of cute
all right so but now we need step 3 so
now step three says okay we we solved
the MDP this sort of imagined mdp that
has these optimistic estimates in it and
let's say that that actually takes us to
a state action pair that we haven't been
to before we learn its value how many
mistakes might we make enroute to that
so when we actually get to that state
and Traverse an edge that we didn't know
about that could have been a mistake
right so that's definitely you know one
and how many mistakes might we make you
know seeking out this state that
actually turned out to be really bad
state from a particular state I guess
the number of states yes right so the
number of transitions that we might take
just to find out that we were wrong
could be as big as n right number n
minus 1 or something like that but yeah
yeah so so enroute to ax to us this
isn't a deterministic MVP onward to any
state our path is going to be either
bounded by N or if it's bigger than end
its infinity because we're actually
looping without actually getting to that
other place where so the most it can be
is in right alright so given that we
might make any mistakes to find out
something new to learn something new to
visit a state action pair that we've
never seen before how many times can
that happen how many times can it be
that there's a policy that brings us to
a state action pair that we haven't been
to before um how many times can that be
yeah so so each time this sort of step 3
maybe I should write it in english all
right so with this step three is that is
it
is that what our our max algorithm tells
us to do is to execute a path that
actually brings us to an unknown state
action pair then we go traverse that
edge find out what it really is and you
know generate a new policy so the number
of steps that it might take to get to
that new state action pair is N and the
number of times that we might discover
new state action pair is what in tons of
our reactions n times the number of
actions because that's the total number
of unknown things in the graph to begin
with right right so we're exploring the
MDP and each time we visit state and
take an action we haven't taken before
that's going to reduce one from the
count so the largest that count is is n
times K and so the total number of
mistakes that this algorithm can make is
bounded by N squared kick hey that's
polynomial in n K it is and it also
doesn't depend on epsilon and it also
doesn't depend on Delta that's even
better so wait if it doesn't depend on
epsilon and it doesn't depend on Delta
then it means it's polynomial in those
things sure yes it's a very simple
polynomial those things so then we're
polynomial and everything yeah yeah so
this is an efficient exploration
algorithm for deterministic mvps right
all it's saying is whenever we don't
know something assume it's awesome
behave accordingly if we learn something
new make note of it and so the number of
times that we take a step that might not
be the best thing to do from that state
can't be any bigger than N squared K
that's beautiful yeah it's pretty cool
this this kind of analysis comes from a
paper that spin Koenig wrote with Reid
Simmons where they actually analyze
q-learning in this kind of setting and
they got a worse bound like n cubed
times K but this algorithm is sort of
pretty simple and and gives us a nice
bound and it sort of does what you'd
wanted to do hmm that's cool I will
point out that in cube k is also
polynomial yes yes but it's a slightly
worse polynomial you're absolutely right
i mean he showed that q-learning has a
polynomial bound in these kinds of
environments if you set up the rewards
correctly and if you set up the initial
value functions correctly all right so
that gets us through stochastic MDPs
that have no state and state MDPs that
have no stochastic and the last thing we
want to dive into is what happens if we
do something like our max except for on
stochastic MDPs well we still have
another case we have to worry about the
case where we have no state and we have
no stochastic ah so okay
well let's do that one really fast all
right so you're talking about an
environment that has one state and all
the actions are deterministic they give
you a deterministic reward and return
you deck to that state right so this is
essentially deterministic bandit problem
right so how many mistakes might meet
well like what would be an algorithm for
this and how many mistakes would it make
well the dumb is one would be or the
simplest one would just be take every
action once and then you know everything
right and each of those so k minus one
of those or mistakes possibly and then
yeah when we know that we just choosing
the optimal action amp thereafter yeah
and you could do a little bit better if
you assume that everything was our max
because as soon as you come across our
max you know you can stop there yet yes
right if you end up finding along the
way an action that actually gives you
the full value the full our max value
then we don't have to explore the rest
of them because they're not going to be
any better yeah okay so that gives us a
it doesn't give us a better bound but it
does probably give us better performance
if we keep track of that so yes so this
this is a silly case well but but it
does allow us to do say we solve three
of the four great all right so let's
dive into number four</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>