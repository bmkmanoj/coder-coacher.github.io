<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Policies Two - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Policies Two - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Policies Two - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sZevPd2IkM8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so now that we've got utility fine and
we've got this pi-star defined we can
actually do an even better job of
writing out pi-star
and let me do that all right so does
this equation make sense Michael let's
see so the policy is that a star
Jenner's that it can it's a star so it's
the optimal policy all right
the optimal action to take at a state is
well look over all the actions and sum
up over all the next states the
transition probability right so that's
like the probability that we end up in
state s Prime
and now we have the utility of s Prime
the problem being that that's not
defined well it sort of is we've defined
it immediately above it leaves with
respect to some policy yeah but that's
concerning because we don't know I mean
the policy that you want to put in there
is got to be the policy that you're
trying to find right so in fact
implicitly what I mean here is a PI star
so yeah
in fact let me write that down that
whenever you see me right from now on
the utility of a state I'm almost always
going to actually mean the utility of
the state if I follow the optimal policy
we might call this the true utility of a
state I see so I'm just going to write
this off to the side here it's something
for you to remember
so this says then that the optimal
policy is the one that for every state
returns the action that maximizes my
expected utility with regard to the
optimal policy it feels rather circular
it is rather circular but you your your
computation less you're a big fan of
recursion we just went to a whole
exercise where we figured out the
geometric series by effectively doing
recursion it's a similar kind of
situation for this it kind of is so let
me write one more equation down and then
you'll be one step closer to actually
seeing it of course if we have an
infinite horizon with a discounted state
even though you're one step closer you
won't actually be any closer
well let's worry about that when we get
there so let me write one more one more
equation down we're never going to get
there it's infinitely long yeah this
wait you demonstrating something with
this lesson by making it infinitely long
we keep I'm certainly demonstrating
something with this lesson I don't know
what it is so let me write this next
equation yeah so then the true utility
of a state yes you
then I'm just basically going to unroll
the equation for utility it's the reward
that I get for being in that state plus
I'm now going to discount all of the
reward that I'm going to get from that
point on got it
all right so once we go to our new state
s Prime we're going to look at the
utility of that state okay that's sort
of fine you know modulo the recursion
we're going to look at over overall
actions which action gives us the
highest value of that oh I see that's
kind of like the pi star expression just
above yep then alright so once we figure
that out we know what action we're going
to take in in state s Prime we're going
to discount that because why because I
guess that just kind of ups the the
gamma factor on all the rewards in the
future right and then we're going to add
to that our immediate reward yes right
so okay I think I follow yeah so in some
sense all I've done is I've kept
substituting pieces back into one
another so the true utility being in the
state is the reward you get in that
state plus the discount of all the
rewards you're going to get at that
point which of course is defined as the
utility you're going to get for the
states that you see but each one of
those is defined similarly and so the
utility you'll get for s double prime se
will also be further discounted but
since it's multiplied by gamma that'll
be gamma squared and then as triple
prime will be gamma cube and so that's
basically just unrolling this notion of
utility up here okay so now it seems
like all the pieces are in one place
right and so it would be nice if we were
done and I'm going to say that we're not
just one step closer but you can see an
oncoming light and it is not an oncoming
train okay so yeah this seems like a
really important equation it is in fact
it's so important it's got a name you
want to guess what the name is bill
that's actually very close its bellman
its bellman equation bellman equation s
square this equation was invented by a
guy named bellman and it turns out to be
in some sense the key equation for
solving MVPs and reinforcement learning
Wow and it's actually even more so it
looks but basically this is the
fundamental recursive equation that
defines the true value of being in some
particular state and it accounts for
everything that we care about in the MD
the utilities themselves deal with the
policy that we want to have the gammas
our discount and all the rewards are
here the transition matrix is here and
the actions where all the actors are
going to take so basically the whole MVP
is referenced inside of here and allows
us by determining utilities to always
know what's the best action to take well
it's the one that's going to maximize
the utility so if we can figure out the
answer to this equation the utilities of
all the states we perforce know what the
optimal policy is it becomes very easy
so we've sort of taken all that neat
stuff about MVPs and stuck it in a
single equation bellman was a very smart
guy so it was he's the same bellman from
the curse of dimensionality yes cool
there can be only one bellman actually
are there any more bellman I don't think
so I think that they retired is like
retiring a jersey they retired his name
I could have sworn that I saw one at the
last hotel that I went to is probably
the same one oh I get it
Hotel bellman I was really good there we
go okay good well so now that we've
we've killed that as much as we could
let's see if we can actually solve this
equation which since this is clearly a
key equation since it has a name okay
yeah that would be cool especially
because it looks like if you could solve
this you could solve it right because
then you have you you could just plug
the U in and get the U app right and
once you have the UN and you get the you
out then you got the policy right for
you it's always been for you it's for us
Michael it's for us</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>