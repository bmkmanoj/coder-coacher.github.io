<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Configuring the Kernel Launch Parameters Part 1 - Intro to Parallel Programming | Coder Coacher - Coaching Coders</title><meta content="Configuring the Kernel Launch Parameters Part 1 - Intro to Parallel Programming - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Configuring the Kernel Launch Parameters Part 1 - Intro to Parallel Programming</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9ZhuzpitHgM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let me come back to one piece of this
code that needs a little more
explanation and that's this kernel call
here's the name of the kernel call
square and we call it with these launch
parameters with these arguments there's
a lot going on in this call so I need to
explain details that we didn't need in
our example that are necessary as we
move forward what I told you is that we
were launching one block of 64 threads
and that's absolutely true but let me
explain what's happening under the hood
and a little more detail when you launch
a kernel you specify both the number of
blocks and the number of threads per
block in our example we only had one
block of 64 threads
we'll run a run bigger problems than
this what you need to know about the
hardware right now is two things one it
is capable of running many blocks at the
same time - each block has a maximum
number of threads that it can support
newer GPUs can support 1024 threads per
block older GPUs can only support 512 so
when you have lots of work to do you'll
divide that work into any number of
blocks each of which has no more than
512 or possibly a thousand 24 threads so
if we wanted to launch 128 threads and
square the values in each of them
instead of 64 threads we could change
this call to square of 1 comma 128 if we
wanted to launch 1280 threads instead we
could call square of 10 comma 128
launching 10 blocks of 128 threads each
or square v comma 256 we're launching 5
blocks of 256 threads each but we can't
call square of 1 comma 1280 because
that's too many threads per block you
should pick the breakdown of threads and
blocks that makes the most sense for
your problem as we saw before each
thread that we launched knows its index
within the block and you won't be
surprised to hear that each thread also
knows the index of its block as well
we'll see how we access this information
in a moment now how do these kernels
actually map into threads and blocks
well when we Square 10 comma 128 we're
going to launch 10 thread blocks of 128
threads each when we square v comma 256
we'll launch five consecutive thread
blocks of 256 threads each these
diagrams
1-dimensional they only progress in one
dimension the X dimension that's fine if
your problem is one dimensional but many
problems are two or three dimensional
you'll be doing image processing in the
homeworks for instance and that's very
definitely a two dimensional problem so
it makes sense that CUDA would natively
support not only one-dimensional layouts
of blocks and threads like we're showing
here but also two and three dimensional
as well for instance perhaps we'd like
to process this 128 by 128 pixel image
we'd like to launch one thread per pixel
we might choose for instance to launch
these 128 by 128 threads as 128 blocks
in the Y dimension where each one of
those blocks is a 128 by one block of
threads in the X dimension or we might
instead choose to what to launch an 8x8
grid of blocks where each block is 16
threads by 16 threads so CUDA supports
one two or three dimensional thread
blocks we can also arrange thread blocks
into one two or three dimensional grids
so now let's return to how we launch
kernels we put two parameters and the
triple Chevron's the first is the
dimensionality of the grid of blocks and
the second is the dimensionality of the
threads within a block we can specify up
to three dimensions for each but if we
don't specify a dimension
it defaults to one more generally you
can specify a three dimensional
configuration for grid of blocks or
block of threads with this dim three
struct which you initialize is dim three
X comma Y comma Z and recall that again
if we don't specify Y or Z they default
to one so when we say dim three of W
that it means the same thing as dem
three of W comma 1 comma 1 and you can
actually abbreviate this with simply the
integer W so when you specified square
of 1 comma 64 that was equivalent to
square of dim 3 1 comma 1 comma 1 dim
360 4 comma 1 comma 1</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>