<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning A Cappella - Overfitting Thriller! | Coder Coacher - Coaching Coders</title><meta content="Machine Learning A Cappella - Overfitting Thriller! - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning A Cappella - Overfitting Thriller!</b></h2><h5 class="post__date">2014-05-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DQWI1kvmwRg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">[MUSIC]
You've got some data, and
need to find a rule that can predict.
You'll want a theta that works for
any instance that you pick.
It's Bellman's curse.
The classifier needs to fit the labels.
Yeah.
You start a search, but
training error that you
minimize won't generalize!
&amp;gt;&amp;gt; You overfit it!
Spit it out.
Mistakes you shouldn't make,
evaluate the testing set.
You overfit it!
Quit it now.
You need another way or
you'll regret it, no doubt.
[MUSIC]
You see the error,
hypotheses are overly complex.
You feel the terror, and
wish your loss had only been convex.
You fit the noise, and
find it's not a standard deviation!
Yeah.
And so destroys the chance for
you to epsilon exhaust,
all hope is lost,
you overfit it, spit it out,
you won't survive the cut
that you recieved
from Occam's razor, Fit it, quit it now,
you just misclassified your
last example, sample, not now.
Model selection -- a labeled
collection that's just too sparse.
Bound by the tension of
VC dimension too wide,
it's the end of the line!
Cross validation could help you
find the function that you seek.
Profound frustration, you failed
because your learner was too weak.
You just regressed,
a cubic to a curve that was a constant.
Oh, and now you're stressed,
attacked by your support vector machine.
[MUSIC]
Just use the mean.
You overfit it, spit it out.
You try to prune the tree, but
find your set of points is shattered.
Fit it, quit it now.
It's time to PAC it in,
'cause you've been tested,
bested, guessed it, messed it all about!
You overfit it, spit it out.
You try to prune the tree, but
find your set of points is shattered,
Fit it, quit it now.
It's time to PAC it in,
'cause you've been
tested, messed it!
&amp;gt;&amp;gt; Boosting, bagging, weight decay.
A gaping margin saves your day.
But what a chilling plot this is.
To find the right hypothesis.
You let your guard down one small bit,
oh no it seems you've underfit.
[MUSIC]
Like scary films in cinema,
you're trapped in local minima.
[MUSIC]
Without a strong and helpful prior,
you're doomed to chase a lone outlier.
The noise runs through your neural net.
[MUSIC]
Bewildering your data set.
And if you try to leave one out,
the folds you must be splitting.
Your algorithm can't
resist the plague of
overfitting [LAUGH]</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>