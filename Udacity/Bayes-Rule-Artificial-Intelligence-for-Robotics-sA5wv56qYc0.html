<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bayes' Rule - Artificial Intelligence for Robotics | Coder Coacher - Coaching Coders</title><meta content="Bayes' Rule - Artificial Intelligence for Robotics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bayes' Rule - Artificial Intelligence for Robotics</b></h2><h5 class="post__date">2012-02-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sA5wv56qYc0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let's look into measurements and they
will lead to something called Bayes rule
you might have heard about Bayes rule
before it's the most fundamental
consideration in probabilistic inference
but the basic rule is really really
simple suppose X is my grid cell and Z
is my measurement then the measurement
update seeks to calculate a belief over
my location after seeing the measurement
how is this computed well it was really
easy to compute in our localization
example and now I'm going to make it a
little bit more formal it turns out
Bayes rule looks like this and then we
will likely be a little bit confusing
but what it does is it takes my prior
distribution P of X and multiplies in
the chances of seeing a red or green
tile for every possible location and
outcomes if you just look at the
denominator here the non-normalized
posterior distribution we had before
recognize this this was our prior this
is our measurement probability if we do
this for all the grid cells we put a
little index I over here then just the
product of the prior of the grid cell
times the measurement probability which
was large if the measurement
corresponding to the correct color and
small if you correspond to a false color
that product gave us the non-normalized
posterior distribution for the grid cell
you remember this because it's what
you're programmed you programmed a
product between the prior probability
distribution and a number the
normalization is now the constant over
here P of Z technically that is the
probability of seeing a measurement
devoid of any location information but
let's not confuse ourselves the easiest
way to understand what's going on is to
realize that this is a function here
that assigns to each grid cell a number
and the P of Z doesn't have the grid
cell as an index so no matter which have
you consider the P of Z is the same and
here's the trick no matter what P of Z
is because the final posterior has to be
distribution by normalizing these
non-normalized products over here we
will exactly calculate P of Z for
differently P of Z is the sum over all I
of just this product over here so this
makes Bayes rule really simple it's a
product of our prior distribution with a
measurement probability which we know to
be large if the color is correct and
small otherwise we do this and assign it
to a so called non-normalized
probability which I'll do with a little
bar over the P and then I compute the
normalizer which I call alpha it's the
sum of all these guys over here and then
I just normalize so my resulting
probability will be 1 over alpha of the
non-normalized probability this is
exactly what we did and this is exactly
baseball</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>