<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Neural Network Sketch | Coder Coacher - Coaching Coders</title><meta content="Neural Network Sketch - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Neural Network Sketch</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/w9OFiOaTFs8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so we're now in a great
position to talk about what the network
part of the neural network is about so
now the idea is that we can construct
using exactly these kinds of sigmoid
units a chain of relationships between
the input layer which are the different
components of X with the output Y and
the way this is going to happen is
there's other layers of units in between
that each one is computing the weighted
sum sigmoid of the layer before it these
other layers of units are often referred
to as hidden layers because you can kind
of see the inputs you can see the
outputs this other stuff is less
constrained or indirectly constrained
and what's happening is that each of
these units it's a it's running exactly
that kind of you know take the weights
multiplied by the things coming into it
put it through the sigmoid and that's
your activation that's your output so so
what's cool about this is in the case
where all these are sigmoid units this
mapping from input to output is
differentiable in terms of the weights
and by saying the whole thing is
differentiable what I'm what what I'm
saying is that we can figure out for any
given weight in the network how moving
it up or down a little bit is going to
change the mapping from inputs to
outputs so we can move all those weights
in the direction of producing something
more like the output that we want even
though there's all these sort of crazy
nonlinearities in between and so this
leads to an idea called back propagation
which is really just at its heart a
computationally beneficial organization
of the chain rule we're just computing
the derivatives with respect to all the
different weights in the network all in
one convenient way that has this this
lovely interpretation of having
information flowing from the inputs to
the outputs and then error information
flowing back from the outputs towards
the inputs and that tells you how to
compute all the derivatives and then
therefore how to make all the weight
updates to make the network produce
something more like what you wanted to
produce so this is where learning is
actually taking place and it's really
neat you know this back propagation is
referring to the fact that the errors
are flowing backwards sometimes it's
even called error back propagation nice
so here's a question for you Michael
what happens if I replace the sigmoid
units with some other function and let's
say that function is also different well
if it's differentiable then we can still
do this this basic kind of trick that
says we can compute derivatives and
therefore we can move weights around to
try to get the network to produce what
we want it to produce that's a big win
this is still act like a perceptron well
even this doesn't act exactly like a
perceptron right so it's really just
analogous to a perceptron because we're
not really doing the hard threshold
engaging some finite time in fact the
error function can have many local
optima and what what we mean by that is
this idea that we're trying to get the
we're trying to set the weight so that
the error is low but you can get to
these situations where none of the
weights can really change without making
the error worse and you'd like to think
well good then we're done we've made the
error as low as we can make it but in
fact it could actually just be stuck in
a local optimum that there's a much
better way of setting the weights it's
just we have to change more than just
one weight at a time to get there so
that makes sense so if we think about
the Sigma the error front the sigmoid in
the error function that we picked right
the error function was sum of squared
errors so that looks like a parabola in
some high dimensional space but once we
start combining them with others like
this over and over and over again then
we have an error space where there might
be lots of places that look low but only
look low if you're standing there but
globally
would not be the lowest point right
exactly right and so you can get these
situations in just the one unit version
where the error function as you said is
this nice little parabola and you can
move down the gradient and when you get
down to the bottom you're done but now
when we start throwing these networks of
units together we can get an error
surface that just in its cartoon form
looks crazy like this that there's it's
smooth but there's these places where it
goes down it comes up again goes down
maybe further it comes up again doesn't
come down as far and you could easily
get yourself stuck at at a point like
this where you're not at the global
minimum you're at some local optimum</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>