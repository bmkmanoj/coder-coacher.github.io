<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>VC of Finite H - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="VC of Finite H - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>VC of Finite H - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QoO3g3oTiMM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so we can actually work out what the VC
dimension of a finite H is and in fact
it's easier to just think about it in
terms of an upper bound so let's let's
imagine that the VC dimension of H is
some number D and the thing to realize
from that is that that implies that
there has to be at least to the D
distinct concept why is that it's
because each of the two to the D
different labelings is going to be
captured by a distinct hypothesis in the
class because if we can't use the same
hypothesis to get two different
labelings so that means that the that
two to the D is going to be less than or
equal to the number of hypotheses it
could be that there's more but there
can't be any fewer otherwise we wouldn't
be able to get things shattered so just
you know simple manipulation here gives
us that D is less than or equal to the
log base two of H so there is this
logarithmic relationship between the
size of a finite hypothesis class and
the VC dimension of it and again and
that's what we were seeing in the other
direction as well that the the log of
the hypothesize of the hypothesis space
was kind of playing the role of the VC
dimension in in the bound okay that
makes sense and and from that it's
pretty easy to see how 13 got in there
yes it should be pretty much obvious to
even the most casual observer of 13 yes
I think that's right so I don't think
there's any reason for us to explain it
yeah I think one would have to really go
back and look at the at the proof to get
the details of why if it has the form
that it has but or at least the details
of the form that the overall structure
of the form I think we understand it's
just that the details come out of the
proof and we're not gonna go through the
proof and I think that's probably best
for everyone
so what we're seeing at the moment is
that a finite hypothesis class or a
finite VC dimension give us finite
bounds and therefore make things pack
learn about what's kind of amazing
though is that there's a general theorem
that says in general if H H is pack
learn about if and only if the VC
dimension is finite so that means that
we know that anything that has finite VC
dimension is learn about from the
previous bound but we're saying that
it's actually the other way as well that
if something is learner belit has finite
VC dimension or to say it another way if
it has infinite VC dimension you can't
learn it VC dimension captures
in one quantity the notion of pac
learnability which is which is really
beautiful
yeah I agree that be in that see guy
they're pretty smart</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>