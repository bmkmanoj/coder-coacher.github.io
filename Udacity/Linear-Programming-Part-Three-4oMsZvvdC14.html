<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Linear Programming Part Three | Coder Coacher - Coaching Coders</title><meta content="Linear Programming Part Three - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Linear Programming Part Three</b></h2><h5 class="post__date">2015-06-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4oMsZvvdC14" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so the process of producing the dual of
a linear program is just a mechanical
process it there's there's a series of
steps that you go through where each
constraint from the primal becomes a
variable in the duel and each variable
in the primal becomes a constraint in
the duel and certain Max's become mins
and and you know bounds become objective
function things so what I've done here
is I've actually gone through that
process you know which we could go
through in detail but let's just pretend
that we did and and we create a new
linear program from the old linear
program and the new one is called the
dual so let's take a look at this duel
in detail because it turns out that the
variables and the constraints actually
have some very nice interpretations that
are that are worth thinking about here
in the duel the thing that we're trying
to optimize is we're trying to maximize
the sum over all state action pairs of
the reward for that state action pair
times this value qsa little qsa so
little qsa probably not a great name for
it because it's not the same as a Q
value but it's something it's something
that we're multiplying by the rewards
and we're trying to maximize it so one
way i like to think about qsa is as
policy flow sort of how much agent pneus
is flowing through each state action
pair if it follows the policy it's going
to spend some time running around in the
environment passing through each state
action pair and each time it passes
through a state action pair we're going
to get the reward associated with that
state action pair and what we want to do
is maximize that reward so from this
sort of policy flow concept it sort of
makes sense what it is we're trying to
maximize its in some ways even more
intuitive than in the primal which we
were saying we're trying to minimize the
value which of course we're not really
trying to minimize the value we're just
trying to minimize it so that it doesn't
end up being an upper bound on the value
here we're actually trying to maximize
expected reward which seems like a
really good thing right right okay so
this is now going to be subject to the
following idea that for each state in
this case we're going to it's easier to
think of them as next States for each
possible next date we want it to be true
that the amount of policy flow through
that next state summed up over all the
actions that are going through
it that should be equal to in some sense
the number of times that next state is
visited which we can get by summing over
all states we might have started at and
inactions we might have taken from there
the policy flow through that state
action pair times the transition
probability that would send us to s
prime as a result of that and we're
going to also include in this equation
sort of the sense that we can also start
at a given state action pair so we're
just going to stick a one-in-four that
so there's some policy flow that we're
injecting into each state action pair of
the system and we're going to add to
that any policy flow that would be
coming through it through other parts of
the system so it's kind of like we're
dumping policy flow all over our mdp and
then letting the MDP kind of pump around
what's left and it's discounted there's
a little discount here so there's
actually you know evaporation or
something happening all over the place
and what we want to know is what's the
way of letting the flow go through I'm
kind of deciding at each state which
action it should flow through so that we
ultimately get the maximum possible
reward and I think just one additional
constraint here just to make sure that
the policy flow is non-negative so that
we can't introduce a whole lot of policy
flow in one place and then drain it out
some other place to make things balance
right you can think of these constraints
is actually being a kind of conservation
of flow right so the amount of stuff
going into s prime it has to equal the
amount of stuff coming out of it and
that's all this is saying right it's
just saying that that has to balance it
has to be a meaningful flow subject to
that constraint maximize reward so again
this is kind of a neat interpretation of
what it means to solve a markov decision
process but it just comes mechanically
out of the primal version of of setting
up this as a linear program ok does that
suggest an algorithm that's different
from the sort of algorithms we've been
using so far hmm interesting thought so
well for one thing it is an algorithm
right in the sense that once we've set
things up this way we can hit it over
the head with a linear programming stick
and a solution will come out sure and in
fact it'll come out in polynomial time
which is nice but but you're right i
mean in some sense if you think about
value iteration is something that's
propagating these values around this is
this has a different form this is really
concentrating more on the policy aspect
of the MDP and and maybe we could make a
an algorithm that mordor
rectly kind of searched in policy space
for the best behavior for the MDP let's
do that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>