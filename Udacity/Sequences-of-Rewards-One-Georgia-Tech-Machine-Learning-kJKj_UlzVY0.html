<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sequences of Rewards One - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Sequences of Rewards One - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sequences of Rewards One - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kJKj_UlzVY0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so having gone through that
exercise Michael I think it's it's
worthwhile to step back a little bit and
think about the assumptions that we've
been making that have been mostly
unspoken and I'm gonna say that the main
assumption that we've been making in
some sense boils down to a single word
and that word is stationarity so let me
tell you what I mean by that and why by
kind of illustrating what it is we've
been sort of doing for a little while
okay
sure okay so the first thing I'm gonna
say is that we've actually been kind of
assuming infinite horizons so what I
mean by that when when we think about
the last grid world that we were playing
with we basically said well you know I
want to avoid going to the end as
quickly as possible if I have rewards of
a certain value or whatever because you
know the game doesn't end until I get to
an absorbing state well that sort of
implies that you basically can live
forever that you have an infinite time
horizon to work with now can you can you
imagine why if you didn't have an
infinite time horizon to work with you
might end up doing something very
different it's different than what what
we were doing in the grid world right so
here let me let me show you the game
that we were we're the grid world are we
doing before might not be to think about
it so here's a grid what we had before
and as you'll recall we had a particular
policy that sort of made sense here I'll
write it out for you again and this was
with a case where we had a reward of
minus 0.04 remember we just did this
remember yep okay and this was the
policy that turned out to be optimal and
in the future I want you to pay
attention to here is that when you're
over right here near possible in state
rather than going up it made sense to
take the long way around because you
were going to get some negative reward
but it's a small enough negative reward
compared to where you might end up okay
with a positive one yeah I see uh-huh
and that makes some sense well that only
makes sense if you're going to be living
long enough that you can afford to take
the long route around what if I told you
you only had say three time steps left
and then the game is going to end no
matter where you end up well it might be
it might make more sense to take some
risk and to just try to take the short
way
because otherwise there's really no
chance you're gonna get to the plus one
I'm not entirely convinced that though
because there's still a chance that
you'd fall into the minus one along the
way right so the exact value the whether
it makes sense to take the risk of not
it's going to depend upon two things
we've already talked about one of them
which is the actual reward that you get
if this reward were you know negative
enough then clearly it makes sense to
just try to end things quickly right we
just show that in the last quiz but the
other thing is going to depend upon is
how much time you have in order to get
to where you're going if you've only got
one or two time steps before everything
is going to end you can imagine that
there are cases where without changing
the reward too much it makes a lot of
sense to try to go ahead and quickly get
to this plus one even though you have
some chance of falling into the minus
one as opposed to trying to move away
where you're then kind of get all but
guaranteed that you're never going to
reach the plus one so whether it makes
sense to take the risk or not will
depend upon the reward but it's also
going to depend upon whether you have an
infinite amount of time to get to where
you want to get to or whether you have a
finite amount of time and the real major
thing that I want you to get out of that
is that if you don't have an infinite
horizon but you have a finite horizon
then two things happen one is the policy
might change because things might end
but secondly and more importantly the
policy will change or can change even
though you're in the same state so if I
told you if you're in this state right
here and I told you you didn't have an
infinite amount of time but you still
had one hundred million time steps then
I think it's clear that it still makes
sense to go the long way around right
yeah I mean that's the probability that
this policy is going to last for a
million time steps has got to be tiny
right so I might as well it's a hundred
million times steps matters will be
infinity but if I make that number not a
hundred million but I make it two or
three or four then suddenly your
calculus might change and in particular
your calculus will change even though
I'm in the same state right so maybe
this state right here if I've got a
million one hundred million time steps I
still want to go the long way around but
if I've only got a few time steps the
only way I'm going to ever have a chance
of getting positive reward is to go this
way does that make sense
I guess so so you're saying for example
even within a single run it could be
that I'm in a state and I try an action
and maybe it doesn't work and I stay
where I am and I tried again and maybe
it doesn't work and I stay where I am
it might then switch to a different
action not because the other one wasn't
working but because it's now running out
of time
right exactly so we talked about this
notion of a policy which maps States to
actions and we talked about this notion
of stationarity so you believe that in
mark this sort of Markovian thing said
well it doesn't matter where I've been
only matters where I am and so if I'm in
this state since it only matters where I
am I'm always going to want to take the
same action well that's only true in
this kind of infinite horizon case if
you're in a finite horizon case and that
finite horizon of course is going to
keep counting down every time you take a
step
well then suddenly depending upon the
time step that's left you might take a
different action so we could write that
I think just for the sake of kind of
seeing it as something like your policy
as a function of both the state you're
in and the time step you're in and that
might lead you to a different set of
actions so this is important this is
important I mean we are not for the for
this course gonna talk about this case
at all where you're in the finite
horizon but I think it's important to
understand that the without this
infinite horizon assumption here you
lose this notion of stationarity in your
policies okay yeah interesting okay so
that I think is you know maybe not
something that's obvious but becomes
obvious after someone points it out to
you so the second thing that I want to
talk about I think is a little bit more
subtle and and that's this notion of
utility of sequences so as we've been
talking Michael we have been sort of
implicitly discussing not just the
rewards we get in a single state but the
rewards that we get through sequences of
states that we take and so I just want
to point out a little fact that that
comes from that and where that ends up
leading us and then we'll get to some
nice little cute series of math so
here's what I want to point out what
utility is what we mean by utility of
sequences it means we have some function
I'm going to call you for utility over
the state of the sequence re of states
and that we're going to see let's call
them as 0 as 1 is 2 and so on and so
forth well I think an assumption that
we've been making even if we haven't
been
very explicit about it is that if we had
two sequences of states s 0 s 1 s 2 dot
dot dot and a different sequence s 0
then s 1 prime in its 2 prime that is
two sequences that might differ from s 1
on but all start in the same start state
ok if we have a utility for the first
and that utility happens to be greater
than the utility for the second then it
also turns out that we believe that the
utility for s 1 s 2 dot dot dot is
greater than the utility for s 1 prime s
2 prime dot dot dot all right so these
are two different sequences s 1 with the
esses and the S Prime's are two
different sequences yes and in the
beginning we're comparing them with with
s 0 stuck in front of both of them yes
and we're saying if I'd prefer the S 0
followed by the all the esses to s 0
followed by the S Prime's then I have
that same preference even with those s
zeroes missing right and so this is
called stationarity of preferences and
another way of saying it is that if I
prefer one sequence of states today over
another sequence of states then I prefer
that sequence of states over the same
sequence of states tomorrow so isn't
this just obvious because the whatever
the rewards are for those two cases
we're just adding the reward that we get
for s 0 so it's gonna be the same ah
but listen what you just said you just
said well it'll be the same because all
we're doing is adding the reward for s 0
but what did we ever say about adding up
rewards I thought I thought that's what
we were doing that's right that is what
we were doing but we never actually sat
down and wrote that down and said this
is what it means to talk about the
utility of a sequence of states as
opposed to the reward that you get in
one state
ok so you're saying that if we if we are
adding rewards then this follows right
ok and then I'm actually been saying
something even stronger which is I will
show you on the next slide which is if
you believe that this is true that the
utility of one sequence of states is
greater than the utility of another
sequence of states both today and
tomorrow then it actually forces you to
do some variation of what you said which
is just adding sequences of states we're
adding the
Ward's of the sequence of states that we
see oh that's really interesting so that
so that adding isn't really an arbitrary
thing it follows from this this deeper
assumption right and you know the reason
I bring this up is because it would make
sense if you were just to grab someone
off the street and start talking about
Markov decision processes one of two
things would happen either they'd run
screaming from you like you're a crazy
person
or they would sit and they would listen
and if they listened they would just
completely buy into the idea that you
just add up sequences of reward you know
sequences of rewards that you see as a
way of talking about how good the states
are because that's a very natural thing
to do but it turns out that
mathematically that if you have this
notion of sort of stationarity of
preferences and this sort of infinite
horizon world you really are in a case
where this has to be true and it has to
be the case that you have to do some
form of addition because nothing else
sort of can be guaranteed to maintain
this property over stationary
preferences I mean as you said if I got
one sequence of states and another
sequence of states and by just pre
pending or appending another set of
states to it I'm still going to always
guarantee that one is greater than the
other you kind of have to do some form
of adding the reward that you see in
those states in both cases if you don't
do that then eventually this inequality
will not hold so let me write that down
in math terms and see where it gets us
okay cool</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>