<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Max Depth Solution - Intro to Computer Science | Coder Coacher - Coaching Coders</title><meta content="Max Depth Solution - Intro to Computer Science - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Max Depth Solution - Intro to Computer Science</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TRNyIIrB73Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">for this question you were asked to
limit the depth the crawler searches too
so in order to understand this question
and what we need to do I'm going to draw
a graph of the links so I'm going to
start off with a page seed page a and
from a I have links to another page B
and another page C now be links to D and
C links to e now just make things a
little tricky e is going to link to D
and finally D links to F what do we
actually need to do well we need to keep
track of what pages we have left to
crawl just like before we need to keep
track of the pages we've already crawled
because once we've crawled them we don't
really want to have to crawl them again
and I'm going to introduce a new list
where we keep track of the pages at the
next step and I'm just going to call
that next depth so we start off it to
crawl and we have just the seed page now
we haven't crawled anything yet and we
don't know what there is to come either
so we start off with just to crawl with
the seed page in it now from there we
look at a and we see that there are two
pages that a links to so we're at a and
the next step the next page is at depth
one are B and C and we're actually going
to move a from to crawl to crawl because
we've actually looked at it now we've
seen that the next pages are C and B now
those are going to be the ones that we
crawl next so I'm going to put those
into two crawled put to crawl sorry and
we'll look at those next so if we look
at B B has a neighbor D
and then C has a neighbor a we do this
in a couple of steps but just to get the
idea of how the codes going to work I'm
just going to go through it looking at
what's at each depth so B and C we've
now crawled so those two both go in two
crawled so now the ones at the next step
our DNA so those are the ones we want to
look at so I'm going to move DNA over
here and then we're going to look at the
neighbors the the next page is sorry of
D and D so D has a neighbor F and E has
a neighbor D and those are at depth 3
now we might not want to add D in here
because actually we've already looked at
D but we will see when we get to that
your code exactly how we're going to
handle that now
we take D and E and put them into
crawled and then we've got F and D over
here and we can carry on depending on
what depth we want to go to so that's
the idea behind how the codes going to
work we're going to start off with a
page we're going to crawl it and
everything else at the same depth and
then add those pages to next step and
switch the code switch the lists around
which hopefully we'll be able to see
more clearly in the code this is the
supplied code for crawl web and we're
going to make a few changes to this to
take account of the depth so what do we
do first what we're going to need
another list next step which will keep
track of the next level of links and
when we start we have a depth of zero
we're going to carry on through the
while loop while there's something into
crawl because if there's nothing to
crawl then we can't carry on and also
while our debt is at most the maximum
depth that supplied as an input to the
function
just as before we're going to pop from
to crawl and we're not we're not going
to make any changes to this line either
but instead of adding to crawl because
we don't want to muddled up the pages
we're searching through now and the
pages at the next level we're going to
actually add the new links to next depth
instead and then just as before we're
going to add our page that we've crawled
to the crud list so that we know we've
already looked through it and we're not
going to look through it again now then
that last bit of code that we need to
add when we finish going through all the
lists of to give and level just like I
showed you in the example we're then
going to change our to crawl list and
our depth list over so to crawl is now
empty next step contains all the new
links and I'm going to use a multiple
assignment where I swap to crawl a next
step so to crawl becomes equal to next
step and next step is going to be empty
again so that we can start the while
loop again with a clean list and then
depth we're going to increase by one and
then we're going to return crawled so
just to quickly recap we carry on
through the while loop as long as that
we've got some page to crawl and our
depth is less than or equal to max step
we take a page and if we haven't crawled
the page before we add all the links
from the page to the next step and then
we take our page and we put it in the
crawled list we keep doing this if we
haven't got any pages left to crawl then
we switch them around but if we have got
pages left to crawl we just carry on
with the while loop until two crawl is
empty and then we make once - crawl is
empty that means that that level is
finished that depth is finished and we
to go on to start the next step and
that's it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>