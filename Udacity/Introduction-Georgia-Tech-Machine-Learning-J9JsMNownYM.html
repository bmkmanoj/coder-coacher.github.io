<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Introduction - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Introduction - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Introduction - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/J9JsMNownYM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi again Michael hey how's it going it's
going pretty well we are ready to do our
last lesson of this mini course on
unsupervised learning and randomized
optimization cool what's it about
it is about feature transformation as
opposed to feature selection cool so
they can change from vehicles into robot
toys to them yeah typically usually it's
from robots into cars but that's a
technical detail ok ok so I'm going to
define feature transformation for you or
at least I'm gonna do my best to it
turns out that it's a slightly more
slippery definition than the one that we
used for feature selection but has a lot
of things in common
ok yep and you tell me if this makes
sense ok so feature transformation as
opposed to feature selection which we
talked about last time is the problem of
doing some kind of pre-processing on a
set of features in order to create a new
set of features now typically we expect
that new set of feature to be smaller or
in some way more compact ok but while we
create this new set of features we want
to explicitly retain as much information
as possible and when I say retain as
much information as possible
I probably mean I think we'll see as we
continue this conversation information
that is relevant and hopefully useful
now the first thing you might ask is
what's the difference between this and
feature selection is that a question you
might ask Michael I was thinking about
that though I think you kind of told me
at the beginning of the feature
selection lecture and well I told you
that I was gonna tell you I'm not sure I
actually told you hmm
so one thing you might say Michael is
that if I looked at this definition this
seems to actually be consistent with
feature selection as well I take a set
of features in feature selection and I
create a new feature set which happens
to be smaller and my goal was to retain
as much information as possible and we
talked about the difference between
relevant features and useful features
but really this sort of describes
feature selection as well you see that
yeah definitely
ok now the difference is in fact
probably the right way to think about
this is that feature selection is in
fact a subset of feature transformation
where the pre-processing you're doing is
literally taking a subset of the feet
here feature transformation can be more
powerful and can be an arbitrary
pre-processing not just something that
goes from a set to to a subset but what
we're going to do is we're going to
restrict our notion of feature
transformation to what's called linear
feature transformation so let me define
that for you explicitly so as before
with the feature subset we said that we
were taking a bunch of features or a
bunch of instances that were in sum in
dimensional feature space and
transforming it into another feature
space of size M and in the feature
selection problem M was meant to be less
than n hopefully much less than n and
that's typically the case in feature
transformation though as we'll discuss
towards the very end it doesn't have to
be but when I say usually we expect em
to be less than usually means almost
always okay and that's because of the
curse of dimensionality problem but the
difference between what we were doing
with feature selection and feature
transformation because this is exactly
what I wrote before is that this
transformation operator is usually
something like this a linear
transformation operator so the goal here
is to find some matrix P such that I can
project my examples my points in my
instance space into this new sub space
typically smaller than the original
subspace and I'll get some set of new
features which are combinations in a
particular linear combinations of the
old features does that make sense I
think so so then that P matrix would
want to be n by M yes so the transpose
would be M by N and that multiplies by
the N dimensional feature space in X and
projects it out to an M dimensional
feature space right ok yeah good good so
if we wanted to write that out we could
say that feature selection was about
taking features like X 1 X 2 X 3 and X 4
and finding a subset like X 1 and X 2
and that would be feature selection but
feature transformation would be taking
something like X 1 X 2 X 3 and X 4 and
translating into something like 2 X 1
plus X 2
which creates a single new feature which
is a linear combination of a subset of
them does that make sense
yeah but it could actually make other
feature combinations as well right right
just project it down to one right it
could be projected down to two or it
could be projected down on the three or
get even project it down to a different
four okay and in principle you could
imagine that you could even project up
into other dimensions which is something
that we've done before conceptually
anyway when we talked about kernels yeah
although those were typically nonlinear
transformations implicit in the notion
was doing a nonlinear feature
transformation but we did it even before
we learned about kernels the very second
thing I think we did perceptrons had a
perceptrons go into a higher dimensional
space well when we talked about XOR
right what we effectively did is we
showed that we could project the
original two dimensional space into what
looks like a three dimensional space
where the third dimension that was a
combination of the first two and then
you could actually do it with a linear
separator but that wasn't a linear
transformation that C was a non linear
transformation right because we're
talking about boolean variables yeah but
in the end of the day it was still a
kind of feature transformation and in
this case a feature transformation where
we went to a higher number of features
and today what we're going to be talking
about is linear translinear
transformations as opposed to nonlinear
transformations and we're going to be
focusing specifically on the case where
we're trying to reduce the number of
dimensions so the implicit assumption
here right the kind of domain knowledge
that we're bringing to here or the
belief that we're bringing here is that
we don't need all n features we need a
much smaller set of features that'll
still capture all the original
information and therefore help us to
overcome the curse of dimensionality
it's just that that smaller subset might
require bringing in information across
the various features okay yeah all right</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>