<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Maximum Parallelism Is Not Always Best For Performance - Intro to Parallel Programming | Coder Coacher - Coaching Coders</title><meta content="Maximum Parallelism Is Not Always Best For Performance - Intro to Parallel Programming - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Maximum Parallelism Is Not Always Best For Performance - Intro to Parallel Programming</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-I-Y-0wAYaE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I want to make a point here that it's a
bit of a ninja topic but it turns out
that exploiting every last bit of
parallelism isn't always the very best
performing code sometimes it helps to do
more work per thread and this is this
leads to an advanced optimization
technique called granularity coarsening
that we'll talk about later that said
the first problem is almost always to
find enough parallelism so keeping that
in mind are we done
is this 0.7 milliseconds the fastest
that we can transpose this matrix on
this GPU well let's reason that out so
two things can limit your performance on
any code time spent fetching and storing
data from and to memory or time spent
performing compute operations on that
data now the transpose code has almost
no computation at all it's entirely
about moving data around so let's ignore
compute for the moment and focus on
memory and the question is are we moving
that data around efficiently how can we
tell there's a handy utility called
device query that's included in the CUDA
SDK let's run it device query spits out
an enormous amount of information most
of which you don't really need to know
right now but buried in here are a few
things that I want to point out the GPU
clock rate is how fast the actual
processors and the GPU are going the
memory clock rate shows you how fast the
memory and the GPU is operating and the
memory bus width describes how many bits
of memory are actually being transferred
for each of these clock cycles so from
this we can actually figure out the
maximum speed of the memory the maximum
bandwidth the maximum amount of data
that we can transfer in a second</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>