<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Another Property in Policy Iteration | Coder Coacher - Coaching Coders</title><meta content="Another Property in Policy Iteration - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Another Property in Policy Iteration</b></h2><h5 class="post__date">2015-06-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ku8nLzZJ1_c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so we're going to need one more
interesting property of this operator be
one but this one is specific to the
context of policy iteration so let's
let's think about it this Way's imagine
we've got some policy pai-1 just like
we've been saying and associated with
pie one is the operator be one and let's
say that q1 is the fixed point of b1
right and how do we know that that this
it has a unique fixed point because you
proved it earlier yeah it was the
contraction property if you want yeah so
let's let's imagine that's happened
we've solved out q1 and this is sort of
how policy iteration works to write you
start off with a policy you take get the
value function of that policy then we're
going to take the greedy policy pie 2
with respect to q1 so in the last slide
or so pi-1 and pi2 were just arbitrary
policies now pi/2 is specifically the
greedy policy with respect to the value
function defined by PI 1 so then let be
to be the operator associated with PI 2
and this is the thing that we want to
show the Q function that comes from
solving for the fixed point of pi 1 so
the value function for pi 1 is less than
or equal to b2 applied to q1 so that
makes sense if we take whatever we end
up with after we solved for the value
function for pi 1 we take PI tues greedy
policy and then we do one bellman back
up with respect to that and we get a
value function that dominates q1 okay
and that's because of other stuff on the
slide yeah so we can just work through
that so intuitively what we're saying is
they do policy evaluation on PI 1 we get
some value function q 1 and then we do
one step of essentially value iteration
using the greedy policy with respect to
q1 and that's going to make it no worse
possibly better and so the reason for
that is well what is q1 q1 it's the
fixed point of the b1 operator so in
this particular case it's the reward
plus the discounted expected value for
taking action PI 1 and then go ending up
in some new state as Prime and taking
action pi 1 of s prime for that state
but think about what PI 2 is we can
actually substitute PI 2 4 PI 1 in here
but pi 2 is the greedy policy with
respect to this in other words it is the
policy that causes this exact quantity
to be maximized that's what the greedy
policy does at every state either we're
do no worse than we did with PI 1 or we
might actually do better by switching to
pi to the greedy policy with respect to
q1 so at each state action pair we're
doing no worse by doing one step of well
one bellman back up with be two on q1 so
this is this is exactly the result that
we get that the Q function that we had
before is less than or equal to the Q
function that we had before push through
the the b2 operator the bellman operator
with respect to the greedy policy with
respect to q1 you know essentially the
idea is if we're going to do one more
update on q1 and we're going to do that
update with respect to a policy that we
know is greedy with respect to q1 then
we're moving up in the world oh icicic s
ok ok well no I was sitting there
thinking well shouldn't this be greater
than or equal to and then I think oh no
I see I was reading kind of reading it
back yeah I wanted to write it as
because domination up to this point we
use greater than or equal to but it just
didn't it didn't feel right to put it
this way because it would have been be
too applied to q 1 is greater than or
equal to q1 which is true but the
ordering feels backwards to me so this
this seems better that we're going to
take you one we're going to apply to be
to operator to it and it's going to make
things better righty the other way and
see what happens sure so when I look at
that that says that applying the greedy
operator to q1 makes it better than it
was before all right so if that is more
helpful for you to read it's really the
same thing so that's that's great no no
it is the same thing I just realized the
whole time you were talking I had been
flipping the thing in my head ah sorry
about that no no no it's not your fault
so now we have all the pieces we need we
had one property we called it
monotonicity what we should call this
property something to value improvement
okay so now with value improvement and
other stuff like the definition of pi/2
and b1 and b2 + q1 and q2 and the
monotonicity property we should have all
the things we need to prove that policy
iteration moves in the right direction
that it actually improves the policy or
in the case where the policies are the
optimal it can't improve it but it
doesn't break it leaves it the same
mm-hmm it doesn't ever make it worse</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>