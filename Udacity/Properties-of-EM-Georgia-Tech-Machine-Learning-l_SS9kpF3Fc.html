<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Properties of EM - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Properties of EM - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Properties of EM - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/l_SS9kpF3Fc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so we talked about the equations that
define expectation maximization and we
step through an example with some actual
data in the sense that it was data
points they weren't actual measured data
points but what I'd like to talk about
now for a moment is some of the
properties of the e/m algorithm more
generally so one of the things that's
good about it is that each time the
iteration of e/m runs the likelihood of
the data is monotonically non-decreasing
right so it's not getting worse
generally it's finding higher and higher
likelihoods and kind of moving in a good
direction that way unfortunately even
though that's true it is not the case
that the algorithm has to converge I
mean have you ever seen it not converge
I've never seen it not converge no
because usually there's some kind of
step that you take and you just you make
the weight lower and lower so yeah or
something like that yeah no I've never
seen it not converge so it doesn't have
to converge I think you can construct
really strange examples that make it do
that but on the other hand so even
though it doesn't converge it can't
diverge right it can't be that these
numbers blow up and become infinitely
large because it really is working in
the space of probabilities and it's it's
pretty well behaved as far as that's
concerned that's a difference between it
and k-means right so the argument for
k-means if I recall which feels like was
about a week ago when we talked about
yes but of course it was it was
milliseconds ago is that there's a
finite number of configurations and
k-means and since you're never getting
worse in our error metric so why don't
you have some way of breaking ties
eventually you have to stop and so
that's how you got convergence right
yeah so an e/m I guess the
configurations are probabilities and I
guess there's an infinite number of
those yeah you could do more than guess
so in fact there are an infinite number
of those exactly it wouldn't necessarily
follow that you wouldn't converge from
that but that alone is one big
difference I guess between the k-means
and the the e/m that's the trade-off you
get for being able to put probabilities
on things so you've got an infinite
number of configurations
you never do worse but you're always
trying to move closer and closer so I
guess what could happen is you could
keep moving closer every single time but
because there's an infinite number of
configurations the step by which you get
better could keep getting smaller
and so you never actually approach the
final best configuration I suppose
that's possible but for all intents and
purposes it converges right exactly
however it can get stuck and this you
see all the time I almost never not see
it do this which is to say that if
there's multiple ways of assigning
things to clusters it could find a way
that doesn't have very good likelihood
but can't really improve on very well so
there's a local Optima problem that is
pretty common and so what do we do when
we get stuck in local optima with a
randomized algorithm cry no we take all
of our toys home and randomly restart
there we go ok and the last thing to
mention is this is what you just
suggested a moment ago on the previous
slide which is that it's nothing there's
nothing specific about gaussians in here
it really is an algorithm that can be
applied anytime that we can work with
probability distribution and so there's
just a ton of different algorithms that
work in different scenarios by defining
different probability distributions and
then all you have to do is figure out
what the Estep and the m-step are how do
you do expectation to work out the
probability of the latent variables and
then how do you do maximization to use
those latent variables to estimate
parameters and usually it's the case
that it's the estimation that's
expensive it's difficult because it
involves probabilistic inference right
so it's just like Bayes net stuff and
the maximization is often just you know
counting things but it isn't in general
the case that it's always harder to do a
than M there's some well-known examples
where M is hard and he is actually quite
easy you know for any given problem you
have some work to do to derive what the
e and M steps are but it's very general
it's it really it's a good tool to have
in your toolbox I like that so basically
it's not that hard because it's just a
small matter of math programming indeed</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>