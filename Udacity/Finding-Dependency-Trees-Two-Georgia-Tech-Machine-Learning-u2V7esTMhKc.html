<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Finding Dependency Trees Two - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Finding Dependency Trees Two - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Finding Dependency Trees Two - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/u2V7esTMhKc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Right. So if, in order for this to minimized you would
have to have picked a parent that tells you a lot
about yourself. All right. Because
entropy is information. Entropy is randomness.
And if I pick a really good parent then knowing something about
the parent tells me something about me and my entropy will
be low. So if I can find the set of parents for
each of my features such that I get the most out
of knowing the value of those parents then I will have the
lowest sort of sum of [UNKNOWN] conditional [UNKNOWN]. You with me?
&amp;gt;&amp;gt; Yeah.
&amp;gt;&amp;gt; Okay, now this is very nice and you would
think we'd be done except it's not entirely clear how
you would go about computing this. Actually, I know how
you go about computing it, and let me tell you, it's
excruciatingly painful. But it turns out that there's a cute
little trick that you can do to make it less
excruciatingly painful, and what I'm going to do is I'm going to
define a slightly different version of this function. And I'm going to
call it, j hat. So as I said before,
we want to minimize this particular cost function, j. Which
we get directly from the Kullback–Leibler divergence. So all I've
done is define a new function j prime, where I've
added this term. Just minus the sum of all
of the unconditional entropies of each of the features. Now
I'm able to do this because nothing in this depends
upon pi and so doesn't actually change the proper pi.
That makes sense?
&amp;gt;&amp;gt; Yeah, except I keep thinking about pie.
&amp;gt;&amp;gt; Mm, pie. Mm, I'm sorry, you got me distracted. Okay but do
you see how minimizing this versus minimizing this should give me the same pi?
&amp;gt;&amp;gt; I do. I mean, it's sort of like adding a constant if
you've got a max. It doesn't change which element gives you the max.
&amp;gt;&amp;gt; Right. But by adding this term I've actually come
up with something kind of cute. What is this expression, Michael?
Do you know?
&amp;gt;&amp;gt; It looks kind of familiar from Information Theory. Is that cross-entropy?
&amp;gt;&amp;gt; No, though sort of, but no. Is it mutual information?
&amp;gt;&amp;gt; It is in fact, mutual information.
&amp;gt;&amp;gt; In fact.
&amp;gt;&amp;gt; It's the negative of, mutual information. So, minimizing this expression,
is the same thing as maximizing, neutral information. Now,
I went through this for a couple of reasons
Michael. One is, I think it's easier to, kind
of see what mutual information is. But also because,
this going to induce a very simple algorithm. Which
I'll show you in a second, for figuring out
how to find a dependency tree. And the trick
here is to realize that, conditional entropies are directional.
Right?
&amp;gt;&amp;gt; Wait, so conditional entropies is, is, oh,
is that, that's the quantity that we started up?
&amp;gt;&amp;gt; Right.
&amp;gt;&amp;gt; At the top with the hq, okay, huh.
&amp;gt;&amp;gt; So, this is, this is directional right? Xi depends upon this
&amp;gt;&amp;gt; Yeah.
&amp;gt;&amp;gt; And if I were to do h of pi given xi, I would
be getting a completely different number. Mutual
information on the other hand is bi-directional.
&amp;gt;&amp;gt; Interesting.
&amp;gt;&amp;gt; It's going to turn out to be easier to think about. But before we do
that let's just make certain that this makes
sense so we wanted to minimize a couple
of liable deductions because that's what Shannon told us
to do. We work it all out and it
turns out we really want to minimize the kind
of cost function another way of rewriting this of conditional
entropy. We threw this little trick in, which just
allows us to turn those conditional interviews into mutual information.
And what this basically says is that to find
the best pi, the best parents, the best dependency tree,
means you should maximize the mutual
information between every feature and its parent.
&amp;gt;&amp;gt; Nice.
&amp;gt;&amp;gt; Right. And that sort of makes sense, right? I want
be associated with the parent that gives the most information about me.
&amp;gt;&amp;gt; Charles, I have a question.
&amp;gt;&amp;gt; Yes?
&amp;gt;&amp;gt; Just a little bit confused. The
the xis, what are they, where's that bound?
&amp;gt;&amp;gt; Oh, by the summation.
&amp;gt;&amp;gt; The, what summation?
&amp;gt;&amp;gt; You know, the summation that's always been there
the entire time I've been talking.
&amp;gt;&amp;gt; Oh, I'm sorry I missed that.
&amp;gt;&amp;gt; I don't know how you missed it man, its right there.
&amp;gt;&amp;gt; Yeah, I mean, you didn't actually put the index in
the summation so I was, guess I was, I was confused.
&amp;gt;&amp;gt; My fault right, so just to be clear of anyone
who noticed that the [INAUDIBLE] version is a summation over all possible
variables in the distribution And so we've done here is carry that
all the way through and so these two steps here in particular.
This new cost function that we're trying to minimize is a sum
over the negative mutual information between
every variable, every feature, and its
parents. And that's the same thing as trying to maximize the mutual
information, the sum of the mutual
informations, between every feature and its parent.
&amp;gt;&amp;gt; Hmm, interesting.
&amp;gt;&amp;gt; Right, and again I think that makes a lot of sense, right? You,
basically the best tree is, the best
dependency tree is the one that captures dependencies
the best.
&amp;gt;&amp;gt; I see. Alright. That's cool. Alright, so now
we need to figure out how to do that optimization.
&amp;gt;&amp;gt; Exactly right. And it turns out it's gloriously easy.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>