<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sample Complexity - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Sample Complexity - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sample Complexity - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qf9yJbETVaA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">that is exactly the right question to
ask it's fun to spend all day finding
the v c-dimension of various hypothesis
classes but that is not why we are here
the reason why why we're here is to use
that insight about VC dimension to
connect it up with sample complexity and
so here is the equation that you get
when you connect these things up it
turns out that if you have a sample set
the size of your training data is at
least as big as this lovely expression
here then that will be sufficient to get
epsilon error or less with probability 1
minus Delta and so the form of this
looks a lot like how things looked in
the finite case but in fact it's a
little bit weirder so 1 over epsilon
times quantity 8 times the VC dimension
of H so that's where this quantity is
coming into play so the VC dimension
gets bigger we're gonna need more data
times the log base 2 of 13 over Epsilon
sure plus 4 times the log base 2 of 2
over Delta so again this log of
something like 1 over Delta the inverse
of Delta was in the other bound as well
that's capturing how certain we need to
be that that things are going to work
and again as as Delta gets small the
failure probability gets small this
quantity gets bigger and then the size
of the sample needs to be bigger but but
this is the cool thing that the VC
dimension is coming in here in this nice
fairly linear way so it sort of plays
the same role as the natural log of the
size of the hypothesis space yes that's
exactly right and in fact that thing's
things actually map out pretty similarly
in the finite case and the infinite case
that there's a additive term having to
do with the error the failure
probability there's a you know 1 over
epsilon in the front of it and then this
quantity here having to do with the
hypothesis space is either the size of
the hypothesis space or the dimension of
it depending well the size here is
logged and the VC dimension is not so
that's that's a little bit of a
difference but but there's a good reason
for that as it turns out there is yes
indeed so why don't we why don't we take
a moment and look to see what is the VC
dimension of a finite hypothesis class
the VC dimension concept doesn't require
that it's continuous it
just that when it's continuous the
vc-dimension is required so that that
maybe that's a useful exercise let's do
that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>