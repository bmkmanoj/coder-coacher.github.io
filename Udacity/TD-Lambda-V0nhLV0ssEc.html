<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>TD (Lambda) | Coder Coacher - Coaching Coders</title><meta content="TD (Lambda) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>TD (Lambda)</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/V0nhLV0ssEc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so now we've talked about two
different algorithms TD 0 and TD 1 and
what we'd like to do next is show that
in fact there is a larger algorithm that
actually can includes both of those as
special cases and so we're going to
write that as TD lambda and it's going
to have the property that when lambda
set to 0 we get TD 0 and when lambda set
to 1 we get TD 1 but we also get update
rules for all sorts of in-between values
of lambda I know that sounds kind of
like witchcraft to me no no no it's just
algorithm design it's all good
hmm all right well I'll believe it when
I see it in particular what's going to
let us do that is the fact that both TD
0 and TD 1 the way that we were writing
them have updates that are based on
differences between temporary successive
predictions temporal differences that's
the TD part and so we can actually unite
them into one and I don't need to say
that again because I've already said it
all right so I copied the algorithm back
down here and you know which algorithm
this is and the hope is that it's a
little bit challenging because in fact
if it's TD 0 TD 1 they actually look an
awful lot the same they have a lot of
stuff in common the update rule itself
looks almost identical but they are
different so which which one is this
well I'm trying to remember and let's
see well let's see there's a 0 1 1 and 1
line and a 1 in another line so it's not
gonna help me um
let's hope I could count the number ones
and actually there's a bunch of ones
right there's a st minus one and there's
another one and here's another one and
here's another one and another one yeah
most of those are minus ones not plus
ones and you didn't ask me was TD minus
one
so maybe this is TD negative one oh that
probably suggests that the way I was
going about this was wrong
I think it's TD one alright so Jen do
you have any particular reason for that
because I recall TD one had a bunch of
ease in them in it yes it is easy to
tell the difference so here the
eligibility is something that doesn't
show up in the TD 0 rule the way that we
wrote it but it does show up in TD 1
this sort of idea that what we're gonna
do is we're gonna update the values for
all states all s and the amount that we
do this this same update varies
depending on the current eligibility of
that state in question ok that's
reasonable I feel better now all right
and so to highlight the similarities and
differences between TD 1 and T 2 0 I
took the TD 1 algorithm we just had and
shrunk
and moved it over to the left and then
made another copy of it put it on the
right and made the minimum changes
necessary to turn td1 what was TD one
into into TD zero and so mainly all that
involved was any time the eligibility
was mentioned I kind of got rid of it so
I got it got rid of it from there I got
it right from there and I don't multiply
it here against the the temporal update
and I don't have to update it here and
the only other difference other than
just getting rid of all references to
the eligibility trace is whereas in TD
one we do this update over all states in
TD zero we just update the state that we
with that we left most recently so I
changed that loop instead of making it a
loop it actually it just says for all s
equal to s t minus 1 in other words
we're just going to do this update for s
equal to s t minus 1 all right so now
we're ready to describe the TD lambda
rule what I've done now is I took I got
rid of those words and I took the TD 1
rule and copied it up here so this is
currently TD 1 and what we what we now
need to do to it is make changes to it
small as we can so that it turns it into
TD 0 when lambda is equal to 0 so we
need to introduce a lambda so here's
what I'm gonna do just to kind of cut to
the chase I'm gonna take this this gamma
decay of the eligibility and I'm gonna
change it to a lambda gamma decay of the
eligibility trace all right so all I've
done here is to it's the same it's
exactly TD lambda except I just threw in
an extra lambda here as a multiplier in
the update of the eligibility role and
boom we're done okay so wait okay right
so I was raised to never put two Greek
letters next one another but let's see
what this does by by this ok let's see
so in lambda is 1 then well nothing's
changed it's exactly the algorithm it
was before right great cuz we because if
lambda is equal to 1 and we're just
multiplying this lambda in there then
nothing's changed so boom it really is
it was TD 1 and it is TD 1 right so
let's see what happens if lambda is 0
well well for the thing we just looked
at that means it's 0 so you just said
yes the eligibly trait is equal to 0
right but you started out with the being
equal to 0 - so let's go back to the top
so let's see for all s you have s is 0
which is where we start
start the episode that's that's nice
then we update e of s of t minus 1/2
bees of s of t plus 1 so now ease of s
of T is equal to 1 right good okay so
the eligibility of the state that we
just left gets bumped up to 1 that's
right and everything else is 0 now we
loop over all states right and we update
V sub T of s to be this thing times
well it'll be 0 for everything except
for s of T minus 1 ok and then we will
take the eligibility for that state and
set it equal to 0 right so in particular
on this update the eligibility is 0 for
all the s is not equal to s t minus 1 so
nothing gets changed there and for the
one where it is equal to s t minus 1
this eligibility is 1 so we just do it
you know we just update based on that
which is exactly what we have for the t
t0 rule and then right as you say then
we zero out the eligibility and start it
all again so it's almost like it just
briefly remembers where it needs to
update and then it loses that memory
right away goes break back into
forgetful mode although that won't
matter anyway because you're doing
forever yes once and then you'll pop
back up and do the episode again and you
start all over again
it's like an invariant and so oh but
that's so they only at the end of the
episode so we really only pop back to
right when we're doing these updates we
this is after each t little T transition
so this part we're actually going to
execute over and over and over again
until the end of the episode so we don't
zero out the eligibility until the end
of the episode but in fact using lambda
equal to zero here does it has the same
effect as it so we don't really need to
do that exactly ok so then the right
thing happens yeah so when lambda is
equal to zero boom we get T d0 and I'm
do you go to one boom we get T d1 and so
what the I guess what makes this
interesting is not just that we can
cleverly shoehorn two algorithms into
one but that it allows us to actually
use values of lambda that are in between
which gives us kind of elements of both
of these it has TD 0 ish things to it
and TD 1 issue things to it and what
we're gonna look at next is how to think
about this rule this this generalize TD
lambda rule so that it kind of makes
some sense to kind of ground it in
quantities that we can understand
or to that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>