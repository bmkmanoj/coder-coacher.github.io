<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Why Use CUB - Intro to Parallel Programming | Coder Coacher - Coaching Coders</title><meta content="Why Use CUB - Intro to Parallel Programming - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Why Use CUB - Intro to Parallel Programming</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hoWvTXEbnR8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">now let me focus on a specific aspect of
writing high-performance kernel code at
a high level GPU programming looks like
this there's a bunch of data sitting in
global memory and you have an algorithm
that you want to run that data that
algorithm will get executed by threads
running on SMS and for various reasons
you might want to stage the data through
shared memory now you've seen for
yourself that loading or storing global
memory into shared memory or into the
local variables of the threads can be
complicated if you're striving for
really high performance think about
problem set two where you loaded an
image tile into shared memory and also
had to load a halo of extra cells around
the image tile in order to account for
the width of the blur when this can be
tricky because the number of threads
that want to perform a computation on
the pixels in that tile is naturally a
different number than the number of
threads you would launch if your goal
was simply to load the pixels in the
tile as well as the pixels outside of
the tile or think about the tiled
transpose example in unit 5 where we
staged through shared memory in order to
pay careful attention to coalescing
global memory think about our discussion
of littles law and the trade-offs that
we went over between latency bandwidth
occupancy the number of threads and the
transaction size per thread remember
that we saw graphs that looked sort of
like this where the bandwidth that we
achieved as we increase the number of
threads was higher if we were able to
access for floating-point values in a
single load versus two floating-point
values in a single load versus a single
floating-point value and in a load
finally there are ninja level
optimizations we haven't even talked
about in this class like using the
kepler ldg in turn Jake in short could
as explicit use of user managed shared
memory enables predictable high
performance in contrast the hardware
managed caches that you find on CPUs can
result in unpredictable performance
especially when you have many multiple
threads sharing the same cache and
they're interfering with each other so
that's an advantage of explicit shared
memory but that advantage comes at the
cost of an additional burden on the
programmer who has to explicitly manage
the movement of data in and out from
global memory and this is a big part of
where cub comes in
KUB puts an abstraction around the
algorithm and its memory access pattern
and deals opaquely with the movement of
data from global memory possibly through
shared memory into the actual local
variables of the threads I want to
mention another programming power tool
called CUDA DMA that focuses
specifically around the movement of data
from global into shared memory</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>