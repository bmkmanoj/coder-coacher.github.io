<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Naive Bayes - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Naive Bayes - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Naive Bayes - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/M59h7CFUwPU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so what we'd like to do is
work up to an algorithm that can
actually do some of these inference
steps instead of having to think it
through each time de novo so what I'm
going to do is let's harken back to an
example that we looked at before which
is about spam detection do you remember
the spam example I do remember the spam
example that was way back in the
boosting lecture right yes I think you
did that one I did it was an excellent
example there you go so but we didn't
think about it in a Bayes net setting it
was in a classification setting and
we're trying to come up with a rule but
let's think about this as a Bayes net
where there's a bunch of different
variables that can be true or false
about any given email message it can
either be spam or not it can contain the
word viagra or not it contain the word
prints or not it maybe contains the word
Udacity or not right and so just as we
think about these as these these random
variables if we're trying to build a
belief net or a Bayes net with these
variables we have to say was kind of
what's dependent on what in terms of
representing the probabilities so how
would you how do you think we should
draw arrows to to relate these different
quantities to each other I think that
the arrows should go down from spam to
the other features of spam mail and I'll
tell you why because if I like this
notion of generation that you talked
about a little bit earlier it seems to
me if you know you're in a spam mail or
not it sort of generates certain words
and as you've written this these are
like words I mean I know the spam
example these are you know kind of
stand-ins for features but they're sort
of features of spam mail yeah I think
that's a really good way to think about
it so in some sense what we're saying if
we draw the Bayes net in this way that
any given email message has some
probability of being spam and given that
it's spam it has some probability of
containing different sets of possible
words right so I would say that well
would so what do you see if we can
actually fill in some of these values so
given that we have a spam message how
likely do you think it would be to
contain a word like well let's say the
word viagra earlier it might be
something like 0.3
but a non spam message might be I don't
know like 0.001 right something like
that so how about a word like prints
well I get a lot of email about prints
because I'm a prince fan yeah I was
thinking that that that's why I thought
it would be an interesting example so if
in your spam messages how likely is it
for prints to come up fairly low maybe
like 0.2 because they're talking about
the Nigerian Prince's and whatnot on the
other hand among your non spam messages
how likely is it for Prince to come up
do you think well I get a lot of non
spam so still frozen so relatively low
but not as low as point zero zero one
all right so let's say point one okay
that's a lot of Prince mail you can
never have enough Prince mail oh alright
so in the messages that you have that
are spam how often does the word Udacity
come up I guess it's pretty low I don't
think I've ever seen a spam that
mentions Udacity all right what about
among your non-spam email
well again increasingly it's getting
higher and higher almost as much as I
get prince mail alright so we'll call
that point one as well then okay alright
so now we have oh and what's the
probability of spam versus not spam
since your questions probably have spam
it's pretty low I'm going to say at this
point actually it's not that low
actually at this point it's probably
half my mail Wow alright I'm going to
say 0.4 alright so this is now a
Bayesian network structure that actually
is it's not exactly generating spam but
it is kind of capturing features of
email messages as they come in so we
should be able to answer questions like
what's the probability that a given
message is spam given that the message
has viagra in it but not print or
Udacity so how would we work this out
well since it says naive Bayes I think
I'd use Bayes rule that would be naive
of you alright so now we've applied
Bayes rule we flipped things around and
why is this giving us an advantage well
for this kind of network structure it
actually is a huge advantage right
because we can actually break this first
quantity up oh I do see that okay so
this is where those conditional
independencies come into play if I'm
reading this network right each one of
those attribute values is conditionally
independent of each other
given that you know the
I use spam excellent so then that means
that the first quantity there is
actually a product of each of those
conditional probabilities yes so this is
a really convenient structure because it
really just decomposes into all these
separate helpful quantities so in
particular we can actually derive this
by applying the chain rule but what we
end up with is that this joint
probability over these three variables
decomposes into a product of three
independent joint probabilities the
probability that's contains viagra given
that it's spam which we have that number
is 0.3 the probability that print
doesn't appear in it given that it's
spam and that is that it doesn't contain
prints given that it is spam so that
should be 0.8 because 1 minus the point
2 and that it's not Udacity given that
it's spam is going to be 1 minus this
point 0:01 which would be 0.9999 all
right so this is the case when things
when it is spam and if it's not spam we
can do the same thing and get a product
and then we can normalize to get what
the relative probabilities are between
it being spam and not spam
so then I'm a big fan of normalization
but of course this makes me think about
since it's sort of a classification
problem we only really care about
knowing which one's more likely we don't
really care about the probability right
do we have to normalize yeah yeah
because we do care about the probability
no we do yeah because we're do I asked
what is the probability of spam given
these other quantities Oh actually but
you're right so the observation that
you're making is a really good one which
is that we can do probability
calculations in this setting and that's
actually going to give us answers to
classification problems so we're going
to connect this back to machine learning
but but first let's write a general form
of this formula because it seems a
little bit specific all right so the
general form of this if we're trying to
figure out the probability of some kind
of a root node like this when you have
all these little bristly things coming
down you can think of it as the
probability of a value give it a bunch
of attributes and that's going to be
equal to the product of the probability
that each of those attributes would be
generated by that underlying that's V
this this the label or the or the
underlying class times the prior
probability that V
we just normalize it by all the
different possible values of V this
quantity across all the possible values
of V so so this is one way of actually
getting a very general kind of inference
done and there's as you were pointing
out Charles there's a really nice reason
to think about things in this form
because it does let you do a kind of
classification so essentially if you
think of this top node as being the
class this is what was playing the role
V here and these are all bunch of
attributes then even if we have a way of
generating attribute values from classes
what this lets us do is to go the other
way that we observe the attribute values
and we can infer the class nice so
what's the equation for that right so
the the maximum a-posteriori class if
you're just trying to figure out what's
the most the most likely class given the
the data that you've seen you can just
take an argument over all the different
possible values of that that root node
of the profits probability times the
product of all the attribute values
given that class so this would actually
let us if you're if you're been paying
attention we could in this particular
case compute map spam which is a
palindrome Wow that is spectacular you
did not see that coming did you no I did
not</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>