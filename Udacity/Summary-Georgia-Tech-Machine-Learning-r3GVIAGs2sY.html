<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Summary - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Summary - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Summary - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/r3GVIAGs2sY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Michael so that wraps up all this
uh Bayesian learning stuff what have we
learned today
we did Bayes rule we learned Bayes rule
we even learned how to derive brave
Bayes rule and it was super useful
because it lets you swap kind of causes
an effect so I like the way you put that
Michael that we're swapping causes and
effects sort of mathematically when we
think about Bayes rule what that really
lets us do is instead of having to
compute the probability of the
hypothesis given the data we instead get
to compute the probability of the data
given the hypothesis which is typically
much easier thing to do and what makes
it of course Bayes rule in general is
that you weight that by the prior
probability over the hypothesis which in
fact is one of the important things that
we learned which is that priors matter
so anything else we learn yep we did the
map hypothesis maximum a posteriori
right we learned about H map and we also
learned about hm
ml right the maximum likelihood
hypothesis and what's the maximum
likelihood sis how does it relate to the
maximum posterior it's the map that you
get when the prior is uniform right all
right and we oh we connected up maximum
a-posteriori and least squares yeah that
was pretty I really like that
so we basically do we drove we derived a
bunch of things we've been doing before
and sort of showed that there's actually
a good argument for them at least if
you're a Bayesian they're good arguments
for doing some doing sum of squared
errors there are good arguments for
Occam's razor we've actually able to
give real justification for doing them
other than well sure it makes sense one
right so that includes the minimum
description length story mm-hmm and then
finally you told me that was all a lie
and you said that really what you want
to do is this other kind of way of
picking that actually factors in the
probability of all the different
hypotheses and having them essentially
vote right we really care about this
classification or learning in the end
and so we also learned about the Bayes
classifiers so in fact what we described
before which is voting of hypotheses
turns out to be the Bayes optimal
classifier I didn't say that but it is
very important to
to note and in fact what you should be
noting there is not only is it the Bayes
optimal classifier
it's the Bayes optimal classifier and
what that means is that on average you
cannot do any better than basically
doing a weighted vote of all the
hypotheses according to the probability
of hypothesis given the data you cannot
do any better than this on average so
again what Bayesian learning gives us
and what Bayesian classification gives
us is a way of talking about optimality
and gold standards what do you think
Michael that's really neat I like it I
have to tell you I I really think that
this stuff is kind of cool it's always
nice to be able to take things that
actually work and explain them according
to some framework some underlying theory
I wonder though it seems like all these
Bayesian equations lead us to the
question of how we actually infer
probabilities from various different
quantities and observations so is there
a way to do that so I think the answer
is yes and maybe you should go figure it
out and then tell me about it next time
okay all right as you wish as stay tuned
anyway this has been a lot of fun
Michael I will talk to you later thanks
bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>