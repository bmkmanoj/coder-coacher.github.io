<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What Have We Learned? - 2 | Coder Coacher - Coaching Coders</title><meta content="What Have We Learned? - 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What Have We Learned? - 2</b></h2><h5 class="post__date">2015-07-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BNkDWiZfbpc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what they showed in some of the recent
work is that you run into problems if
you try to minimize the loss function
that we talked about which is trying to
minimize this the kind of TD error
directly and instead you actually get
something that's much more well behaved
if you incorporate the function
approximation itself into the loss
function so take advantage of the fact
that we know that we're changing
parameters that are ultimately going to
influence a linear representation of the
function and that through that linear
representation of the function that
that's where we're going to actually
measure the loss and so the projection
with respect to the function
approximator ends up showing up in the
loss function and that makes things
happier yeah nice well done so we had
successes we had problems and then we
have successes again that's pretty good
yeah and oh one other thing to note on
the success aside is that the the
current we didn't talk about this so
much but the go-to algorithm these days
is called fitted Q iteration which is a
way of mixing function approximation and
Q value learning together that again it
it it has the same kinds of problems it
need not work it need not converge it
can behaved really badly but this is
something that a lot of people have
gotten to work decently so if you're you
know if you're trying to solve these
kinds of problems this is this is the go
to GTD is not quite there yet it's more
theoretical interest but the hope is
that these will actually come more
together in the future and there's one
other topic that we that we spent time
on that I think is worth mentioning
which is the notion of average errs yes
and the cool thing about an average ER
was not only were things well-behaved
you got convergence with with a function
approximator but it actually could be
viewed as a kind of MVP itself the
function approximator could be viewed as
kind of MVP itself which you know lets
us get all kinds of nice properties like
that there's a unique value function
that it needs to converge to one thing
we didn't talk about that's also really
related to this this topic is in the
average ER case we had a set of anchor
points remember that yes I do as the
number of anchor points increases the
error in the value function
approximation that you get actually goes
down and so you can actually converge
overtime on not only an answer but
actually the right answer the right
value function in the limit right
because this is just KN right or was a
lot like cannon and that would be true
for Canon as well yes maybe if you had
an infinite number
points for example you could actually
represent the function so in our
optional reading will actually put a
link to it to the first paper that
really goes through that argument and
shows that yeah in fact as the amount of
data increases things like averages
kernel methods converge to the optimal
value function the true value function
that's really cool so is that everything
we learned including stuff we didn't
learn I think that's everything we
learned and a bunch of things that we
didn't learn and then I probably should
also just say LSP I and just leave it at
that I think that's right we have to
leave them to go learn something on
their own or those what would the point
be yes least squares policy iteration
which is which is a kind of linear
function approximation where policy
improvement is actually incorporated
into the system itself and it has some
super nice properties and people have
gotten it to do good things it the
tricky thing is always the feature set
getting the right features so that
linear function approximation does an
okay job well it's always true isn't it
exactly it's if we could always have the
right features then the problem becomes
easy so that's a lot to learn yeah and
to have learned what are we gonna do
next time Michael here I'll give you a
hint i'm gonna i'm hiding the slide
we're gonna talk about partially
observable models okay that makes sense
that means i have lots of opportunities
for puns all right we'll see you there
then wouldn't want to miss that see it
has already begun all right well by
Michael see Charles</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>