<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Overlap Misses - Georgia Tech - HPCA: Part 4 | Coder Coacher - Coaching Coders</title><meta content="Overlap Misses - Georgia Tech - HPCA: Part 4 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Overlap Misses - Georgia Tech - HPCA: Part 4</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TmHgVCgl0-A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so going back to our choices for
reducing the a mat we have seen that we
can reduce the hit time we have now seen
that there are techniques that reduce
the Miss rate and the final set of
techniques are those that reduce the
Miss penalty so when we miss we don't
suffer as much as before so the first
technique for reducing the Miss penalty
is to overlap multiple misses if time
goes in this direction our processor
does a lot of activity multiple
instructions per cycle and at some point
it does a load for example and now that
load tries to be found in the cache and
if not we're gonna go to memory and it
eventually comes back now if you have a
fancy out-of-order processor it doesn't
stop here and wait what it does is it
finds other instructions to do so even
after we start fetching the data from
memory the processor is continuing but
eventually it starts running out of
things to do and probably before the
load comes back from memory the
processor runs out of resources remember
that it cannot commit this load so
eventually for example it will fill up
the arrow B or maybe even sooner than
that it will run out of reservation
stations if a lot of things depend on
this load so some part of this miss
latency is gonna be directly added to
the execution time but some part of it
is actually overlapping with the
processor activity during the time
between trying to execute this load and
running out of things to do this
processor might actually issue another
load that would be a cache miss so if we
have what is called a blocking cache
then this load cannot be done until the
first load is finished and only at this
point this load can really be tried in
the cache we realize it's a miss we
suffer the Miss latency and meanwhile
because the processor can commit these
instructions here the processor can
overlap some of this miss latency with
some other activity we can also have a
non-blocking cache and a non blocking
cache can support things like hid under
a miss
meaning while we are having a cache miss
hits two other blocks in the cache that
are sent by the processor will be
serviced and returned to the processor
with data and also we can have what is
called a Miss under a Miss in which case
while we are having a Miss we can send
another request to memory so let's look
at what that looks like
so our processor is happily chugging
along it has this first load that
suffers a Miss we check in the cache we
wait for it to come back from memory we
continue working and eventually run out
of things to do because they depend on
this first load but the idea is that now
this load here that also suffers a Miss
will have its own check in the cache and
when we realize it's a Miss we will send
it to memory so it will come back here
now what we have is when the first load
comes back from memory there is a burst
of activity it starts dying out because
we're still waiting for the second load
but then the second load comes back and
we are very quickly back to our normal
operation so now as you can see the
inactivity in the processor used to be
this and this and now it's just this and
maybe a small amount here so by
overlapping the Miss time of the two
loads
we have almost cut the penalty to on
performance to half of what it was
because before we had to wait twice this
now we really wait once and maybe some
little more if we manage to find three
or four loads that overlap then with a
blocking cache we would pay the penalty
three or four times here we might be
paying one penalty plus a little bit
more the property that the processor is
exploiting here is called memory level
parallelism here the memory never gets
more than one axis at a time here the
memory gets accesses to process them in
parallel so of course our memory needs
to be able to do this but if it can then
a non-blocking cache that can do miss
under miss can dramatically cut down on
the cost of misses instead of seeing the
full miss latency being added to the
reacts this time we are really saying
that the penalties of the two misses
overlap so really we pay the penalty
once and we get one to 15 minutes in
exchange</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>