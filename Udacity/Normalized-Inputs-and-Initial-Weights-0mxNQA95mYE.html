<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Normalized Inputs and Initial Weights | Coder Coacher - Coaching Coders</title><meta content="Normalized Inputs and Initial Weights - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Normalized Inputs and Initial Weights</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0mxNQA95mYE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in the example in the quiz the math says
the result should be 1.0 but the code
says 0.95 that's a big difference
go ahead replace the 1 billion with just
1 and you'll see that the error becomes
very tiny we're going to want the values
involved in the calculation of this big
loss function that we care about to
never get too big or too small
one good guiding principle is that we
always want our variables to have 0 mean
and equal variance whenever possible on
top of the numerical issues there are
also really good mathematical reasons to
keep values you compute roughly around a
mean of zero an equal variance when
you're doing optimization the Badly
condition problem means that the
optimizer has to do a lot of searching
to go and find a good solution a
well-conditioned problem makes it a lot
easier for the optimizer to do its job
if you're dealing with images it's
simple you can take the pixel values of
your image that are typically between 0
and 255 and simply subtract 128 and
divide by 128 it doesn't change the
content of your image but it makes it
much easier for the optimization to
proceed numerically you also want your
weights and biases to be initialized at
a good enough starting point for the
gradient descent to proceed there are
lots of fancy schemes to find good
initialization values but we're going to
focus on a simple general method draw
the weights randomly from a Gaussian
distribution with mean 0 and standard
deviation Sigma the Sigma value
determines the order of magnitude of
your outputs at the initial point of
your optimization because of the softmax
on top of it the order of magnitude also
determines the pickiness of your initial
probability distribution a large Sigma
will means that your distribution will
have large peaks it's going to be very
opinionated a small Sigma means that
your distribution is very uncertain
about things it's usually better to
begin with an uncertain distribution and
let the optimization become more
confident as the training progress so
use those small Sigma to begin with ok
so now we actually have everything we
need to actually train this classifier
we've got our training data which is
normalized to have 0 mean
and you need variants we multiply it by
a large matrix which is initialized with
random weights we apply the softmax then
the cross-entropy loss and we calculate
the average of this loss over the entire
training data then our magical
optimization package computes the
derivative of this loss with respect to
the weights and to the biases and takes
a step back in the direction opposite to
that derivative and then we start all
over again repeat the process until we
reach a minimum of the loss function</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>