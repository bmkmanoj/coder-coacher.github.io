<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What Have We Learned | Coder Coacher - Coaching Coders</title><meta content="What Have We Learned - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What Have We Learned</b></h2><h5 class="post__date">2015-06-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SpAYfn2si-g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">So in this section on advanced
algorithmic analysis,
what have we learned?
&amp;gt;&amp;gt; Well we learned a bunch of stuff.
It's a compact and dense lesson.
I think that's pretty good.
&amp;gt;&amp;gt; Oh that was a math joke, wasn't it?
&amp;gt;&amp;gt; Yes it was a math joke.
Well the first thing we learned
is that you like alliteration.
&amp;gt;&amp;gt; [LAUGH]
&amp;gt;&amp;gt; This whole section was
advanced algorithmic analysis.
&amp;gt;&amp;gt; Okay.
&amp;gt;&amp;gt; Mm-hm.
And in particular we talked about
three things, all ending in I.
VI, LI and PI.
&amp;gt;&amp;gt; [LAUGH] Yeah there's
a problem with LI though.
&amp;gt;&amp;gt; Yeah that it's not really a thing.
&amp;gt;&amp;gt; Yeah, it's LP in your programming.
&amp;gt;&amp;gt; But then it wouldn't end in an I.
&amp;gt;&amp;gt; I agree with that, but
at least it ends with P and
the other one starts with P.
There is some over lap.
All of these things
overlap with each other.
&amp;gt;&amp;gt; If you call it VLP for
very linear programming.
&amp;gt;&amp;gt; No, It's just linear programming.
Let's just go with that.
&amp;gt;&amp;gt; Okay.
Very linear is still linear.
In any case, all right.
So what did we learn?
We learned a little bit
about each of those things.
So in particular for
value iteration, we learned that it
converges to optimal in a finite number
of steps, or it gets bound towards that.
&amp;gt;&amp;gt; Okay.
Can we be more precise there?
The value function doesn't converge
in a finite number of steps.
&amp;gt;&amp;gt; But the policy does.
&amp;gt;&amp;gt; The greedy policy does, yeah.
&amp;gt;&amp;gt; Right.
But really, that's what we want.
&amp;gt;&amp;gt; Yeah.
I agree with that.
&amp;gt;&amp;gt; Sure.
But that's a good point.
It's not that the value function
itself has to, because otherwise
we go all the way back to 'em' and
have that conversation again.
But that the policy itself
will stop changing.
The greedy policy will stop changing
at some point and in finite time.
&amp;gt;&amp;gt; Right.
And it will be optimal at that point.
Yeah, and so the value function itself
doesn't necessarily converge, but
it's going to get boundedly
close to the solution.
So that is to say,
if we want to be epsilon close to
the solution to the Belmont equation,
we can define how many iterations of
value iteration we would need to
accomplish that degree of approximation.
&amp;gt;&amp;gt; Right.
And that's fine.
Because in the end that's interesting,
but what's really interesting is that
we get a policy that we can act on.
&amp;gt;&amp;gt; Because we want to maximize reward.
That's just what we are about.
&amp;gt;&amp;gt; That's what we are all about, really.
So let's see.
Then we talked about linear programming,
which is another way to
think about solving MVPs.
What I remember most about it
is you actually define the dual.
&amp;gt;&amp;gt; Okay.
&amp;gt;&amp;gt; That was really cool because it
allowed us to solve the problem and
like so many things that we do,
it was a call back to our first class.
And now I actually understand how
support vector machines work.
&amp;gt;&amp;gt; [LAUGH] Okay.
I think you did that lecture, so
I was hoping you understood it already.
&amp;gt;&amp;gt; Well,
there was just a point where you say,
well or could you solve the dual,
and then I rapidly wave my hands and
said don't worry about it,
it's a dual, it's a thing.
&amp;gt;&amp;gt; Okay.
&amp;gt;&amp;gt; But now we talked about the dual, so
I think that that's good.
&amp;gt;&amp;gt; I feel like the concept of dual
is somehow relevant to superheroes.
&amp;gt;&amp;gt; You think so?
&amp;gt;&amp;gt; I don't know.
You know way more about comic books
than I do, but isn't there like every
superhero has a dual,
like anti-Batman or the dark Spiderman?
&amp;gt;&amp;gt; Venom?
So you're saying that primal and
duals are like nemesis.
&amp;gt;&amp;gt; Yeah, or
mirror images of each other in some way.
&amp;gt;&amp;gt; So the flow,
the sort of policy flow idea,
is like the nemesis of the value idea,
or the mirror image of it.
&amp;gt;&amp;gt; Yeah, sure.
&amp;gt;&amp;gt; Let's go with that.
I was thinking more of a hip-hop
analogy, but I think that's pretty good.
&amp;gt;&amp;gt; Oh, flow is a hip-hop thing.
&amp;gt;&amp;gt; It's a hip-hop thing.
&amp;gt;&amp;gt; And values.
People in hip hop have values.
&amp;gt;&amp;gt; That's right, so
it just follows in one step.
Okay, speaking in one step.
So, then there's policy iteration,
which I thought was kind of cool.
We got a chance to prove
a bunch of things about it.
And I think the first thing
we talked about there or
somewhere in that mix was domination,
which turned out to be really cool.
&amp;gt;&amp;gt; Yeah and that comes up actually
in all of the algorithms.
Value iteration has some of that.
Linear programming has some of that.
And policy iteration.
The proof of policy iteration
depends significantly on that.
&amp;gt;&amp;gt; Right.
Speaking of things the proof of policy
iteration depends on, we also talked
about value non deprovement, or
value improvement as I believe is
the non-technical term for it.
&amp;gt;&amp;gt; That's right and we argued that we'd
actually get strict value improvement
any time that we didn't have
a policy that was already optimal.
If it was already optimal then a step of
policy iteration resulted in value non
deprovement which is just our broken
way of saying it doesn't get worse.
It might not get better,
but it doesn't get worse.
&amp;gt;&amp;gt; Yeah.
It's like a monotonically
non-decreasing function.
But the other thing is,
we had a discussion that I think
is actually pretty important,
that when we talk about things like
domination, you writing them down for
good notational reasons, talking
about an entire value function, but
really this is about things have to
also be true on a state by state basis.
&amp;gt;&amp;gt; That's right.
&amp;gt;&amp;gt; Because if that weren't true,
then in principle, you could cycle, and
then you get stuck in local optimum.
But here,
we don't get stuck in local optimum,
because things also dominate
on a state by state basis.
&amp;gt;&amp;gt; Cool.
&amp;gt;&amp;gt; Kind of neat.
We also talked about monotonicity.
&amp;gt;&amp;gt; And
that you can contract it from kissing.
&amp;gt;&amp;gt; Yeah.
Oh, you said contract.
Very well done, Mike.
&amp;gt;&amp;gt; Oh, yeah.
That was pun intentional.
Sorry.
&amp;gt;&amp;gt; Pun intentional.
[LAUGH] Okay and really when you
put all these things together.
I think the real thing we learned in
this lesson is that proofs do not suck.
&amp;gt;&amp;gt; [LAUGH] So that's great.
Because if I recall we started our last
class with a little discussion about how
I don't mind proofs as much as you do.
&amp;gt;&amp;gt; That's right.
Speaking of proof you
forgot the s on proofs.
&amp;gt;&amp;gt; See, I did a proof of your proof.
That was pretty good.
&amp;gt;&amp;gt; You proof read it?
&amp;gt;&amp;gt; Yes.
Yes.
&amp;gt;&amp;gt; Nice.
&amp;gt;&amp;gt; I feel like we have converged.
&amp;gt;&amp;gt; [LAUGH] And
that's what this was all about.
&amp;gt;&amp;gt; That's what this is all about.
&amp;gt;&amp;gt; Yeah, there's something
very satisfying about getting
to the convergence point.
&amp;gt;&amp;gt; Very nice.
Very nice.
Okay well,
I think that's everything that I was
capable of learning in one lesson.
&amp;gt;&amp;gt; [LAUGH] Yeah, all right, well so next
time we'll start in with something else.
I think we're going to talk about how
we can modify reward functions and
get different things out of them.
&amp;gt;&amp;gt; Oh yeah, that's my favorite.
Excellent.
Well, then I will see you then.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>