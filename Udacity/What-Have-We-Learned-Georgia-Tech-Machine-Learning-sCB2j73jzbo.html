<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What Have We Learned - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="What Have We Learned - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What Have We Learned - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sCB2j73jzbo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Michael so I think with that little
bit of discussion I feel like we're done
cool all right well it's been nice
talking to you I hope the course went
well and oh you mean just for this
lesson
yeah just for this lesson let's see what
have we learned so remind me Michael
what have we learned well we were
talking about instance based learning
that's true
that's the first thing we learned you'll
notice by the way I never actually told
you why it was called instance based
learning
Charles why is it called instance based
learning I don't know but I'm willing to
guess that it has to do with the fact
that we look at the exact instances that
we have and we base our learning on that
all right and we brought it up in by
starting off thinking about eager and
lazy learning right
what is the difference Michael I will
tell you when I need to tell you that's
exactly right so lazy learning is about
putting off the putting off the work
until it's actually needed eager is
about as soon as the problem is posed
solve it and then you know if you're
lucky then you know the answer will
eventually come in handy
exactly right okay what else so as a
concrete example of a lazy learner we
talked about K nearest neighbors or KN
okay MN and this whole notion of nearest
neighbor is in fact one way of talking
about similarity functions right and the
similarity functions play a really
central role in all of this right
similarity we talked about it as if they
were distance functions but distance is
just another way of talking about a
similarity so this is actually a key
thing Kanan was a specific algorithm we
used and we talked about various
versions of it and the nearest neighbor
part really got us to think a little bit
about similarity and distance and and
what all that means a really important
thing here is I think that similarity is
just another way of capturing domain
knowledge and K and K and n is another
way of capturing domain knowledge and
that if we saw through the quizzes and
through some of our discussion that this
is actually very very important that
domain knowledge matters now we also
talked about KNN in the context of both
rushon and classification I see what you
did there
canalla JH he's got K&amp;amp;N in it okay
so yeah so classification or regression
or different things but KN can handle
both of them and at the end of the day
that's all stuck in our notion of
similarity and our notion of averaging
which we kind of took as an overall term
for a bunch of different things you
might do which some people might find
confusing but I think that other people
would really kind of understand what we
mean I'm very tempted to take that out
of Wikipedia but that would just be rude
it would be rude anything else
we learned one big thing we looked at
how to compose various different
learning algorithms together for example
in the context of locally weighted
linear regression we used this this
instance-based idea along with linear
regression to get something that was
both sort of locally smooth but globally
bumpy right so I'm gonna just say
locally weighted regression where we can
do any kind of regression we might want
to I was gonna call that dollar sign
excellent so sticking your favorite
value let's see what else Oh Oh a really
big thing was Bellman's curse of
dimensionality yes and the idea there
was that the more features that you
include the more data that you need to
fill up that space yep it's exponential
and in fact I even just I decided to go
and play with this a little bit so that
example that you were doing before where
the y equals x1 squared plus x2 mm-hmm
when I gave it well as we saw in the
example we gave it like ten or twelve
examples and it did really badly so it
continued to do somewhat badly until I
got to about a hundred thousand and then
it was actually doing really well but
that seems like an awful lot of examples
for what is otherwise a very simple
problem right well if you think about it
the amount of data you have to see to
determine the relative relevance of the
two different dimensions is quite a bit
in that particular kind of function hmm
yeah that's a lot of space to cover all
possible real values across a
potentially infinite space
yeah I guess that's true yeah so the
curse of dimensionality is real and we
just sort of can't get around it
although as I mentioned earlier we will
see in the second part of the course
ways that people try to get around the
curse of dimensionality aha okay blunt
it right yeah at least blood cuz you
can't actually get around the cursor
dimensionality you can only deal with it
there is no free lunch there is no free
lunch in fact that's a theorem
yeah I think so what's the theorem no
free lunch that any learning algorithm
that you create is going to have the
property that if you average over all
possible instances it's not doing any
different than random right and and
another way of thinking about that a
practical way of thinking about that is
if I don't know anything about the data
that I'm gonna have to learn over then
it doesn't really matter what I do
because there's all possible kind of
data sets however if I have domain
knowledge I can use that to choose the
best learning algorithm for the problems
that I'm going to encounter so does that
mean that all of machine learning really
comes down to you have to already know
what you need to solve the problem to
apply these techniques to solve the
problem no but you have to know a little
bit about your problem in order to
decide what to do and in fact you can
make the argument that this entire class
is about exposing the students to a wide
range of techniques and giving them
enough practice so that they can do a
pretty good job of telling given a
problem would it be better to use this
kind of technique or this kind of
technique it's can in a sort of better
way of approaching it decision trees a
better way of approaching it there's a
lot of what this class is about is
helping them to get enough domain
knowledge or enough knowledge anyway so
that they can apply it to particular
domains cool all right what that seems
like a plenty useful lesson
yes it's very hopeful note to end on so
let's end on that all right see you next
time all right bye Michael</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>