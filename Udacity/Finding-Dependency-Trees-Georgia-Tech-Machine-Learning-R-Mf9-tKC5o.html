<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Finding Dependency Trees - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Finding Dependency Trees - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Finding Dependency Trees - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/R-Mf9-tKC5o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Michael so what I'm gonna do is I'm
gonna step through how you find
dependency trees and just stop me if it
doesn't make any sense
okay sure but hopefully it's fairly
straightforward at least if you
understand information theory and again
I want to stress that although we're
gonna we're gonna spend some time doing
this this is just one example of ways
that you could represent a probability
distribution it's just going to turn out
that this one's particularly easy and
powerful okay exciting okay so the first
thing we have to remember Michaels that
we have some true probability
distribution which I'm going to write as
P that we're trying to do that's our pea
soup theta in this case but the general
question of how you represent a
dependency tree doesn't depend on theta
or anything else
there's just some underlying
distribution we care about let's call
that P okay and we're gonna estimate it
with another distribution which I'm
gonna represent as P hat because you
know for approximation that's going to
depend upon that parent function that we
defined before okay okay so somehow I
would have you sound skeptical Michael
well because I saw that you wrote those
train tracks and you figure they must
mean something
well I know they mean something that's
it's an information theory thing but I'm
not gonna remember what it is you're
gonna it's gonna you're gonna say
something like KL divergence that's
exactly right
so somehow we want to not just find a P
hat sub theta that is a dependency tree
that represents the underlying
distribution but we want to find the
best one and so best one here sort of
means closest one or most similar or the
ones that would generate the points in
the best possible way and it turns out
for those of you who remember
information theory that there's actually
a particular measure for that and it's
called the KL divergence you remember
the KL divergence stands for Michael
Cole backlight blur that's right and it
has a particular form and in this case
let's assume non continuous variables it
has basically this form and that's
basically a measure of the divergence
that's what the D stands for of the
divergence between the underlying
distribution P that we care about and
some other candidate distribution P hat
that we're trying to get as close as
possible so if these are the same
distributions if P hat appear the same
distribution the Kobler level divergence
is equal to zero
and as they differ or as they diverge
this number gets larger and larger now
it turns out that these numbers are unit
lists they don't obey the triangle
inequality there's a whole this is not a
distance this is truly a divergence but
if we can get this number to be
minimized then we know that we have
found a distribution P hat that is as
close as we can get to P okay and we
have a whole unit that a pushkar will do
to remind everybody about information
theory and where this comes from but
basically this is the right way to
define similarity between probability
distributions you just have to kind of
take that okay okay well I'll pause this
and go back and listen to pushcarts
lecture and then I'll be ready for you
okay beautiful okay so what we really
want to do is we want to minimize the
public liability vergence the that is
minimize the difference between the
distribution that we're going to
estimate with the dependency tree and
the true underlying distribution and
just by doing some algebra you end up
getting down to what looks like a fairly
simple function so Michael if you were
if you were paying close attention to
the algebra you will realize that well P
log P now that you've come back from
push cars lecture is just entropy yep so
or it's minus the entropy and so I can
rewrite this as simply minus the entropy
of the underlying distribution plus the
sum of the conditional entropies for
each of the excise given its parent
which has some sort of intuitive
niceness to it but whatever this is what
you end up with just by doing the
substitution P log P gives you minus
entropy of P minus P log P hat which
gives you the conditional entropy
according to the function the parent
function pi okay well in the end all we
care about is finding the best PI so
this term doesn't matter at all and so
we end up with a kind of cost function
that we would like to minimize which I'm
going to call here J which depends upon
pi which is just the sum of all the
conditional entropies basically the best
tree that we can find will be the one
that minimizes all of the entropy for
each of the features given its parents
does that make any intuitive sense to
you yeah
think so because we want we want to
choose parents that are going to give us
a lot of information about the values of
the corresponding features</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>