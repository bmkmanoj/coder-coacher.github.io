<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Libraries - Intro to Parallel Programming | Coder Coacher - Coaching Coders</title><meta content="Libraries - Intro to Parallel Programming - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Libraries - Intro to Parallel Programming</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xiIrfLzIC50" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let's shift gears now and talk about
ways to avoid programming in the first
place
there are many libraries available for
CUDA more come out every day there are
some libraries that are developed by
Nvidia some by third parties there's
open source libraries and there's
commercial products and of course
there's no way I can cover all the
available libraries out there but I do
want hit a few highlights here and talk
about some of the more popular and
useful libraries out there the libraries
I'm going to talk about are for the most
part fairly mature they're designed and
optimized for performance by experts and
they get tuned up and re-released every
time a new GPU architecture comes out so
if you can use these libraries you
should so the first thing I want to
highlight is called cube Blas and cube
loss is an implementation of the Blas or
basic linear algebra subroutines this is
a venerable library that's been around
for a long time it's used in Fortran and
C and bi and scientists and engineers
everywhere you know use this as one of
their go-to workhorses for dealing with
dense linear algebra next to CU fft fft
stands for fast fourier transform and so
not surprisingly this is the cuda fast
fourier transform this includes various
batch transforms as well as support for
various you know real complex complex to
complex FFTs and so forth it has an
interface similar in in ways to the fftw
the popular fastest fourier transform in
the West routine so it's a familiar
interface to anybody he uses FFTs as
part of their bread-and-butter toolbox
see you sparse as blast like routines
for doing linear algebra on sparse
matrix formats sparse matrices as you
know are matrices that are mostly 0 and
therefore stored in some sort of
compressed format and see you sparse
supports a variety of formats including
and includes higher-level routines like
incomplete Lu factorization CU r and is
a bunch of pseudo and quasi random
number generation routines for making
random numbers and this includes device
side functions as well as host
interfaces for quickly filling arrays
with numbers drawn from particular
distributions various high-quality
random number generation basically NP P
stands for NVIDIA performance primitives
and this is basically for the most part
low-level image processing primitives so
highly optimized low level primitives
for image processing magma is developed
by the same group that wrote the
original lay packed library and lay pack
is another one of these tools in a in
the toolbox of
scientists and engineers who simply use
these tools for linear algebra all the
time so magma provides GPU and
multi-core CPU implementations of many
late PAC routines so sort of a modern
parallel rethinking of our
implementation of late pack style
linear algebra another linear algebra
tool is kula which is implementations or
of eigen solvers and matrix
factorizations and matrix solvers and
for dense matrix sorry for dense
matrices similar to the late pack api
and there's also a sparse cooler package
as well and array fire is a framework
for data parallel manipulation of
different types of array data including
a wide variety of built-in operations
from numerical linear algebra decennial
signal processing to financial so this
is somewhere in between a
domain-specific library like these
others and a programming power tool
which is sort of the next category we'll
talk about</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>