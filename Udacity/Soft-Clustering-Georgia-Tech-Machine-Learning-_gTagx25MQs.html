<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Soft Clustering - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Soft Clustering - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Soft Clustering - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_gTagx25MQs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so to do soft clustering we're going to
use a similar trick to what we've used
in some of the other lectures which is
to lean on probability theory so that
now points instead of coming from one
cluster or another cluster can be
probabilistically from one of multiple
possible clusters that seem like a good
idea it does I feel a song coming on
lean on probability when you're not
strongly believing one thing almost yeah
so that's because it's soft clustering
or soft assignments a set of hard ones I
like it all right so to do this we're
gonna have to connect up a probabilistic
generative process with the data that we
actually observe so let's assume and
there's many ways to go down this road
but we're gonna go down the road this
way assume that the data was generated
by what happens is we're gonna select
one of K possible Gaussian distributions
so we're gonna mention that the data is
going to be generated by draws from
gaussians from normals mm-hmm let's
assume that we know the variance Sigma
squared and that the K gaussians are
sampled from uniformly okay and then
what we're gonna do is given that
Gaussian we're gonna select an actual
point an actual data point in the space
from that Gaussian and then we repeat
that end times so if n is bigger than K
then we're gonna see some points that
come from the same Gaussian and if those
gaussians are well separated they're
gonna look like clusters assuming they
have very different means all right
that's what I mean by well separated
yeah exactly so yeah okay good all right
and in particular what we'd like to do
now is say all right well now what we're
really happening is we're given the data
we're thinking kind of Bayesian lis
right we're given the data and we want
to try to figure out what the clusters
would have been to have generated that
data so we're gonna try to find a
hypothesis which in this case is just
going to be a collection of K means not
to be confused with k-means hmm that
maximizes the probability of the data
right so find me came you values which
are the means of those Gaussian
distributions so that the probability of
the day given that hypothesis is as high
as possible and this is an m/l
hypothesis and ml of course stands for
michael Lippmann I don't think that's
right machine learning that
closer but not quite right maximum
likelihood that I think is correct all
right so that's now the problem set up I
didn't actually give you an algorithm
for doing this but presumably it's going
to depend on various kinds of things in
probability theory and optimization but
is it sort of clear what we're shooting
for now it is it is and I actually think
the fact that we're looking for k-means
probably means that we are going to end
up tying it back to k-means maybe so but
again it's it's a softer kind of k-means
it's a softer gentler kind of Campion's
mm-hmm I think some people call it K
gaussians is that is that sun right no
all right then I mean it sounds correct
but it doesn't sound right all right
then</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>