<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Monte Carlo Tree Search p3 | Coder Coacher - Coaching Coders</title><meta content="Monte Carlo Tree Search p3 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Monte Carlo Tree Search p3</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EGN1KAjtNS4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so now that I have that I backed
everything up and I can just do
selection again now this time around
because I've changed sort of my
estimates of the queue functions along
the way I might want to make a different
selection and so that actually tells us
where this original policy comes from
this original policy is just an estimate
of Q values that I have where I happen
to feel very comfortable about those Q
bags I've visited these states many
times I backed up a lot of information
from them so I have a decent estimate
and I'm willing to do the kind of greedy
policy that is implied by these q
functions I see so we're taking greedy
greedy steps down the black part of the
tree when we hit a leaf then we do the
rollout policy PI R to get estimates of
those different values do do we now know
anything new about the tree yes and in
fact if we did this you know a hundred
bazillion times or however many number
of times we sort of wanted to we just
can say well I know the actions that I
can take from this node and so I feel
comfortable and expanding my policy all
the way down to here for this particular
node as well and when if I did this
again and let's say the selection
algorithm had me go here here here and I
got back to here again I would actually
know its action I wanted to take say it
ended up with me being here but then
once again I've gotten to a place where
I don't feel comfortable about what to
do next
and I would do the expansion and the
rollout again over and over and over
again so I just keep doing this again
and again and again and again and again
and again and again and again and what
this gives me is an ever expanding tree
where I feel more and more comfortable
about my Q values and therefore more and
more comfortable about a policy that I
should take and it seems like it's
getting more accurate over time and in
fact if you were at each of these nodes
right and you took those actions and you
did the expansion and you did that oh
let's say an infinite number of times
and then you did an infinite number of
rollouts over and over again eventually
you would converge to a proper Q value
cool essentially so and right so this is
a planning algorithm we need to know the
transition model to be able to do those
rollouts right or you have to have some
way of doing the simulation so when you
say you need to know the transition
models you could mean
a couple of things you can mean I
actually have the transition model and I
can do calculations with it or you could
just mean I have some way of simulating
the world and I can just sample from it
so this is pretty neat
right I think it has some neat little
features to it it allows me to kind of
build up a model the world do a bunch of
simulation and figure out what to do but
there are a couple of questions that
should pop out to you even though I kind
of ran over them fairly quickly I think
the first one is when do you stop what
is this actually I mean this is kind of
an inner loop of a process right I'm at
this state I have an estimate of Q
values that I'm comfortable enough in
that I'm gonna take actual actions
following the policy that's implied by
them and then I'm gonna keep expanding
and learning more and more about it well
in principle I could just do that for
eternity but at some point you have to
stop and when do you stop well you stop
when you get bored and what do you do
when you stop well once you stop it's
the next question what do I do after I
stop well you execute you do a sort of
one-step policy based upon what you've
learned and then you throw it all the
way and you do it all over again
wait so you take actually do you take
that step in the world using the policy
the whole chain that you figured out
well you could but and in fact sometimes
I mean well you can do actually any
number of things there but sort of the
simplest version of this is I started
out with an empty tree I'm in some
particular state I'm gonna hallucinate
all the things that I might do from
there as I do that I'll be learning more
and more about the world this will allow
me to do better and better exploration
as I do this kind of Monte Carlo search
and simulation and hallucination and
then eventually I learn enough that I
know what the right thing to do is from
this state and then I do it and then I
end up wherever it is I end up in the
real world and when I'm asked to do
something again I just do it all over
again
Wow so it does a lot of thinking each
time it takes a step yeah but it's very
fast thinking because you know I'm just
flipping coins if I have a fast
simulator and I'm able to do addition
pretty quickly and maybe some you know
averaging I can actually do this
estimate fairly well what's going to
well impact this a lot is sort of how
far down I can go before I run out of
computational power typically that's
what board really means board means okay
I've taken 15 seconds that's too much
time I really or maybe I've taken three
seconds that's too much time it's time
to do the best thing that I can based
upon what I know
but if I can do that every single time
if I can expand this tree pretty deeply
at every time step then I you know I
lose 15 milliseconds but 50 milliseconds
is a long time for a computer and not
any time at all for human beings and the
alternative would be to actually explore
the entire space and try to solve the
underlying MTP if you have a whole lot
of states and a whole lot of actions
then that can take a very very long time
so do you take 50 or 200 milliseconds
every time you need to take an action
and do it online or do you take three
and a half months of super computing
power to figure out the optimal policy
and then use that and the answer is it
depends</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>