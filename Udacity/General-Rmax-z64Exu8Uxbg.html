<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>General Rmax | Coder Coacher - Coaching Coders</title><meta content="General Rmax - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>General Rmax</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/z64Exu8Uxbg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we're gonna make a general rmx algorithm
and it's gonna be really simple because
it's gonna be an awful lot like the R
max algorithm we already looked at good
so I copied over the R max algorithm
that we had before
and it went like this we're gonna keep
track of the MDP which is to say we're
gonna have an estimate of the
transitions and rewards for all the
state action pairs any unknown state
action pair we're gonna assume has a
reward of rmx and then a self-loop so
that we can just get like for a long
time then we actually build that MVP we
solved that MVP and we take an action
from the optimal policy with respect to
that MVP that we solved the only
difference is going to be this notion of
unknown and the idea is that we're gonna
have some parameter call it see that
state action pair is unknown if it's
been visited or we you know we've tried
it out fewer than C times okay is that C
different for every state action pair
that's a good question so I want to
first kind of give the parametrized
algorithm then what we have to do is if
we're gonna say that this algorithm is
efficient we have to derive some value
for C and show that it's you know not
too big and not too small but you know
just right just right yeah so the way
that we're defining unknown here is that
we have some parameters C and we
consider a state action pair s a unknown
if it was tried fewer than C times after
that point then we switch over to the
maximum likelihood estimate so so you
know we've tried it C times and we saw
you know we went from state si sorry if
we went from state s with action a to
some state you know s 53 so if we did
that some number of times we only do
that once
then we're gonna estimate a probability
of 1 over C for getting that transition
okay so this gives us a new estimate of
the transitions and rewards based on the
data that we've actually gathered ok ok
so I just want to point out that this is
the R max algorithm we used in the
deterministic case and this very much is
related to the huffing bound estimate we
used in the stochastic non sequential
case in the Bandit case but just
grafting the two things together</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>