<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Slac and Big Data - Intro to Computer Science | Coder Coacher - Coaching Coders</title><meta content="Slac and Big Data - Intro to Computer Science - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Slac and Big Data - Intro to Computer Science</b></h2><h5 class="post__date">2014-07-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4_0sCB_csRI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we here at SLAC National Accelerator lab
and we're going to see how they use
computing to understand the mysteries of
the universe we're standing in the clay
jean gallery formerly the longest
building in the world you're here at
SLAC National Accelerator Laboratory
this is a 50 year old laboratory as all
the the flags on the on the lampposts
around the lab are telling you it was
founded to build a 2 mile long linear
accelerator slack is an accelerator
laboratory still its main science is
based on accelerating particles and
creating new states of matter or dis
exploring the the nature of matter with
the accelerated particles and this
always has generated a lot of data a lot
of information it's it's very
data-intensive experimental science so
from the the earliest days of slack
computing to analyze data has been a
major part of the the activity here you
really can only study the cosmos by
studying studying it in a computer you
get one chance to look at it but to
understand how it evolved into the state
it is now you have to do all this in the
computer so there are massive
computations going on for that sort of
simulation massive computations in
catalysis in material science and
massive data analysis going on here as
well the particular particle physics
experiment that I am involved in right
now has some 300 terrible petabytes of
disk space some 300 thousand terabytes
some 300 million gigabytes of disk space
around the world to do this analysis and
of course we are far from understanding
everything about the universe but this
is one of the most probably the most
data intensive activity in science today
the raw data rate coming out of the
Atlas detector that I'm involved in is
about a petabyte a second that's a a
million
gigabytes a second you can't store that
with any budget known to man so most of
it is inspected on the fly and reduced
to a small storable a much smaller but
still large storable amount of data
right now we are sifting through these
many many petabytes of data to look for
signals of the Higgs boson as no doubt
people have heard in the news and there
are tantalizing hints that I I'm not
holding my breath about at all right now
but this is the way we do it you need to
have those vast amounts of data just to
pick out the things that will really let
revolutionize physics in there and you
need to understand all of it in detail
because what you're looking for is
something slightly unusual compared with
everything else if you don't understand
everything else perfectly then you you
don't understand anything so we're
looking at one of the one of the racks
that contains the Atlas proof cluster
here at SLAC so Atlas is the experiment
the Large Hadron Collider in Geneva
Switzerland that could collides protons
the fundamental you know building blocks
nature at traveling at very very very
close to the speed of light
some trillions of times of the energy of
they have at room temperature and so you
get many and many of these collisions
happening at once and this enormous
machine that reads out trillions of data
channels and at the end of the day you
have this enormous amount of data
petabytes of data that you have to
analyze looking for very rare very
particular signatures inside of that if
I want to look for a rare signature so
something that had a lot of energy and
then a lot of really strange particles
at once there are trillions and
trillions of these events stored on this
machine to look for them in any
reasonable amount of time I have to look
to it many searches at once I have to
use the computers all the cores on the
computer so the hundreds of cores on the
machine all running at full speed at the
same time to have any hope of doing it
at any reasonable amount of time this
isn't the sort of thing that search
engines currently do they're looking for
text strings and kack and indexing all
the text strings
find in in in some way like this what we
have is very very structured we know the
structure of the these data we we know
exactly how to go to anything that we
want to get to in these data because
they the way in which everything is
linked together is very well understood
things will go wrong all the time you
cannot assume you won't lose data from
the disk you send it by network from one
computer center to another you cannot
assume it arrives undamaged
you cannot assume your computers don't
die in the middle of calculations
everything can pop go wrong so the
computing we do for the LHC has many
layers of error correction and retry and
some of the basic failure rates are
quite high but by the time everything
has been fairly automatically retried
and errors have been corrected we get
high throughput and and a high success
rate</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>