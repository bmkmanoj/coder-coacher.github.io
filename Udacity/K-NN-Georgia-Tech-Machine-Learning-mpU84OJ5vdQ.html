<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>K NN - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="K NN - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>K NN - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mpU84OJ5vdQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so what we have here again is a
pseudo code for our knn algorithm and
i'm sort of writing it as like a
function so you're going to be given
some training data D that's a little XY
point six x y1 X Y 2 XY 3 so on and so
forth you're given some kind of distance
metric or similarity function and this
is important because this represents our
domain knowledge as I think we've
already said you get some number of
neighbors that you care about kay it's
the KNN which also by the way represents
domain knowledge tells you something but
how many neighbors you think should have
and then you're given some particular
new query point and I want to output
some kind of answer so I'm label some
value so the kanan our rhythm is
remarkably simple given these things you
simply find a set of nearest neighbors
such that they are the K closest to your
query point ok I'm surprised just
processing this so the data the capital
D are those pairs and there's a set of
pairs yes
ok and K smallest distances so this n n
this is a set yes and it it consists of
all the elements in the data that are
closest to the query point yep and the
query point is a parameter of that ok
yeah all right I think oh and then it's
then it says just return yeah so we
haven't figured out what to return so
there's two separate cases we've been
talking about so far one is where we're
doing classification and one is where
we're doing regression so a question for
you would be what do you think we should
do when we're doing classification sort
of what we were doing before on the map
what would be a way of returning a
proper label so you want to label not a
like a weight on a label or something
like that no I want to label you have to
produce an answer yet to commit to
something Michael all right can I commit
to more than one thing nope
ok so I would say that a reasonable
thing to do there would be with the Y's
associated with the things in nn yep so
I would go with they should vote nah I
like that I think that's a good one so
we'll simply vote and what does it mean
to vote it means let's see so feel like
there would be a way to represent it in
terms of an end the set like do you want
me to write it formally no oh then I
would just say the closest point which
ever why I is most frequent among the
closest points wins
yeah right so you want to find a vote of
basically a vote of the Y's that are a
part of the neighborhood side and you
take the plurality aa plurality I see so
it's whichever one occurs the most it's
the most ties the mode right hmm I love
mode what if there are ties that's a
good point
well if there are ties among the output
then you're just gonna have to pick one
okay and there's lots of ways you might
do that you might say well I'll take the
one that is say most commonly
represented in the data period or I'll
just randomly pick each time or any
number of ways you mind it you can
imagine doing that the one that's first
alphabetically one is first
lexicographically okay what about in the
regression case okay so in the
regression case our Y's are numbers and
we have the closest Y eyes so we have a
bunch of those numbers and it seems like
if you if you have a pile of numbers and
have to return one a standard thing to
do would be to take the average or the
mean yeah let's just simply take the
mean of the Y's and at least there you'd
have to worry about a tie that's right
though I guess you know we didn't really
deal with the question of what happens
if there's more than K smallest like
what if they're all exactly the same
distance all n of them are exactly the
same distance so which are the K closest
well there's lots of things you could do
there I guess what I would suggest doing
is take the if you have more than K that
are close that are closest because you
have a bunch of ties in terms of the
distance just take all of them get the
smallest number greater than or equal to
K okay that's seem reasonable yeah I
think that's what college rankings do
actually that is what college rankings
do and then they yeah that's exactly
what college rankings do so let's do
that we know the college rankings make
sense yeah those are they're
scientifically proven to be used scary
scary to people in colleges that's
exactly right so here's what we got
Michael so all we do is you take the
training data we have some notion of
similarity
distance we have a notion of the number
of neighbors that we care about we have
a query point we find the K closest ones
you know breaking ties accordingly and
then we basically average in some way in
a way they make sense for classification
we make sense for regression and we're
done it's a very simple algorithm but
some of that's because a lot of
decisions are being left up to the
designer the distance metric the number
K how you're going to break ties exactly
how you choose to implement voting
exactly how you choose to implement the
mean or the average operation that you
chose to do here and you could put a
bunch of different things here and you
end up in completely you could end up
with completely different answers by the
way one thing that you might do just to
give you an example of just how much
range there is here is rather than doing
a simple vote by counting you could do a
vote that is say weighted by how far
away you are so we could have a weighted
vote that might help us with ties that
could help with ties and you could do a
weighted average yes right so you're
basically saying that the Y values that
correspond to X values that are closer
to the query point have more of an
influence on the mean which makes some
sense right no I think it makes a lot of
sense so how would you weight that by
the way what would you do I'd weighted
by the similarity right so well in this
case the similarity is we have a
distance not a similarity so you'd have
to you know weighted by something like 1
over the distance oh I see okay that
seems like a hack sure but it's a hack
that sort of makes sense ok ok so anyway
simple algorithm lots and lots of
decisions to make here all of which
could in principle have a pretty big
effect and so in order to see that I
want to do two quizzes that I hope get
to the heart of this and maybe give us a
little bit of insight into how some of
these decisions might matter on the one
hand and exactly just how simple or not
simple this algorithm turns out to be ok
awesome</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>