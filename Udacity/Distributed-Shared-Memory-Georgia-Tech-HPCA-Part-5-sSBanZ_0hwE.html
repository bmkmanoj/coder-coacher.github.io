<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Distributed Shared Memory - Georgia Tech - HPCA: Part 5 | Coder Coacher - Coaching Coders</title><meta content="Distributed Shared Memory - Georgia Tech - HPCA: Part 5 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Distributed Shared Memory - Georgia Tech - HPCA: Part 5</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sSBanZ_0hwE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the next organization that we will be
looking at is similar to distributed
shared memory except that we are no
longer sharing memories it's called
distributed memory no sharing of memory
means that only one core can access a
memory slice and the others cannot so
what we now have is cores that have
their own caches and each of them has
its own memory that can only be locally
accessed here so really each of the core
has pretty much what looks like a
complete single core computer system and
a network interface card that connects
it to a network but now when a cork has
a cache miss that cache miss goes
directly to this memory and if the core
wants to access something that is in
another course memory it cannot simply
issue an access that misses in the cache
and goes there now what it needs to do
is actually create a network message
using some sort of a send primitive in
the operating system to actually send a
request the program here needs to
receive that see what is being requested
respond to it and so on so now
communication is explicit it's no longer
sufficient to simply put the data in
memory and then another gorgeous reads
it from there you have to actually send
the data explicitly to another core if
we want to communicate so that means
that you write programs differently from
this symmetric shared memory and
distributed shared memory pass data
around using shared memory meaning reads
and writes to memory are used to
exchange data now we are using what is
called message passing for communication
so now you're pretty much writing a
program as if these were independent
machines that communicate over a network
if you have a distributed memory
supercomputer for example it's just that
this network and these network cards are
a lot faster than your normal Ethernet
connection this type of a system is also
called a multi computer because really
each of these is like a complete
pewter it has the processor the cache
the memory some IO and the processor
only directly accesses the local memory
so it looks like a complete unit
processor system these are also called
cluster computers because really you put
a bunch of normal computers together
into a tightly networked cluster and you
get something like a distributed memory
system these types of computers tend to
scale to a large number of processors
the reason is not that they are
fundamentally better at communicating
than shared memory systems
the reason is mainly that the programmer
is forced to explicitly think about
communication because you use a
different type of primitive to
communicate then you use to access your
local memory so the programmer is aware
of communication going on and then
naturally they will tend to minimize
communication and if at all possible
access the local memory so this approach
works better primarily because it forces
the programmer to figure out things that
otherwise it might not notice because it
forces the programmer to figure out how
to communicate efficiently
whereas in shared memory the programmer
may not even realize that some of the
lattices are not local and thus are slow
and causing a lot of communication to
happen</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>