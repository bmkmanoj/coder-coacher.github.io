<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>TD(0) Rule | Coder Coacher - Coaching Coders</title><meta content="TD(0) Rule - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>TD(0) Rule</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/G2D0Quzpsq4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so here is a rule we're gonna
call the td0 rule which gives it a
different name from td1 but eventually
we're going to connect those you're on
the one together and the rule looks like
this not so unfamiliar looking that the
way we're going to compute our value
estimate for the state that we just left
when we make a transition at epoch T you
know for trend trajectory t big t is
what the previous value was plus we're
going to move a little bit in the
direction with our learning rate toward
what toward the reward that we observed
plus the discounted estimated value of
the state that we ended up in minus the
estimated value of the state that we
just left so does this look familiar
it's got these and R's and Alpha's in it
well sure all right but let's be maybe
we can be a little more precise about
this so what what would we expect this
outcome to look like on average right so
the thing that's random here at least
the way we've been talking about it is
if we were in some state s t minus 1 and
we make a transition we don't know what
state we're gonna end up in but there's
some probabilities of those so basically
this update is being done more often in
the states that we end up in more often
than the more common STS and less often
in the STS that are less common they're
less likely and so really if we take the
expectation of this what is it going to
look like if we repeat this update over
and over again
what really doing is sampling different
possible s T values all right so really
we're taking an expectation over what we
get as the next state of the reward plus
the discounted estimated value of that
next state that seems right that's kind
of what we want yeah so this is exactly
what the maximum likelihood estimate is
supposed to be as long as these
probabilities for for what the next
state is going to be match what the data
has shown so far as the as the
transition to s T is that that blue
stuff above the equations is about yeah
so here's the idea is that if we repeat
this update rule on the finite data that
we've got over and over and over again
then we're actually taking an average
with respect to how often we've seen
each of those transitions so it really
is computing the maximum likelihood
estimate so this does the right thing ok
over over the fight
so that finite data thing is important
because if we had infinite data then
everything would work so right in the
infinite data case this is also true but
also td1 does the right thing in
infinite data kind of everything does
the right thing in Internet data
yeah everything's getting all averaged
out and it'll do the right thing but but
here we've got it we've got a finite set
of data so far and the issue is that if
we run our update rule over that data
over and over and over again then we're
gonna get the effect of having a maximum
likelihood model and that's not true of
the outcome based model because in the
like in the data that we just saw where
we were only saw one transition from s2
to anything else we can run over that
over and over and over again but the
estimates not gonna change it's always
going to be exactly that okay okay sure
sure sure about that we can contrast'
this with the outcome based idea where
we're not doing this sort of
bootstrapping we're not using the
estimate that we've gotten at some other
state we actually use literally the
reward sequence that we saw and so as a
result of that if we've only seen a
reward sequence once like like in the
case of s2 repeating that update over
and over again doesn't change anything
right sure sure cuz it right because the
expectations the expectation right
whereas in here we're actually using the
intermediate estimates that we've
computed and refined on all the
intermediate nodes on all the states
that we've encountered along the way to
improve our estimate of the value of
every other state right so this kind of
is more self-consistent right it's
actually kind of connecting the values
of states to the other values of the
states which is what you'd want and this
is more just you know literally using
the experience that it saw and ignoring
the fact that there's these intermediate
states okay well when you put it that
way it makes it sound like you should
always use td0 and never use td1 yes but
but that's oversimplifying okay can you
under scim pool by it we'll get to that
okay good I look forward to it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>