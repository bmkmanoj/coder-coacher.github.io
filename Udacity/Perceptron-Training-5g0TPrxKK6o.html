<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Perceptron Training | Coder Coacher - Coaching Coders</title><meta content="Perceptron Training - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Perceptron Training</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5g0TPrxKK6o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright so in the examples up to this
point we've been setting the weights by
hand to make various functions happen
and that's not really that useful in the
context of machine learning we'd really
like a system that given examples fines
weights that math the inputs to the
outputs and we're going to actually look
at two different rules that have been
developed for doing exactly that
figuring out what the weights ought to
be from training examples one is called
the perceptron role and the other one's
called gradient descent or the Delta
rule and the difference between them is
the perceptron rule is going to make use
of the thresholded outputs and the other
mechanism is going to use unthread
shouldered values all right so what we
need to talk about now is the perceptron
rule for which is how to set the weights
of a single unit so that it matches some
training set so we've got a training set
which is a bunch of examples of X these
are vectors and we have y's which are
zeros and ones which are the the outputs
that we want to hit and what we want to
do is set the set the weights so that we
capture this this same data set and
we're going to do that by modifying the
weights over time oh Michael what's the
Y a series of dashes over on the left oh
sorry
right I should mention that so one of
the things that we're going to do here
is we're going to give a learning rate
for the weights W and not give a
learning rule for the theta but we do
need to learn the theta so there's a
there's a very convenient trick for
actually learning them and by just
treating it as another kind of weight so
if you think about the way that the the
threshold inquiry taking a linear
combination of the WS and the exes then
we were comparing it to theta but if you
think about just subtracting theta from
both sides then in some sense theta just
becomes another one of the weights and
we're just comparing to zero so what I
did here is I took the actual data the
X's and I added what is sometimes called
a bias unit to it so basically the input
is one always to that and the weight
corresponding to it is going to
correspond to negative theta ultimately
so just just again this just simplifies
things so that the threshold can be
treated the same as the weights so from
now on we don't have to worry about the
threshold it just gets folded into the
weight
and all our comparisons are going to be
just to zero instead of 'some instead of
theta it's any trick yeah it certainly
makes the math shorter so okay so this
is what we're going to do we're going to
iterate over this training set grabbing
an X which includes the bias piece and
the Y where Y is our target X is our
input and what we're going to do is
we're going to change weight I the the
weight corresponding to the ayat' by the
amount that we're changing the weight by
so this is sort of a tautology right
this is really just saying the amount we
change the weight by is exactly the
Delta W in other words the amount we
change the weight by so we need to
define that what that weight change is
the weight change is going to be defined
as follows we're going to take the
target the thing that we want the output
to be and compare it to what the network
with the current weight actually spits
out so we compute this this Y hat this
approximate output Y by again summing up
the the inputs according to the weights
comparing it to zero that gets us a 0-1
value and so we're now comparing that to
what the actual value is so what's going
to happen here if they're both zeros so
let's let's let's look at this each of Y
and y hat can only be 0 and 1 if they're
both zeros then this Y minus y hat is 0
if they're both ones and that what does
that mean it means that the output
should have been 0 and the output of the
of our current network really was 0 so
that's that's kind of good if they are
both ones it means the output was
supposed to be 1 in our network
outputted 1 and the difference between
them is going to be 0 but in this other
case Y minus y hat if the output was
supposed to be 0 but we said 1 our
network says 1 then we get a negative 1
if the output was supposed to be 1 and
we said 0 then we get a positive 1 okay
so those are the four cases for what's
happening here we're going to take that
value multiply it by the current input
to that unit I scale it down by this
little thing that is going to be called
the learning rate and use that as the
the weight update change so essentially
what we're saying is if the output is
already correct either both on or both
off then there's going to be no change
to the weights but if our output is
wrong
let's say that we are giving a one when
we should have been giving a zero that
means our the total here is too large
and so we need to make it smaller how
are we going to make it smaller
whichever input excise correspond to
very large values we're going to move
those weights very far in a negative
direction taking this negative one times
that value times this this little
learning rate all right the other case
is if the output was supposed to be 1
but we're outputting a zero that means
our total is too small and what this
rule says is increase the weights
essentially to try to make the sum
bigger now we don't want to kind of
overdo it and that's what this learning
rate is about learning rate basically
says well figure out the direction we
want to move things and just take a
little step in that direction we're
going to keep repeating over all of the
the input/output pairs so we'll have a
chance to get in to really build things
up but we're going to do it a little bit
at a time so that we don't overshoot and
that's the rule it's actually extremely
simple like you're actually writing this
in code is is quite trivial and and yet
it does some remarkable things so let's
imagine for a second that we have a
training set that looks like this it's
in two dimensions just again so that
it's easy to visualize that we've got a
bunch of positive examples these Green
X's and we've got a bunch of negatives
ampuls these red X's and we're trying to
learn basically a half plane right we're
trying to learn a half plane that
separates the positive from the negative
examples so Charles do you see a half
plane that we could put in here that
would do the trick I do what would it
look like that one by that what do you
mean this one yeah
that's exactly what I was thinking
Michael that's awesome yeah there
actually isn't a whole lot of
flexibility in what the right answer is
in this case if we really want to get
all the greens on one side and all the
Reds on the other if there is such a
half plane that separates the positive
from the negative examples then we say
that the data set is linearly separable
right that there is a way of separating
the positive negatives with a line and
what's cool about the perceptron rule is
that if we have data that is linearly
separable the perceptron rule will find
it it only needs a finite number of
iterations to find it in fact
which I guess is really the same as
saying that it will actually find it it
won't eventually get around to getting
to something close to it it will
actually find a line it will stop saying
okay I now have a set of weights that
that do the trick so that's what happens
if the data set is in fact linearly
separable and and that's pretty cool
it's pretty amazing that it can do that
it's a very simple rule and it just goes
through and iterates and and solves the
problem so Charles do see a neat and
solves the problem so I can think of one
what if it's not linearly separable hmm
I see so that if the data is linearly
separable then the algorithm works
so the algorithm simply needs to only be
run when the data is linearly separable
it's not generally that easy to tell
actually when your data is linearly
separable especially here we have it in
two dimensions if it's in 50 dimensions
knowing whether or not there's a setting
of those parameters that makes it
linearly separable not so clear well
there's one way you can do it what's
that you could run this algorithm and
see if it ever stops I see yes of course
there's a problem with that particular
scheme right which says well for one
thing this algorithm never stops so wait
we need to we need to address that but
but we really we should only be running
this loop here while there's some error
so I neglected to say that before but
what you'll notice is if you continue to
run this after the point where it's
getting all the answers right it's found
a set of weights that linearly separate
the positive and negative instances what
will happen is when it gets to this
Delta W line that Y minus y hat will
always be zero the weights will never
change we'll go back and you know update
them by adding zero to them repeatedly
over and over and over again so if it
ever does reach zero error if it ever
does separate the data set then we can
just put a little condition in there and
tell it to stop the loop so what you're
suggesting is we could run this
algorithm and if it stops then we know
it's linearly separable and if it
doesn't stop then we know that it's not
linearly separable right by this
guarantee here the problem is we we
don't know when finite is done right if
this were like a thousand iterations we
could run it for it
if it wasn't done it's not done but all
we know at this point is that it's a
finite number of iterations and so that
could be a thousand ten thousand a
million 10 million we don't know so we
never know when to stop and declare the
data set not linearly separable so if we
could do that then we would have solved
the halting problem and we'd all have
Nobel prizes well that's not necessarily
the case but it certainly the other
direction is true that if we could solve
the halting problem then we could solve
this but it could be that this problem
might be solvable even without solving
the halting problem fair enough okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>