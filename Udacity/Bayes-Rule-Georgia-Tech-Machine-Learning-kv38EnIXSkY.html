<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bayes Rule - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Bayes Rule - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bayes Rule - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kv38EnIXSkY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right Michael so like I said we're
gonna spend all this time trying to
unpack this particular equation and the
first thing we need to do is we need to
come up with another form of it that we
might have some chance of actually
understanding of actually getting
through so I want to use something
called Bayes rule do you remember Bayes
rule I do
okay what's Bayes rule the man with the
Bayes makes the rules oh wait no that's
the golden rule the Bayes rule is that
is it relates it I don't know I think of
it as just letting you switch which
thing is on which side of the bar okay
so just give the whole expression yeah
give me this person so if we're gonna
apply Bayes rule to the probability of H
given D we can move turn it around and
make it equal to the probability of D
given H and it would be great if we
could just stop with that but we can't
we have to now kind of put them in the
same space so we multiply by the
probability of H and then we divide by
the probability of D and sometimes
that's just a normalization and we don't
have to worry about it too much but
that's that's the bait that's Bayes rule
right there so this is basis rule and it
actually is really easy to derive it
falls it follows directly from the chain
rule in probability theory do you think
it's worthwhile showing people that or
just you might be able to just see it
just the thing on top of the
normalization probability of D given H
times probably of eight that's actually
just the probability of D and H together
right so the probability of H time
they're probably D given H as you say if
the chain rule tells us basically the
definition of the conditional
probability in conjunctions and if you
move the probability of D over to the
left-hand side you can see we're really
just saying the same thing two different
ways it's just the probability of H and
D so then we're done no that's right so
I could write down what you just said
and use different letters just to make
it more confusing so okay you can point
out that the probability of a and B by
the chain rule is just the probability
of a given B times the probability of B
but because order doesn't matter it's
also case that the probability of a and
B is the probability of B given a time's
the probability of a and that's just the
chain rule and so if these two quantity
is equal to one another is exactly what
you say I could say well the probability
of a given B is just the probability of
given a time's the probability of a
divided by the probability of B and
that's exactly what we have over here
okay so now that we've mastered that all
your base are belong to us how long have
you been waiting to say that they're
just only about three or four minutes
fair enough okay so we have basis rule
and what's really nice about Bayes rule
is that well it's a very simple thing
it's also true it follows directly from
probability theory but more importantly
from machine learning it gives us a
handle to talk about what it is we're
exactly trying to do when we say we're
trying to find the most probable
hypothesis given the data so let's just
take a moment to the thing about what
all these terms mean we know what this
term here means the it's just the
probability of some hypothesis given the
data but what do all these other terms
mean I want to start with this term the
the probability of the data it's really
nothing more than your prior belief of
seeing some particular set of data now
and as you point out Michael often this
just ends up to be a normalizing term
and typically does not matter though
we'll see a couple of cases where it
does matter helps us to sort of to think
about a few things but generally
speaking whatever it is since the only
thing that we care about is the
hypothesis and we're trying to find that
the probability of the data doesn't
depend on the hypothesis so typically we
ignore it but it's nice to just be clear
about what it means the other terms are
a bit more interesting they matter a
little bit more this term here the
probability is the probability of data
given the hypothesis right which seems
like learning backwards it does seem
like learning backwards but what's
really nice about this quantity is that
unlike the other quantity the
probability the hypothesis given the
data it's actually turns out to be
pretty easy to think about the
likelihood that we would see some data
given that we were in a world where some
hypothesis H is true so there's a little
bit of a subtlety there and and I and
let me let me unpack that subtlety a
little bit so we've been talking about
the data as if it's it's sort of thing
that's floating out in the air but we
know that the data is actually our
training data and it's a set of inputs
and let's just say for the sake of
argument we're going to do
classification learning it's a set of
labels that are associated with those
inputs so just to drive the point home
I'm going to call those D little DS and
so our data is made up of a bunch of
these training exam
and these training examples are whatever
input that we get coming from a teacher
coming from ourselves coming from nature
coming from somewhere and the associated
label that goes along with them so when
you talk about the probability of the
data given hypothesis what you're
talking about well what's the likelihood
that given that I've got all of these
excise and given that I'm living in a
world where this particular hypothesis
is true that I would see these
particular labels does that make sense
Michael I see yes so so I could imagine
a more complicated kind of notation here
where we're kind of accepting the X's is
given yep but the labels is what we're
actually I see yeah the labels are we're
actually saying is is something that we
want to assign probability to right so
it's not really that the X's matter in
the sense that you know that we're
trying to understand those what really
matters is the labels that are
associated with them and we'll see we'll
see an example of that in a moment but I
just want to make certain that you get
this subtlety so in a sense then I guess
you're saying that the probability of D
given H component or quantity is really
like running the hypothesis it's like
it's like labeling the data
okay Michael just may certainly get this
let's imagine we're in a universe where
the following hypothesis is true it
returns true in exactly the cases where
some input number X is greater than or
equal to 10 and it returns false
otherwise okay yep okay so here's a
question for you let's say that our data
was made up of exactly one point and
that value set X equal to seven okay
what is the probability that the label
associated with seven would be true so
you're saying we're in a world where H
is holding and that H H is what's being
used to generate labels so it wouldn't
do that right so the probability ought
to be zero that's exactly right and
what's the probability that it would be
false one minus zero but you'll call one
which will call one that's exactly right
so it's just that simple that the
probability of the data given the
hypothesis is really about given a set
of X's what's the probability that I
would see some particular label now
what's nice about that is as you point
out is that it's as if we're
running the hypothesis well given
hypothesis it's really easy
or at least it's easier usually to
compute the probability of us seeing
some labels so this quantity is a lot
easier to figure out than the original
quantity that we're looking for the
probability of the hypothesis given the
data yeah I could see that it's sort of
reminding me a little bit of the version
space but I can't quite crystallize what
the connection is well that's good you
bring that up because I think in a
couple of seconds I'll give you an
example that might really help you to
see that okay okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>