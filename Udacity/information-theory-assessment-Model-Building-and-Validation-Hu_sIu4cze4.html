<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>information theory assessment - Model Building and Validation | Coder Coacher - Coaching Coders</title><meta content="information theory assessment - Model Building and Validation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>information theory assessment - Model Building and Validation</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Hu_sIu4cze4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">recall that if we have a random variable
X and we want to understand its
relationship with another random
variable V the way we do that is we
examine the expected loss and entropy
due to that variable specifically to
understand the relationship between x
and y we examine the expected loss and
entropy due to Y similarly for Z we do
exactly the same thing we examine the
expected loss and entropy due to Z after
we know these two quantities the feature
that we want is the one that results in
the biggest drop from H of X so as you
can see in order to do this we'll have
to calculate three quantities H of X the
conditional entropy the entropy of X the
conditional entropy of x given Y and the
conditional entropy of x given Z I've
written them here over off to the side
so that we can keep track of them as we
do our calculations let's go ahead and
find them so H of X is just this
expression minus one times the
probability of x times log of
probability of X minus the probability
of X 2 times the log of probability of X
2 now remember the base of the logarithm
doesn't really matter traditionally
either base 2 or base E is used let's
assume for these calculations we're
using base E so we can just pull the
values for the probability of x1 and x2
straight from the joint probability
table and if we do that we find we get
minus 1 times 0 point four seven two
times log of 0 point for 72 minus 0.5 28
times log of 0 point 5 to 8 which is
approximately point 691 so let's just
keep track of that by writing it up here
so one of three down now we just have to
do h of x given y + h of x given Z
recall that the conditional entropy of h
of x given Y is just a conditional
expectation it is the average of how
much we expect H of X to change due to
y1 and due to y2
so the first term is just this
expression below so first so first let's
tackle h of x given y 1 h of x given y 1
is just minus 1 times the probability of
x1 given y 1 x log of probability x 1
given y 1 minus probability of x 2 given
y 1 times the log of probability of x 2
given y 1 and again we can just pull
these numbers straight out of the joint
probability table above and if we do
that we find we get minus 1 times 0
point 0 3 18 times log point 0 3 0 18
minus 0 point 7 7 9 times log point 779
and that's approximately 0 point three
or four three let's just make a note of
that here the calculation for h of x
given y 2 proceeds in exactly the same
way and if you evaluate that you get
approximately 0 point six eight eight
eight and let's just make a note of that
here so if we put it all together h of x
given Y is 0 point 3 2 0 75 x 0 point
three or four three plus OH point 6 7 9
25 x 0 point six eight eight eight which
is approximately point five six six
let's make a note of our second term
above now the calculation for the
conditional entropy of x given Z
proceeds in exactly the same way I'm not
going to step through the calculation
for this one as well I'll leave that to
you but you'll find that if you do you
get approximately 0 point five 787 so
now we have all three of our terms
before knowing anything about y or z h
of x is 0 point 691 it drops to 0 point
566 when we conditioned on Y and it
drops the point 5787 when we conditioned
on Z so recall that the feature that we
want is the one that results in the
biggest change in entropy which is the
re/max of h of x minus h of x given a
variable we're in this case the
variables range / y and z so you can see
that in this case the answer is why
although not by that much this
difference for y is point one
five whereas for Z it's just slightly
less at point one one two three so
although why is the winner here Z
doesn't do that much worse and so in
practice if I were going to build a
model in order to understand acts I
might consider including both y and z
but if I had to choose one why is the
winner</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>