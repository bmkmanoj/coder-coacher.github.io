<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>More on VI Part Two | Coder Coacher - Coaching Coders</title><meta content="More on VI Part Two - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>More on VI Part Two</b></h2><h5 class="post__date">2015-06-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/14PQQLNjHZs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so here's another thing that we know
that is if we run value iteration we get
a series of value functions save you
know VT if we compare the value function
we have it sums time step T to the same
value function that we have one time
step later VT plus one that if it if it
basically has stopped changing if the
change the the amount of change from VT
sorry from t to t plus 1 is less than
epsilon for all states in the MVP then
the largest difference between the
optimal value function and the value
function that we get sorry for this but
by taking that value function taking the
greedy policy with respect to that value
function and taking the value function
of that greedy policy is small right so
we're going to be doing almost optimal
from all states and the smallness here
ends up being two times that epsilon
times gamma divided by one minus gamma
are you sure that's intuitively obvious
well maybe not intuitively obvious but
the sing and ye proof of this is not so
complicated not so different from the
stuff that we've been doing and you get
the one over one minus gamma because
this little bit of epsilon that we could
be off could get magnified over every
single step as we think about this into
the future that's why we get a 1 minus
gamma in the denominator and then you
know to epsilon because I don't know
which side of the epsilon were on times
gamma because it doesn't take place
right away it's one step from now so it
does kind of make sense but the the real
important upshot of this is the notion
that if you can get a good enough
approximation of the value function then
you're going to have a good enough well
then you can at least put a bound on how
far off you are in terms of following
the resulting policy from how far that's
going to be from optimal and furthermore
we don't even need to know how close we
are to v star all we need for this
result to hold is we look at two
consecutive iterations of value
iteration right which that we have you
know we're running valuation we know
what it was a step ago we know what it
is now and so we can compute this
epsilon and that gives us a bound on
what if we just stop now and follow that
policy so that's that's a really useful
thing otherwise you know knowing that
valuation converges in the limit is not
super duper helpful because you're never
going to get to the limit and knowing
that value duration eventually is
optimal even if
night is still not that helpful because
you don't know when that takes place so
what this is telling us is we can
actually test on our own when is a
decent time to stop so that's cool so
here's something I just noticed if I
look at both your one and two I noticed
that they both would encourage you to
pick small gammas mmm right yes it is
certainly the case that if gamma is
really tiny like gamma is 0 then this
bounced a small this bound stays small
it's all good The Closer gamma gets to
one the more things are problematic but
what do you infer from that well mainly
why is that well that's because gamma
really tells you what your horizon is
and so the smaller gamma is to sort of
further into the future you don't look
that makes sense and so that makes sense
then write that if I'm not really
worried about the future then it doesn't
take much for me to be doing what
appears to be optimal yes that's right
right it's easy to optimize over a very
short horizon and the horizon the effect
of horizon is always something like 1
over 1 minus gamma that's kind of how
far into the future do rewards actually
matter right and so right so so gamma
closer to one is really looking ahead a
lot further and that's why it's a harder
problem to do it's harder to get things
exactly right so then why in practice
don't we always set gamma to be really
really small yeah that's where I thought
you were gonna go so it's a good
question I mean one of the things that
happens if you set gambit to be
something very close to zero is the
agent who's acting in the world acts
very myopically fairies like short-term
thinking so um you know you know I feel
like eating now instead of finishing
recording this lecture so ah I might as
well just do that right so you get
shortsightedness I am eating though well
that's good for you I am NOT eating now
I would like to eat now but I won't
because I'm sufficiently far sighted to
know that just getting something yummy
to eat right now is going to interfere
with my ability to finish this course
because are you know a wonderful and
talented video editor eventually is
going to stop processing this stuff and
and then we're
than were hosed so yes gamma smaller
gamma means easier problem but more
short-sighted behavior closer to 1 gamma
is looking ahead more into the distance
but can make the computations more
difficult so it's a it's basically but
these two results are saying is that
there's a trade-off between horizon and
sort of feasibility of solving a problem
in a reasonable amount of time yeah
that's a good way to say it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>