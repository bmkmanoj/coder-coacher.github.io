<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Good Features to Match Between Images | Coder Coacher - Coaching Coders</title><meta content="Good Features to Match Between Images - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Good Features to Match Between Images</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/W3yu7ziXfNE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">So now let's start thinking about what are Good Features that we can find
between two images of the similar scene or a same object, that we can then
use to be able to register that same feature for across two different images.
I'm going to use this simple object of a water bottle to
help us illustrate this.
So as I've said before, we are interested in extracting features.
Basically, we want to find parts of an image that lets us encode that image in
a compact form, so then I can do comparisons from one image to the other.
I'm going to propose the concept of Edges here.
Edges on an image, in a variety of ways,
basically could be one way of encoding such an information.
So what kinds of information do we want to look at?
Basically what we want to look for
is various forms of discontinuities in an image or a scene, and
that basically is one of the ways to think about what an edge is.
So, an edge in an image basically is trying to capture the discontinuity in
a scene, and use that to be able to then find information that would be again,
somewhat repeatable across images.
We'll look at these more in detail.
So let's look at, for example, this water bottle.
And see what kind of changes in the scene we could actually look for.
And actually look for certain discontinuities in this.
So, for example, at the water bottle at the top of it you basically if I was to
start growing surface normals,
you would basically see the surface normals going this way.
And, after a while,
there are no more surface normals because they are on the other side.
So, this form of discontinuity, if there was a sudden change in it,
would allow us to start thinking about how we could actually look for
discontinuities of this form and the surface normals.
So those would be surface normal discontinuities.
Let's look at depth discontinuities.
In this case, of course, we know there's an object of course, and it might be in
front of something, so by just being able to look at the, you know, the sides
here, you know that anything behind that would be at a different depth.
This would be in front of everything behind that.
So that would be a depth discontinuity.
How can we actually look at the term water here, and be able to separate it out?
Well, in this case, if you can notice is,
water is written with a different color than the background.
So surface color is basically also a cue, a discontinuity that we can look at,
that basically points out how we can actually separate out terms like water, or
even this band here.
Or, in fact, even looking at how we can take difference between the bottle and
the cap here.
Another form of discontinuity is illumination.
This object, of course, is being lit in different ways.
And, of course it, creates a shadow.
Or it could actually have other types of reflections and specularities and stuff
like that, that are also coming in because of the lighting conditions on it.
That basically is referring to discontinuities because of lighting changes and
such on the object.
So hopefully you see now, that by just looking at various types of
discontinuities, we can start extracting some information about an image.
So of course, what we now want to do is go back to the image itself and look for
certain sets of discontinuities that would basically best capture the changes
because of the normals the depth, surface color, and illumination.
Of course, that is a rather big problem and
we should be looking at it, and this is one of the bigger things we look at in
computer vision when we do analysis of scenes.
Now one thing I wanted to emphasize here,
this is an extremely important concept.
The Edges basically are encoding change in a spatial way of looking at an image.
Anywhere afterwards I look for a change, and
that chain encodes a lot of information.
And in fact some may argue that this is a very important set of
informations about an image.
So in a information theoretic manner,
what we an actually say that edges and basically are encoding change, and
therefore are an efficient way to encode an image.
So basically any time I can think about an image is,
if I can start looking at where the changes are, that basically starts giving me
a lot of information about what the image would be,
because now I've actually taken a a differential code to represent an image.
Wherever there are changes, I'm going to highlight those.
We'll look at that a little bit more carefully in a bit.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>