<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Still Support Vector Machines - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Still Support Vector Machines - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Still Support Vector Machines - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-ePtSxkmD4g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so we're still talking about
support vector machines although I
haven't told you what support vector
machines are yet we're getting there
Michael bear with me and what we got
from our last discussion is that what we
want to do somehow is maximize a
particular equation that is 2 over the
length of W and as a reminder W of the
parameters of our hyperplane so somehow
we want to maximize that equation
subject to the constraints that we still
classify everything correctly ok so we
want to maximize 2 over the length of W
while classifying everything correctly
but while classifying everything
correctly is not a very mathematically
satisfying expression but it turns out
we can turn that into a mathematically
sine a satisfying expression and let me
show you how to do that so here's a
simple equation while classifying
everything correctly turns out to be the
same as and I'm just going to write I'm
going to write it out for you Michael
and see if you can you can guess why
this works so what I've written here is
y I times w transpose X I plus B greater
than or equal to 1 for all I that is for
all of our training data examples so
what is this work what we really want is
that the class that linear classifier WT
X I plus B is greater than equal to 1
for the positive examples and less than
or equal to negative 1 for the negative
examples but you cleverly multiply by
the label on the left hand side which
does exactly that if Y I is 1 it leaves
it untouched and if Y is negative 1 it
flips everything around so that we're
really talking about less than or equal
to minus 1 that's that's very clever it
is very clever and I'm going to pretend
that I came up with that idea myself so
it turns out that trying to solve this
particular problem
maximizing 2 over W while satisfying
that constraint is a little painful to
do but that we can solve an equivalent
problem which turns out to be much
easier to do and that is this problem
that is rather than trying to maximize 2
over the length of W we can instead try
to minimize 1/2 times W squared now can
you see that those will always have the
same answer
yeah so what not the same answer but it
will be minimum the the point that
maximizes one will minimize the other
yeah because the we took the reciprocal
as long as we're talking about positive
things and since these are lengths
there'll be positive
taking the reciprocal exactly you know
changes the direction of what the answer
is and the squaring is is makes it
monotone it doesn't it doesn't it
magnifies it but it doesn't change the
ordering of things so yeah that that
that seems fine I don't know why that's
any easier but it seems the same well do
you want to know why it's easier because
I'll tell you please this is easier
because when you have an optimization
problem of this form something like a
minimizing a W squared subject to a
bunch of constraints
that's called a quadratic programming
problem and people know how to solve
quadratic programming problems in
relatively straightforward ways awesome
now what else is nice about that is a
couple of things one is it turns out
that these always have a solution and in
fact have a unique solution now I'm not
going to tell you how to solve quadratic
programming problems because I don't
know how to do it other than to call it
up in the MATLAB but there's a whole set
of classes out there where they teach
you how to do quadratic programming we
could take an aside I can learn all
about quadratic programming and then we
could talk about it for two hours but
it's really beside the point the
important thing is that we have defined
a specific optimization problem and that
there are known techniques that come
from linear algebra that tell us how to
solve them and we can just plug and play
and go okay okay fair enough okay fair
enough so in particular it turns out
that we can transform again this
particular quadratic programming problem
into a different quadratic programming
problem or actually truthfully into the
normal form for a quadratic programming
problem that has the following form so
here's what this equation tells you
Michael we basically started out by
trying to maximize the margin and that's
the same thing as trying to maximize 2
over the length of W I think I convinced
you of subject to a particular set of
constraints which are how we codify that
we want to classify every data point
correctly in the training set we've
argued that's equivalent to minimizing
one half time
length of W squared subject to the same
constraints and then notice because we
happen to know this that you can convert
that into a quadratic programming
problem which we know how to solve and
it turns out that quadratic programming
problem has a very particular form
rather than trying to minimize one-half
of W squared we can try to maximize
another function that has a different
set of parameters - I'll call alpha and
that equation has the following form
it's the sum over all of the data points
I indexed by I of this new set of
parameters alpha minus one-half times
for every pair of examples the product
of their alphas their labels and their
values subject to a different set of
constraints namely that all of the
alphas are non-negative and that the sum
of the product of the alphas and the
labels that go along with them are equal
to zero holy cow now it's so obvious how
you get from one step to the other I'm
not going to bother to explain it to you
but instead tell you to go read a
quadratic programming book what I really
need you to believe though mainly
because I'm asserting it is that these
are equivalent so if you buy up to the
point that we're trying to maximize the
margin and that's the same thing as
maximizing - or the length of W and you
buy that that's the same as trying to
minimize one-half times W squared then
you just have to take a leap of faith
here that if we instead maximize this
other equation it turns out that we're
solving the same problem and that we
know how to do it using quadratic
programming or other people know how to
do it and they've written code for us
okay all right all right so trust me on
this this is what it is that we want to
solve now it turns out that we can run
little programs to solve this and you
end up with answers but what's really
interesting is what this equation
actually tells us about what we're
trying to do so let me just show you
this let me just talk a little bit about
the properties of this equation and a
property of the solutions to this
equation for a second okay okay so let
me move a few things around so that we
can look at</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>