<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Basic Update Rule | Coder Coacher - Coaching Coders</title><meta content="Basic Update Rule - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Basic Update Rule</b></h2><h5 class="post__date">2015-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vVDKzIxzkzQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I think it will help if we think
through what the basic update rule is
when you're doing reinforcement learning
or q-learning and then to try to adapt
it to this notion that are we're
representing our our value function
using some kind of function approximator
alright so here's a way to think about
this we've got our Q function that we're
going to try to represent as some
function of a set of parameters or wait
maybe let's say one for one set of
parameters for each action sure and what
f is doing is taking a state and
translating it into a set of features
we're going to feed those features and
the parameters of the function into
something called f and what will come
out of that will be the Q value so this
is intended to be fairly general and
we'll talk about some specific examples
of this in a moment but but so let's
just imagine that with Q is being
represented by some kind of function
approximator and here's how Q learning
works we have an experienced couple that
comes in state action reward next state
and then we do an update now what's the
update usually it's it's we try to move
the the value of that state action pair
a little bit towards the reward plus the
discounted value of the next state s
prime so that that's the bellman
equation kind of hidden in here and the
difference between that and the current
Q value is the TD error and what do we
want to do with the TD error we want to
the TDR tells us in our current
prediction too high to low just right
you know does it like porridge and what
we want to do is change that's the three
bears i guess but but the fact that
matter is we actually need to know the
kind of the direction that we should be
going or we are we too high or too low
and we want to move the parameters that
represent the Q function you know so a
given weight say W super a sub I we want
to move it a little bit this is a
learning rate alpha in the direction of
that TD error but how much do we move
the weight it depends on how that weight
actually influences the Q function right
so what's the partial derivative of the
Q value for that state action pair as a
function of that particular weight that
we want to do we want to change and
we're going to change that weight
proportionate we're gonna change things
proportionately to that to that gradient
so this should look kind of familiar
does this look familiar yeah it looks
exactly like the you know we drove off
update rule
things that we do with perceptrons and
it's a yeah it's a normal kind of
gradient update rule yeah right exactly
so right and so when you're when you're
representing a function in terms of a
set of parameters we want to know how
did the parameters impact the value that
we're predicting and let's move those
parameters in a direction that makes the
prediction more accurate so this is this
is a way we can actually view q-learning
as an update rule for underlying
parameters of a function approximator
yeah and I guess the only difference is
that unlike the sort of Y star minus y
thing that we used to do this is what we
got on this particular step so that's
what makes a TD air as opposed to you
know actual error that makes any sense
does that make sense yeah I think that's
exactly right so so when we're thinking
about supervised learning what what's
happening is we have some kind of output
for our current input and we call that Y
and then we have a target output Y star
and we want to change the parameters of
the function so that instead of getting
something like why we get something much
more like Y star so I think I think what
your point is is that this bellman
equation kind of value and this kind of
current prediction do play the roles of
Y star and why so this I think is a
really good analogy but one way that
it's different is that y star when we're
doing supervised learning is actually
the right value right it's coming from
some kind of trusted source and it is
the label here we're bootstrapping which
i think is a kind of wrapping and what
we're doing with bootstrapping is we're
using our current predictions as a kind
of way of making a target for what our
other predictions should be moving
toward so this this should look a little
bit unnerving but it's it's not a bad
idea but it's also not clearly a great
idea because we're instead of using real
label data we're actually making up our
own labels using the the current
prediction from the Q function right and
we'll see sometimes this can really run
us into trouble and we have to be a
little bit careful to make sure that it
doesn't okay all right so this is this
is in a very general form I think it
would be helpful though to make it make
more sense if we dive in and think about
some particular representations of the Q
function okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>