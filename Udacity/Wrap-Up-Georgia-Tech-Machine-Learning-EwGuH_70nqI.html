<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Wrap Up - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Wrap Up - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Wrap Up - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EwGuH_70nqI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Michael so we've gone on a journey
of discovery through unsupervised
learning and so my question to you is
what have we learned I think we learned
a little bit about ourselves and a
little bit about America so what aids
have we learned today Michael so there
was PCA mm-hmm I ca mm-hmm lda mm-hmm
RCA on USA USA we're number one whoo
okay we're just gonna race that little
bit okay yeah ok so we learned about a
lot of aids today which is the same
grade that all our students are going to
get I'm sure at the gate or that's if
they're truly independent if they aren't
independent then the central limit
theorem says there will be a normal
distribution of crossroads ring that
bell curve oh yeah baby ok so we learned
about pc i CLD and RCA what else do we
learn about well I think that was it but
we talked about specifically we talked
in detail about the relationships which
in some of these mm-hmm in particular
these are all examples of feature
transformation that's right okay so we
found about the relationships between
different transformation analysis oh
here's something we learned we learned
that the a doesn't just stand for
analysis in the algorithms but it
actually does stand for analysis of the
data because that sets your provides
learning that's right and that in
particular I gave some examples where I
see a tells you what the underlying
structure of the data is you can use it
to find structure so that for example
the independent components of natural
scenes are edges so it's interesting
because I feel like the other time that
you've emphasized structure was when you
were talking about mimic which was a
piece of work that you did when you were
graduate student yep one would almost
want to guess that maybe you worked on
ICA when you were graduate student I
actually did my very first paper as a
young
was on mimic and my very last paper as a
young graduate student my actual
dissertation was on independent
component analysis I had that sense from
the number of strong points that you
felt the need to make I ca well listen
and I really structure runs my life as
you know everything about my life is
well structured yeah sure okay do we
learn anything else so yeah so I mean I
feel like we spent a lot of time talking
about so peace I ca is a more
probabilistic kind of modeling method
and PCA is a more i want to say linear
algebraic modeling model that's a really
good point Michael so we didn't say it
explicitly this way but actually even
our own work it often comes up that
sometimes you want to think about
information there you want to think
about probability and sometimes you
really just want to think about linear
algebra and you could see PCA as being
really about linear algebra and
sometimes only coincidentally being
about probability whereas i see a is all
about probability and information theory
and only coincidentally ever about
linear algebra yeah that's helpful
that's just seem to be a fundamental
split in a lot of work that happens in
machine learning and it makes some sense
i mean we know what the right answer is
in probability but we know what the
right answer is in linear algebra and I
guess it's often the case Michael would
you agree that that the linear algebra
approach is often easier to think about
or easier to do in practice I mean
sometimes can be interpreted as if it's
probabilities and that typically breaks
down on the edge cases but you know you
can kind of work around it for sort of
common cases yeah that the linear
algebra algorithms are often cheaper to
implement execute less prone to local
minima issues there's sort of a
well-defined answer that there that
they're finding but it's often not quite
the answer that you want the probability
methods give you the answer that you
want but can be very hard to find right
and in fact you can see that in ICM PC a
in that PCA is very well understood
there are lots of fast algorithms for it
I'm and you know that the principal
components always exist interestingly we
didn't talk about this but by contrast
ICA with its more information theoretic
and probabilistic roots has a very
specific model and it isn't always the
case of that model fits and so in fact
sometimes you can't find independent
components because they don't act
exist except in the most trivial sense
so it's both more expensive because of
the way you end up searching the space
and it doesn't always produce an answer
but when it does produce an answer it
tends to produce very satisfying ones
well I think that's a good place to stop
in the sense that I wanted to know just
one more interesting fact about ICA and
now that I've got that I feel fully
satisfied well there's another fact i
can tell you which is that it's the only
one of these algorithms that start with
a vowel and now i'm more than satisfied
it is always my goal to leave you more
than satisfied all right then okay well
I think we are done with this entire sub
lesson mini course thingy about
unsupervised learning well that's not
that's great what is that what does that
leave us to do doesn't leave us to do
anything at least not with this
particular mini course I think actually
the description we had here what we've
learned for this particular lesson
actually applies even going backwards to
some of our other lessons and what we're
going to get to do next is decision
problems and reinforcement learning
exciting it is exciting but I think
first people probably have some homework
e stuff to do projects to do in the
context of this mini course yes and
probably an exam of some sort everyone
on that yes we're absolutely sure you'll
do fine be sure to go over these lessons
and be sure to read all of the material
because there's a lot of detail in the
material that would make a lot of sense
for us to cover in this format but do
give it a read come back look at the
stuff that we've talked about it should
help you to understand the intuition
behind what's really happening there
excellent well this is great thanks
thanks Charles I learned a lot okay I
did too by Michael I will hear from you
soon alright awesome bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>