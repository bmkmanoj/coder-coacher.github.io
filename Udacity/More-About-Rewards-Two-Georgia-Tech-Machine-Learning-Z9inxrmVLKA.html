<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>More About Rewards Two - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="More About Rewards Two - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>More About Rewards Two - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Z9inxrmVLKA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we're really interested in solving this
temple credit assignment problem as we
will see as we talk later on in the
course this sort of issue comes up again
and again and again but I didn't really
want to talk about that in any great
detail I just wanted to make certain
that that people saw the difference
between supervised learning and the sort
of problems that we're trying to solve
now that really it has to do with
sequences in time what I want to do is
focus a little bit more on rewards and
in particular I want to look at the
little grid world that we've been
playing with so far and think about how
we would learn how to get from our start
state which we always started over here
over to one of the plus one or the minus
one
depending upon the kind of rewards that
we see so let's push on this rewards
notion for a minute if we think about
our MVPs are made up and I'm just going
to write down for you what the reward is
for the states in this particular grid
world ok yeah
so let's say I'm gonna set the rewards
to be equal to minus 0.04 so the first
thing I want to do is I wanna make sure
you can understand what I mean by this
so what I mean when I write this down
for this particular example is that the
reward that I'm gonna receive in every
single state is minus point zero four
except of course for these two states
which I've already labeled as plus one
and minus one okay all right so seems
like you should just never ever move
then right uh no I don't think that's
true let's think about that for a little
while so if I set this reward to be
minus point zero four and you're
starting out here in this state and you
have to keep living you always have to
take an action right that's the way I've
set this up and you don't get to stop
until you reach either plus 1 or minus 1
we'll call these terminating or
absorbing States once you reach into one
of these things that ends the game what
would having a small negative reward
like this encourage you to do it's it's
making me think about kind of walking
across a hot beach because like each
time you take a step you're gonna get a
little bit of punishment and the only
thing that ends it is if you can get
into that nice cool ocean and then you
get a plus one for that and the minus
point oh four minus point oh fours stop
right so that means oh wait how would
you put that another way it sounds like
what you're saying is
by having a small negative reward
everywhere it encourages you to end the
game yeah that's yeah I agree with that
okay yeah and I think that's exactly
right I like that analogy a lot your na
well it's not a very hot Beach it's just
a you know slightly unpleasantly warm
Beach and you really want to hurry up
and get into the ocean and in fact you
wanna even get into the ocean even
though on your way to get into the ocean
you might step on some class the minus
one is so let's go back to this notion
of policy a mapping States to action and
let's think about as being in this world
where you have a reward of minus 0.04
everywhere except in the absorbing
States what do you think the best set of
actions are to take in these different
states what do you think the best policy
is actually before you tell me let me
write down what the actual policy is I'm
gonna tell you how we get here but I'm
about to write down the policy that's
sort of the best one to take in a world
where you have these sets of rewards
okay sure
okay you see this Michael yeah that
makes a lot of sense like if you're
basically heading towards the rewarding
state the green state right so if I
start out here in the leftmost bottom
state basically it says go up and then
go to the right which coincidentally is
the policy that we had found before cool
there's a couple spots that are a little
bit strange though like what well I'm
thinking about the one not directly
under the minus one that that makes
sense to me but the one it takes you
down to a position where it seems like
you want to go straight up to the goal
but yet it's going the long way around
it it just didn't see the goal no that's
not it at all because of the way we're
gonna learn this you're gonna so this is
a global policy and I'm just telling you
that it's the optimal policy given the
rewards so let's kind of work out why
going to the left makes sense here well
so here's my argument what this
basically says is take the long way
around
yes yeah but by taking the long way
around what's happening well on the
downside I'm gonna pick up a little bit
of negative reward for a while you know
one two three four five six or so which
is something like negative point two
negative point three something like that
right but by doing this I avoid ever
getting into a state where I might fall
into the negative one because it's it's
stochastic right it's exactly this no
cassadines
if I'm in this state here no matter what
I do I have some if I go up I have some
chance of moving to the right and
falling into the negative one it's a
relatively low chance it's only ten
percent but it works out that moving
from here to here the probability of me
falling into here is too high compared
to the sort of near impossibility of me
ending up falling in the minus one if I
can follow this path interesting okay I
guess that makes sense that's cool
which actually suggests so to the point
that I want to make here which is that
minor changes to your reward function
actually matter so if we had a slightly
different reward here say not minus
point zero four but something else you
might find that some of these decisions
would be different I think you can see
that I mean at the level that I'm
understanding it it seems like if it's
zero that then it's in no particular
hurry if it's minus something big then
maybe it's in a bigger hurry but I don't
yeah I don't see exactly what the
difference is gonna be okay so let's see
if we can help you see the difference by
making you take a quiz</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>