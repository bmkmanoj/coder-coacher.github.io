<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Policy Iteration | Coder Coacher - Coaching Coders</title><meta content="Policy Iteration - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Policy Iteration</b></h2><h5 class="post__date">2015-06-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bJMib3EPwAE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Charles I'm gonna say that you invented
an algorithm just now that we're gonna
call policy iteration it's kind of like
value iteration except it's gonna
iterate in policy space I'm so smart
yeah though you invented it a little bit
too late because it dates back to you
know 1960 or thereabout thanks drew
thanks for always has ideas before I do
yeah texture takes is pretty clever but
this is actually Ron Howard you mean the
runner the director oh sorry no no no
it's the Ron Howard the algorithm
designer oh I always get them confused
one one played Opie and the other one
proved contraction mappings they're very
similar things so this is what I was
imagining you were explaining here take
it step by step so what we're gonna do
is we're gonna start off picking an
arbitrary Q function like we often do
we'll call that the initialization step
then we're gonna iterate the following
we're gonna take the teeth Q function
and compute its greedy policy call that
pi sub T then that policy we're going to
evaluate it to get a new Q function Q T
plus 1 and then we're gonna repeat and
iterate this over and over and over
again so each time we go around this
loop we're taking our previous Q
function finding its greedy policy
taking that greedy policy finding its
value function and repeating leatheran
through people exactly so unlike when I
take a shower we actually get
convergence in finite time so in
particular the sequence of Q functions
that we get converges to Q star which is
good that's like how policy iteration
works but even better the convergence is
exact and complete in finite time hmm
though I guess that was kind of true
value duration as well and it converges
at least as fast as value iteration in
that if at any point we sync up the Q
functions we you know we start value
duration and policy duration from the
same Q function then each step that
policy iteration takes is moving us
towards the optimal Q function no more
slowly than value duration does okay so
that kind of suggests that this is just
you know way better yeah isn't it way
better so why is it not way better
there's kind of some excitement going on
in here that that we need to take into
consideration there is a bit of a
trade-off as you like to say mmm
what's the trade-off so where's the
trade-off here we're getting we're
getting faster convergence at the cost
of greater computational expense so in
particular this step this policy
evaluation step that says take the
policy and then work out the Q function
for that policy you can do that by say
solving a system of linear equations or
make perhaps more commonly by running
something kind of like value iteration
to completion hmm so in the inner loop
of policy iteration is something that's
an awful lot like value iteration and so
maybe it's not so surprising that it
goes at least as fast as value iteration
it's it's it's doing a lot more work
than value iteration each iteration of
policy duration is doing pretty much all
the work of value iteration yeah well it
just depends upon what you're counting I
say we just count the outer loop then we
win or at least we don't lose in fact
this is kind of an interesting
outstanding question so we don't really
know how many iterations policy
iteration takes so an open question is
what the convergence time really is we
know a couple things about it but it
turns out to be fairly weak we know that
there are some MDPs such that the number
of iterations that policy duration takes
is linear it's at least as large as the
number of states in the MVP though we
can't I don't think anybody's actually
shown like two times the number of
states so there's all we know is
something really really basic which is
that it takes at least number of states
iterations in the worst case and we know
it can't be any worse than number of
actions raised to the number of states
and exponential but where sits in
between we don't really know and so if
it's closer to linear then it totally
awesome and it blows the doors off of
value duration if it's more like
exponential then
it's probably still better than value
iteration but it's definitely more of a
wash</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>