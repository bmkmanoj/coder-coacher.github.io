<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Simulatiom Lemma   Part Two | Coder Coacher - Coaching Coders</title><meta content="Simulatiom Lemma   Part Two - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Simulatiom Lemma   Part Two</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Sr6NHebsCr0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I actually just started going through
the process of approving this and it was
an awful lot like stuff that we've
already done in terms of taking bellman
equations minus other bellman equations
so let me just jump to the punch line so
if you push the math all the way through
what you find is that if all the
transitions and rewards are alpha close
to each other then the difference in the
values that you get for following a
policy is going to be no more than G
over 1 minus gamma where G is that that
alpha that error amount plus a
discounted version of that error amount
times the maximum reward that we get for
forever right so this is actually the
maximum value that you can get if you
get this the maximum reward over and
over and over and over and over again
okay so this just gives this just gives
a quantity and it has some properties
that you'd expect like the fact that if
we estimate the parameters perfectly an
alpha is 0 then this is 0 plus 0 and and
then this is 0 / 1 minus gamma which is
0 and so the two value functions are
actually identical but they fall off
from each other so if this alpha is
something not 0 some some slightly
bigger than 0 then these value functions
are going to be off by a bit more so
ultimately what we're going to want is
we want to know how accurately do we
have to estimate the transition and
rewards what should alpha be so that the
difference between the value that we get
by running in our simulation of the
model and the value that we get running
in the actual MVP are really small like
epsilon close so I think all we need to
do actually is solve for alpha in terms
of epsilon to be able to see how
accurately we need to do our estimate to
make sure that ultimately our values are
going to be close that makes sense to me
so this is me now solving for alpha and
we end up with this expression here
which says that that we need to take the
epsilon that we want how close we want
the values to be and then transform it
in various ways actually make it a heck
of a lot smaller and and that gives us
how accurate we need the estimates of
the transitions and rewards to be so
that we know that we're going to be that
close in terms of the value sure
intuitively obvious even to the most
casual observer well okay you're
probably being sarcastic um so it's
maybe not completely obvious but I you
know I did algebra here so it's not it's
not deep but it's not it's not
intuitively obvious to me why it should
have exactly this form in fact usually I
don't know epsilon czar like 1 over 0
but I guess we're trying to define a
quantity alpha which is like an epsilon
quantity so it wouldn't make it sense
for to be it you know giant number it
should be a tiny tiny number but yeah no
no you're right it's not it's not
completely obvious but the point that i
want to make with this is it's their
polynomial e related to each other right
so if we have our max and 1 1 over 1
minus gamma and one of our eps alon then
the kind of alpha that we get out is
going to be polynomial related to the
other things okay so that's that's
reassuring I mean if you actually work
out what these numbers are they're
pretty scary but you know we're trying
not to let ourselves be scared of that
the important thing though is that alpha
is not really a parameter of our max the
r max algorithm right what was the
parameter that we had when we talked
about the the algorithm si excelente
buenisimo so that is to say what we want
is a parameter that says how many times
do we have to try a state action pair
before we are confident that our
estimate of that transition probability
is is better than alpha so how are we
going to get that last piece how we
going to turn alpha into see more
algebra yeah but specifically there's
bounds that come into play here so wait
said so now we've actually turned it
into the same kind of problem as the
Bandit problems that we were talking
about right that we need to know how
many times do you pull a bandit arm
before your estimate in it in the mean
is sufficiently close and that was the
huffing bound right you did say we were
going to do something about yeah and the
Union bound oh yeah we want to
simultaneously make this alpha close
estimate hold across all state action
pairs so we need to make the the see big
enough so that the probability that it
fails in any one of them is sufficiently
small but it really is exactly the same
argument that we did when we were
talking about bandits okay and sure so
every every state action pairs like a
bandit
and you have to pull it a certain number
times it's like an estimate of an arm
right and we need to all the arm
estimates to be accurate if we want the
arm that we eventually picked to be
nearly optimal right okay no that makes
that you know I can see the analogy I
can see the metaphor I can see the
simile all right perfect so okay there's
one more piece to this story and then I
think we're done with exploration hey
we'd be done exploring exploration that
said it's sort of sad</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>