<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Wont You Compute My Neighbors Quiz Solution - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Wont You Compute My Neighbors Quiz Solution - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Wont You Compute My Neighbors Quiz Solution - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1t6Wm37e1VM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Michael are you ready I am afraid
so all right which one you want to fill
out first that's just to him in order so
one nearest neighbor you you explained
how the training works we just take the
sorted list and leave it there and we
have the the classifier or the regressor
itself has linear space and now a query
time we need to find the nearest
neighbor mm-hmm which we could do by
taking the query point and running
through the whole list and seeing which
one is closest to but because it's
sorted I think we ought to be able to
use binary search and and in log time
find the closest point to the query
that's exactly right you should be able
to do that in log base two time what if
it weren't sorted
yeah then like I said I think you can
just scan through the whole list and
that would be linear time and that's not
a big deal right yeah we could do it in
linear time but because I gave you a
sorted list because I'm so helpful you
can do it in log it back okay that was
that was very very thoughtful of you it
was I thought I thought it through so
what about on the space side all right
so the amount of space that you need to
process this query is linear we don't
need to take any special set aside space
beyond a couple simple variables and the
data that we're given which we've
already counted for right so then why
would it be linear if we've already
accounted for it can I say linear ice I
meant constant yes yes that's right
constant that's what you meant that's
what you said that's what happened okay
and good thing that this wasn't being
recorded so we could verify one way or
the other it is a good maybe we'll look
it up in Wikipedia and I'll say
confusingly linear sometimes used to
mean constant it's pretty confusing
okay what about KN all right KN so KN so
the training process the learning
process is exactly the same as it is for
one nearest neighbor which is to say you
do nothing and you pass all the data
forward to the the query processor so
it's going to be one end that is correct
nice all right now query Inc seems like
it's a little more so so we can find the
single nearest neighbor in login time
where are we gonna get the other K minus
one so I'm pretty sure that
once we find the nearest neighbor we can
kind of start doing a little spread out
search from there until we found the K
nearest neighbors sure so you're saying
you know you've got these points they're
already in a line you find the nearest
neighbor you know the the next nearest
neighbors have to be within K of the
points surrounding it and so you could
just move in kind of either direction
and pick them up as you go yeah
something like that
okay I mean the way the way that I was
thinking about it is it I think you can
use the same algorithm that's used for
merging lists in merge sort
but here the lists actually correspond
to being to the left of the query point
and mean to the right of the query point
and they're both sorted in terms of
their distance from the query point sure
yeah by that so so that ought to give us
login plus K okay so do we need to write
the K I'm gonna say yes because if K is
on the order of like n over 2 and it's
gonna dominate if K is on the order of
log n then it's not gonna dominate
that's a good point so yeah we'll do K I
will point out that if K is on the order
of n over 2 you're right it will
dominate and then really this is Big O
of n but it's on the order of log n then
it's just log n plus log in and so it's
Big O of long in log n but you're right
so we should probably keep the K around
because we don't know its relationship
to in okay fair enough ok what about the
space requirements we know one bit of
relationship it's smaller than or equal
to n that's true because that would be
really weird if I gave you ten data
points and asked for the twenty nearest
neighbors
that's the sort of thing you would do
it's the sort of thing you would do but
then it would be really confusing
no no sort of thing you would do again
let's go to Wikipedia confusingly 20 is
sometimes many MIT Tim okay so what
about space space so I don't understand
why it would ever need more than
constant space so so we're gonna zip
around and that lead it really badly we
can use K space to kind of copy over
what those possible nearest neighbors
are but we don't need to keep them to
keep track of them we can just point to
them in place on it so it's constant
okay yeah that's true in fact because
it's sorted all you really need to know
is the beginning and the ending so
that's two things so it's constant okay
cool
all right good so what about linear
regression your favorite little
algorithm thing that you did
when we talked about this before I do
like linear regression alright so the
learning in this case is what we're
mapping a real number input to a real
number output and the way we're doing
that is we're taking it's probably like
MX plus B sort of form so we need to
find the multiplier and the additive
constant which it seems like well so in
general doing a regression involves
inverting a matrix but in this case I
think the matrix that we're talking
about is of constant size so inverting
it is constant time
I think it's as easy as basically just
scanning through the list to populate
that that constant size matrix so I'm
gonna say order n yep to process the
data that is correct
there's probably a really nice algorithm
for that yeah probably some kind of
linear regression algorithm yeah no I
mean like the general linear regression
algorithm is involves inverting a matrix
right or something like it something
equivalent to it but here because we're
it's all in scalar land I think it's
simpler yeah I think that's right okay
so what about space
alright so space in terms of the data
that you pass forward from the learning
algorithm to the regressor is just well
MX plus B it's just M and B which is
constant there's the two numbers right
that's 2 2 is like 1 for sufficiently
large values of 1 so yeah all right now
a query time you give me an X I multiply
it by M and add B so that's constant
time yep so before the query cost was
expensive and the learning cost was
cheap and now we've kind of swapped that
around
yeah so space would be space oh that
you're asking me space for the query
would be constant as well right exactly
so yeah so you made a good point here so
earlier on we had this situation where
learning was fast constant and querying
was you know not as fast it was you know
probably logarithmic but when the linear
regression case learning was expensive
and the query aim was easy so we swapped
around that's exactly right so why would
you care about that well let's see I'll
point out something which is though even
though we swapped out what was expensive
in terms of time and what wasn't you'll
notice that
it's only logarithmic at query time for
these first two but it's linear for the
learning time in linear regression so it
doesn't mean that linear regression is
always slower and worse well not really
because we don't know how many times we
only have to learn once but we could
query many many times right right so I
guess if we query more than you know n
times for example it'll certainly be
worse overall in terms of running time
that's right okay though it's though
it's interesting because like when I see
numbers like this my my algorithm hat
tells me that I should try to balance
them a little bit more like here it's
it's you can make learning essentially
free and the other one you can make
querying essentially free really you
want to split those somewhat evenly
though it's not obvious to me how you
would do that like square root of n
learning time and then square root of n
query time or say like that yeah but you
did say something else that was
important right which is that you only
only have to learn once yeah that's you
so the balance you know really depends
by walking to do querying and exactly
what power gives you I mean the
trade-off is really there
don't you think yeah I guess so I mean
so so specifically in the in the version
where you just query once right then the
balance think could be more interesting
sure okay cool all right anything else
you want observe about this um let's see
we got the trade-off between learning
versus querying so either you do all
your work upfront or you put it off and
do your work only when you're forced to
a query time yeah I guess well one thing
is I want to point out that there's a
nice mr. Rogers reference in the title
that's that was very cool thank you very
much
and the second thing is that it does
strike me in a sense that what's going
on here for the nearest neighbor
algorithms is you just put off doing any
work until you absolutely have to mm-hmm
which strikes me as kind of a
procrastinator II approach so that's a
good word that you use there
procrastinate the words that people use
in the literature are lazy they say that
these are lazy learners versus something
like linear regression which is an eager
learner eager yes so linear regression
is eager it wants to learn right away
and it does but nearest neighbor
algorithms are lazy they want to put off
doing any learning until the absolutely
- at query time and so we refer to these
classes lazy in this class is eager I
see so I guess it's the case that if we
never query it then the lazy learner
definitely comes out ahead right that
makes sense yeah it's just in time
learning poor Jill Forge it'll I guess
which doesn't roll off the tongue
anywhere near as well okay cool so we've
gotten to this quiz would you like to do
another one yeah I just have to get this
JIT all off my tongue</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>