<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Finding Policies Three - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Finding Policies Three - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Finding Policies Three - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5WwuAruQrj0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so Michael let's see if we can do
something that might be faster and just
as good as what we've been doing with
value durations and what we're going to
do is we're going to emphasize the fact
that we care about policies and not
values now it's true that given the true
utilities we can find a policy but maybe
we don't have to find the true utilities
in order to find the optimal policy so
here's a little sketch of an algorithm
okay so it's going to look a lot like
value iteration we are going to start
with some initial policy let's call it
PI Sub Zero and that's just going to be
a guest so it's just going to be an
arbitrary setting of actions that you
should take in different states then
we're going to evaluate how good that
policy is and the way we're going to
evaluate it at time T is to calculate
its utility which I'm going to call use
of T which is equal to the utilities you
get by following that policy okay and
I'll show you in a moment exactly how
you do that okay but I just want to make
certain that you you you kind of buy
that maybe we could do that so we have
given a policy we ought to be able to
evaluate it by figuring out what the
utility of that policy is and again
we'll talk about that in a second and
then after we know what the utility of
that policy is we're actually going to
improve that policy in a way similar to
what we did with value iteration we're
going to update our policy time T plus 1
be the policy that takes the actions
that maximizes the expected utility
based on what we just calculated for pi
so T now notice that will allow us to
change PI over time because imagine that
we discover in some state that actually
there's an action that's very good in
that state that gets you some place
really nice gives you a really big
reward and then that point on you do
fairly well well then all other states
that can reach that state might end up
taking a different action than they did
before because now the best action would
be to move towards that state so these
two steps will actually or can actually
lead to some improvement of the policy
over time but the key thing here is we
have to figure out exactly how to
compute use of T well the good news is
we know how to do that and it's actually
pretty easy and it boils down to our
favorite equation the omens equation so
our utility at time T that is the
utility that we get
by following the policy at time t is
just well the true reward that we're
gonna get by entering that state plus
gamma times the expected utility which
now looks like this there's that make
sense you see that it looks a little
different from the other equation so did
you mean for it to have T UT is defined
in terms of UT and not UT minus one yes
okay that's interesting and the max is
gone but instead of a max there's a
policy stuck into the transition
function the choice of action is
determined by the policy right and
that's actually the only difference
between what we were doing before is
that rather than having this max over
actions we already know what action
we're going to take it's determined by
the policy we're currently following
okay but isn't this just as hard as
solving the thing with the max you said
well what was the problem that we were
solving before with the max that was the
spellman equation yes but we were
solving a bunch of equations how many of
them and so we were solving in equations
and how many unknowns and what's the
difference between this n equations and
n unknowns and the other n equations and
n unknowns well and it's the same mm-hmm
there's no max though there's no max and
what was it that made solving that hard
before it made the max made it nonlinear
the max is gone now you're saying this
is this is a set of linear equations
yeah because well there's just a bunch
of sums and the PI is not like some
weird function this is just effectively
a constant I see so now I have n
equations and n unknowns but it's in
linear equations and now that I have in
linear equations and unknowns I can
actually compute this in a reasonable
amount of time by doing matrix
inversions and progression and other
magic hand-wavy things that's very slick
it seems it's still more expensive than
doing the value duration update I guess
yeah but you don't have to perhaps do as
many iterations as you were doing before
so once you've evaluated it which we now
know how to do and you've improved it
you just keep doing that until your
policy doesn't change very cool mm-hmm
and this does look a lot like value
iteration you doesn't yeah though it
seems like it's making bigger jumps
somehow it is and that's because instead
of making jumps
in value space it's making jumps in
policy space which is why we call this
class of algorithms policy ever cool
right now this inversion can still be
fairly painful it's you know if we don't
worry about being highly efficient you
know it's roughly in cubed and if there
are a lot of states this can be kind of
painful but it turns out there's little
tricks you can do like do a little step
of value iteration here for a while to
get an estimate and then you know kind
of cycle through so there's all kinds of
clever things you might want to do but
at a high level without worrying about
you know the details of constants this
general process of moving through policy
space and taking advantage of the fact
that by picking a specific policy you're
able to turn your non-linear equations
and the linear equations turns out to
often to be very helpful
so is it guaranteed to converge yes nice
well there that was easy I'm not gonna
go into it but you know there's a finite
number of policies you're always getting
better so eventually you have to
converge it's very similar to the or at
least intuitively it's very similar to
the argument you might make for k-means
cool</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>