<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Relevance - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Relevance - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Relevance - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YD-uvMgcwgY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Michael so let's see if we can jump
on that notion of useless and some other
words you use I'm gonna define what it
was you were trying to say by calling it
relevance okay okay so it turns out this
thing that you were hanging on to where
you were saying that well see you
thought was useless and then it turned
out to be useful and you were you were
coming up with reasons why it doesn't
change anything it doesn't provide any
information we can actually make very
precise and formal so here's a formal
definition of the notion of relevance
and I'm just going to go through this
let me know if it makes sense
so we have a set of features and let's
just say there are X I so there's X 1 X
2 X 3 X n features and any particular
feature X sub I is going to be strongly
relevant exactly in the case if removing
that feature degrades the Bayes optimal
classifier that's what boc stands for
okay does that make sense did we talk
about what fair is yes way way back in
the supervised learning lesson on
Bayesian learning right we ended up
talking about the Bayes optimal
classifier is the one that takes the
weighted average of all of the
hypotheses based on their probability of
being the correct hypothesis given the
data remember that yeah and we and what
we actually said is that the Bayes
optimal classifier is the best that you
could possibly do on average if you
could actually find it right that coming
back to you yeah I mean so we need a
notion of priors and stuff to be able to
define strongly relevant no it really
just says that there is the Bayes
optimal classifier which is to say the
best that you could do that X of I is
strongly relevant in the case when if
you didn't have that feature you
couldn't do as well as the Bayes optimal
classifier that had access to all of the
features in the quiz that we did before
we know that the actual function we were
looking for really was a and B so if I
remove a I can't actually compute a and
B similarly if I remove B I can't
actually compute a and B so both a and B
are strongly relevant okay I mean in
this case it uses the fact that not only
that it is a and B but there is nothing
else that has the same information is
and the same information is B right
exactly and that little that that
difference you just noted is the
difference between being strongly
relevant and being weakly relevant oh so
a variable a feature is called weakly
relevant if it's the case that it's not
strongly relevant so the order of these
definitions matter and it turns out that
there exists some subset of your
features let's call that subset s such
that if I added the feature to that
subset s it would in fact improve the
Bayes optimal classifier so in this case
we're talking about the Bayes optimal
classifier on oh I guess in both cases
we're talking about the Bayes optimal
classifier on the reduced set of
features right comparing it to the Bayes
optimal classifier on the full set of
features in the first case right or just
on any subset of features so for in
particularly the strongly relevant case
that's the case we're saying well what
would the Bayes optimal classifier be on
all of the features versus the Bayes
optimal classifier on all of the
features except X sub I and if we're
moving X of I degrades the performance
then we say it's strongly relevant on
the other hand if X of I is not strongly
relevant that is removing it doesn't
actually hurt the Bayes optimal
classifier it can still be weakly
relevant in the case where there is some
subset of the features such that if I
added X of I to that subset it would
improve the Bayes optimal classifier on
just that subset so let me I can make
that a little bit more concrete imagine
that we had another variable let's call
it e ok which had these values so if you
look carefully you'll notice that E is
in fact not a right so that means that
neither a nor E is strongly relevant
because I can remove a and still learn
learn B and Abe by basically making it
be and not E or I could remove eat and
still learn a and B by just simply using
a and B agreed yeah so it's so then B
would be strongly relevant but the other
two not right however both a and E are
still weakly relevant because there
exists a subset such that adding it back
in gives you better performance in
particular a is weakly relevant for any
subset that doesn't include E and E is
weakly relevant for any subset that
doesn't include a
and that includes the null set in this
case yes exactly
if you have a particular feature which
is not strongly relevant and is not
weakly relevant then we call it
irrelevant so in this case when what you
were calling see is being useless what
it actually is is irrelevant because it
provides no information to the
classifier you might as well have just
simply had the normal subset and just
always output no and you would do just
as well as having the value see and yet
it somehow turned out to be helpful for
the perceptron case right and that's
because there's another notion that we
could think about which is not relevance
but usefulness so let me define that for
you okay sure</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>