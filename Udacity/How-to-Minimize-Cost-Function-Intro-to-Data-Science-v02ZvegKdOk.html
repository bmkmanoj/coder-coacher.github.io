<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Minimize Cost Function - Intro to Data Science | Coder Coacher - Coaching Coders</title><meta content="How to Minimize Cost Function - Intro to Data Science - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Minimize Cost Function - Intro to Data Science</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/v02ZvegKdOk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so how do we find the correct values of
theta to minimize our cost function J of
big theta we'll use a search algorithm
that takes some initial guess for theta
and iteratively changes theta so that J
of theta keeps on getting smaller until
it converges on some minimum value the
algorithm we're going to discuss is
called gradient descent here's a
one-dimensional depiction of what
gradient descent might look like we have
some starting point where J of theta is
large and we continue to try new values
of theta and J of theta keeps on getting
smaller and smaller until we arrive here
the final value this theta value gives
us our smallest value of J of theta and
so we know that we've minimized J of
theta this is what gradient descent
might look like in one dimension let's
see what it looks like in two dimensions
here again we see that we start at some
high value of J of theta and we continue
to iteratively update theta 1 and theta
2 until we arrive here the global
minimum of J of big theta gradient
descent updates the values of theta
according to the following equation
theta n is equal to theta n itself minus
alpha times D D theta n J of big theta
note the derivative here D D theta n of
J of theta if you're less familiar with
calculus this is calculating the rate of
change of J with respect to this
particular theta n so if we change this
theta n a little bit how much this J of
big theta change we simultaneously
perform this update for every theta n in
our big theta that is every single theta
n in the entire set of Thetas the Alpha
in this equation is a parameter of the
algorithm called the learning rate what
we're essentially saying here is that in
the space of all possible values of
theta what's the smallest step I can
make in a certain direction such that
I'll make the value of J smaller so each
time I do this I'm taking one of these
small steps towards the minimum value of
J of theta I'm not going to go into the
calculus here but based on the cost
function J is dependence on the various
theta ends if we actually perform this
differentiation the actual equation that
we'll use to update our theta n values
is this theta n equals theta n minus
alpha times the sum from I equals 1/2
of Y predicted of X I - y observed X I
times X sub n I you may be wondering how
we set a value for the learning
parameter alpha our rigorous discussion
is beyond the scope of this course
however the important thing to note is
that the smaller learning rate will
cause the algorithm to take longer to
converge on the optimal theta values
since you're taking smaller and smaller
steps down the curve larger values of
alpha can converge more quickly but
there are more prone to skip over the
cost functions minimum value this can
cause J to increase rather than decrease
monotonically the best way to make sure
that your alpha is suitable is simply to
keep track of your cost functions value
as you iteratively update it and make
sure it's always going down if not your
learning rate is probably too high</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>