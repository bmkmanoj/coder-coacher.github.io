<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Markov Decision Processes Three - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Markov Decision Processes Three - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Markov Decision Processes Three - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Jw3ZnWFjDfM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">now I actually snuck something important
in here actually stuck to things that
are important here
the first is the what's called the
Markovian property do you remember the
Markovian property is Michael
from what from I don't know actually
where does the microvia property come
from I'm gonna say Russia okay yeah from
the from the Russian yeah so I have
Russian ancestors and they passed on to
me this idea that Markov means that you
don't have to condition on anything past
the most recent State that's exactly
right the Markovian property is that
only the present matters and they had to
pass that down to me you know one
generation at a time because you know
Markov property that's exactly right
that was very good Michael so what does
this mean what this means is our
transition function which tells you the
probability you end up in some state s
prime given that you're in state s and
took action a only depends upon the
current state s if it also depended upon
where you were 20 minutes before then
you would have to have more SS here and
then you would be violating the
Markovian property so it's just like to
historians hate this well you know one
never learns anything from history no
you're supposed to learn from history or
you're doomed to I don't know let me
make something up repeat it fair enough
historians probably don't like this but
there is a way for mathematicians to
convince them that they're ok with it
and the way that you mathematicians
convince you that you're ok with this is
to point out that you can turn almost
anything into a Markovian process by
simply making certain that your current
state remembers everything you need to
remember from the past I say
so in general even if something isn't
really Markovian you need to know what
you're not only what you're doing now
but what you were doing five minutes ago
you could just turn your current state
into what you're doing now and what you
were doing five minutes ago the obvious
problem with that of course is that if
you have to remember everything from the
beginning of time you're only going to
see every state once and it's going to
be very difficult to learn anything but
that Markovian property turns out to be
turns out to be important and it
actually allows us to solve these
problems in a tractable way but I snuck
in something else Michael what I snuck
in is this idea about the transition
model is that nothing ever changed
so the second property that matters for
Markov decision processes at least were
the sets of things that we're going to
be talking about in the beginning is
that things are stationary that is these
for the purposes of this discussion it
means that these rules don't change over
time
that's one notion of stationer okay does
that mean that the agent can never leave
the start State no the agent can leave
the start state any time it takes the
action that gets it out interesting how
is it stationary it's not stationary the
world is stationary the the transition
model is stationary the physics are
stationary the rules don't change money
the rules don't change right okay I see
there's another notion of stationary
that we'll see a little bit later okay
last thing to point out about the
definition of a Markov decision process
is the notion of reward
so reward is simply a scalar value that
you get for being into in a state so for
example we might say you know this Green
goal is a really good goal and so if you
get there we're going to give you a
dollar
this red goal on the other hand is a
very very bad state we don't want you to
in there and so if you end up there
we're going to take away a dollar from
you
but if I don't have a dollar someone
will give you the dollar the universe
will give you the dollar or in the
ticket away the universe will take it
away even if you don't have it you'll
have negative dollars oh man
okay so reward is very important here in
a couple of ways reward is one of the
things that as I'm always harping on
encompasses our domain knowledge so the
rewards you get for an estate tells you
the usefulness of entering into that
state now I wrote out three different
definitions of are here because
sometimes it's very useful to think
about them differently I've been talking
about the rewards you get for entering
into the state but there's also a notion
of reward that you get for entering into
a state and taking an action there's a
reward or being in a state in taking an
action there's a reward that you could
get for being in a state taking an
action and then ending up in another
state s prime it turns out these are all
mathematically equivalent but often it's
easier to think about one form or the
other but for the purposes of the you
know for the rest of this discussion
really you can just focus on this one
the reward or the value of entering into
a state and those four things by
themselves along with this Markov
property and non-stationarity defines
what's called a Markov this
process or an MDP got it I'm a little
stuck on the how those could be
mathematically equivalent well we'll get
to that later do you like a little bit
of intuition sure well you can imagine
that just as before we were dealing with
the the notion of making a non Markovian
thing Markovian by putting a little bit
of history into your state you can
always fold in the action that you took
to be in a state or the action that you
took to get to a state as a part of your
state but that would be a different
Markov decision process it would but
they would work out to have the same
solution oh I see</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>