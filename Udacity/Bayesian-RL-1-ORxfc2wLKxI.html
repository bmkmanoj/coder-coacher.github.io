<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bayesian RL - 1 | Coder Coacher - Coaching Coders</title><meta content="Bayesian RL - 1 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bayesian RL - 1</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ORxfc2wLKxI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">now that we talked about doing
reinforcement learning in a pom DP
setting we're actually now going to talk
about a much weirder combination we're
going to talk about how we can think of
reinforcement learning as actually being
itself upon DP okay this is an area that
is often referred to as Bayesian
reinforcement learning the idea is that
we're going to keep a Bayesian posterior
over possible MVPs that we're in and
we're going to use that to actually a
optimally balanced exploration and
exploitation so we're actually going to
behave optimally and reinforcement
learning becomes planning okay so does
it mean we're gonna use Bayes rule well
I guess technically in that whenever you
compute a posterior you're actually
doing something like Bayes rule yeah so
but maybe this is not so clear let me
let me try to draw a picture so I'm
hoping this example might make things a
little bit clearer so so let's consider
a really simple reinforcement learning
problem we've got two states state one
and state two two actions a black one
and a I don't know purplish kind of one
and so if we were doing reinforcement
learning in this environment like and
you were say the reinforcement learner
what would you do
let's say we're in state one what would
you do would it be better to do purple
or black I don't know okay so then what
would you do I'd pick one of them all
right so let's say you pick black okay
and then we observe a transition and
ultimately we can sort of build out a
model or use cue learning or something
like that to work out the values so what
I'd like to do is actually try to have
you think about this like a palm DP and
I think a simpler way to show you that
would be instead of imagining all
possible mdps this could be let's just
pretend it comes from a small finite set
okay are there any small sets that
aren't finite no good then I think we're
on the same page all right then I agree
with you so here's the set that I had in
mind we've got three possible worlds
that we're in a b and c so these are
three possible MVPs right okay and they
all look exactly the same to me except
where you get reward yeah so it just so
happens that the actions all do the same
thing in all three DS and we know that
we just don't know where the reward is
is the reward something you get for
purple when you go from state two is it
something that you get for purple from
state one or is it something that you
get from taking the black action from
state one and we don't know now if you
had to solve this reinforcement learning
problem it's kind of reinforcement
learning a problem right you're in some
state you get to see which state you're
in one or
you can try actions black or purple and
what you're trying to do is actually
figure out how to maximize reward but
now that it's of just a finite set of
possibilities we can think about this
like a pom DP sure and in fact I think
this one's pretty straightforward if I'm
in state two I know what I should do and
what is that I should take the purple
action mmm and why is that explain
because I either immediately get reward
for it or I know that I get reward for
being in state one and then when I'm in
state one well actually it's not clear
which one I should do hmm all right so
let's um well let's do let's do two
things let's start off by just tracking
belief states in this because I think
that's a that's already kind of an
interesting problem in and of itself
what we're thinking about here is that
we're in a pom teepee with six states
often state 1 but we don't know which
MVP were in then the belief states gonna
look something like this
agreed agreed if this is where we
starting let me know nothing else and we
haven't done anything yet then we can be
in any of those states presumably with
equal probability good all right so now
the question is if you were to do the
purple action from here
what would your expected reward be my
expected reward would be 1/3 right so if
we happen to been an MD PB which we
don't know if we are and we took the
action purple we'd get a 1 if not then
we get 0 but what also would happen if
we get if we get the 1 what's what's our
new belief state if we get the 1 yeah
well then we know that we are in B 1
good B 1 right so we're in this state so
the belief state becomes basically fully
observable at this point we know which
MDP we're in and we know what state of
that MVP we're in and we can just
execute the optimal policy for that MVP
which in this case as you might guess
it's just purple all the time all the
time the time yeah so let's um make sure
you get this we'll do a little quiz II
kind of thing ok</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>