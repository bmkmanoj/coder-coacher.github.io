<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>PCA vs ICA Quiz Solution - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="PCA vs ICA Quiz Solution - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>PCA vs ICA Quiz Solution - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SjM2Qm7N9CU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Michael you got answers for me
thanks oh okay good all right so let's
look at the first one mutually
orthogonal does that apply to PCA ICA
both or neither so it was one of the
defining properties in PCA so I would
say PCA okay
that's fair enough that is actually
correct what about ICA I don't I don't
know it's not one of the defining
features it wasn't it wasn't even
thinking about orthogonality that's
right and in fact ICA by finding
independent projections are almost all
but guaranteed to find ones that are not
mutually orthogonal it doesn't care at
all so in fact this is what makes PCA a
global algorithm since it has this
global constraint of mutual recursion
allottee where ICA really there's
nothing in its definition that cares
about that so this should be unchecked
okay I'm going to put an X there to
represent unchecked okay got it yeah
okay
what about mutually independent so that
was how I ca was trying to construct its
I don't want to say features yeah I
guess the dirt the transformed features
that's right it was trying to create
them to be mutually independent so I
would check the ICA in that case and PCA
didn't use that language at all so I
would just not do that okay that's fair
I will point out though that it turns
out that PCA is trying to do something
that sometimes will create things that
are mutually independent but we'll see
that when we answer the next question
but you're right PCA does not care about
mutual independence okay what about the
third phrase maximal variance all right
again I feel like this was one of the
defining features of PCA so it was
trying to choose dimensions that
maximize variance that's right what
about ICA in terms of variance again
that there wasn't it wasn't really
discussed in that context right I see is
specifically interested in notions of
independent statistical independence not
an imp not in issues of variance so in
fact ICA does not care about maximizing
variance now we've gotten this far let
me point something out it turns out that
because of this orthogonality constraint
this lack of an independence constraint
but this goal of maximizing variance
there are cases under which PCA happens
to find independent projections what's
really going on here with these three
constraints or lack of at least one in
this case is that PCA is tending to find
things that are
related by maximizing variance along
orthogonal dimensions it's finding
uncorrelated dimensions and that makes
some sense given what it's trying to do
but that is not the same thing as
finding things that are statistically
independent again there is a space there
is a specific case where that does work
out and that's when all of your data is
in fact gauja Oh interesting
and why Gaussian because it turns out
that the distribution that maximizes
variance is in fact the normal
distribution but that's my
interpretation of a normal distribution
so maximizing variance means that what
PCA is doing is it's trying to find a
bunch of orthogonal gaussians more or
less does that make sense yeah but you
were saying that and it aligns with ICA
in that case in that case yes because
it's going to turn out that uncorrelated
ends up being independent under very
specific distributions but that's a
coincidence it's not a normal fact haha
but by the way this is probably worth
pointing out here something that at
least I think is kind of interesting
here which is since ICA is only trying
to find things that are independent of
one another here's our little symbol for
independence as opposed to things that
are uncorrelated things that are
Gaussian it turns out that whatever it
is PCA is doing it is not working under
the same underlying model as ICA right
let's think about what ICA is trying to
do IC is trying to find a bunch of these
projections all of which are
statistically independent right what
happens if I take a whole bunch of
statistically independent variables and
I add them together in other words I
create a linear combination of them what
am I going to get bunch of sums of
things that are independent right and
what does that tend towards in the limit
I want to say the law of large numbers
tells us that it turns normal that's
exactly right
if I take a bunch of independent
variables and I sum them together that
as I create a linear combination I in
fact end up with a Gaussian and that is
the central limit theorem so one
argument you might make is that if you
believe in a world where there are
independent causes giving rise to
observables which is what ICA believes
then whatever you do you should not be
maximizing variance because you're
guaranteeing to be summing together
otherwise independent variables oh I see
I see I see so by trying to find things
that are maximal variance it's trying to
mix together through the central limit
theorem all these things that are
independent so it's it's specifically
not teasing apart the independent things
it's trying to push together the
independent things right under certain
assumptions about the distributions of
these individual variables so another
assumption that ICA is making is not
just that these variables are
independent but that they're highly not
normally distributed and if that's the
case then ending up with things that
look like gaussians has got to be
exactly the wrong thing to do if that
assumption holds to be true okay what
about maximal mutual information my
understanding of what you described for
ICA said that this is what it's trying
to do it's it's trying to find a new
feature space where the features are
maximally the mutual information between
them is as large as possible so I would
check the ICA in that case and not the
PCA let maybe let me clarify something
you said how can it be trying to
maximize mutual information while also
trying to make things that are mutually
independent I think you describe them
both in the same language so what is it
trying to make mutually independent
different features the new - new
transformed features right so each of
the new transform features is
independent with all the other new
transform features so what is it trying
to maximize mutual information between
oh the I see the information that
content of the original features and the
new features right so this is about
joint mutual information between all the
original features together and all of
the transformed features there it's
trying to maximize mutual information
but inside the new transformed features
just trying to make them pairwise
mutually independent all right I think I
set that wrong because I understood it
wrong so thank you for clarifying your
work but now you understand it right
maybe okay let's go with that because
you've got the check mark right what
about PCA I just X that I don't
understand what that would mean right so
if I were to put something here that it
could get a check mark for what I put
down is maximal reconstruction
right right and notice that maximum
reconstruction of your original data is
not the same thing as maximizing mutual
information though of course in the
limit they work out to be the same
interesting okay but the one projection
that maximizes variance is not
necessarily the same as the first
projection you find for maximizing
mutual information so these things
really are doing two completely
different things okay
last - what about ordered features so in
PCA it was actually assigning you know
taking them the maximum variance
dimension first and then the next you
know after that's been subtracted out
whatever has the largest remaining
variance and so forth so the features
end up coming out in in a very specific
order and it has the property that you
can drop the the last group of features
if you want to still have as high
reconstruction error as possible given
the number features that you keep so I
would check PCA for that okay
good what about ICA you didn't say
anything about the ordering or how you'd
actually find features it seemed like in
the in the blind source separation
example that you gave it just came out
with the three so I'm going to say not
the ordered that's right and in fact if
you think about the blind source
separation example how in the world
would you order people anyway it mean
other than in the obvious way it just
doesn't really mean anything ICA doesn't
have a notion of causes being more
important than other causes merely that
they're independent so it doesn't really
worry about ordered features it turns
out in practice that you can actually
try to order the features by using
something called kurtosis which is the
fourth central moment of a distribution
but that's really just something that's
useful in some specific cases almost by
coincidence ICA itself does not
particularly care about ordering at
least not classical ICA okay what about
the last one bag of features so I would
view what you just said about ICA as B
as implying that what ICA produces is a
bag of features there's no particular
ordering to them it's just a collection
of things that make up the whole I guess
you know PCA after you've thrown away
whichever features you don't want the
features that remain are just features
they could be treated as a bad
so I don't know if I would check that or
not I for symmetry I guess I would say
not okay but I'm gonna say yes because
an ordered set of features is still a
bag of features but we would accept
either either a check or an ax both are
sort of perfectly fine so what do we
really learn from this Michael I think
what we learned is these things have
fundamentally different assumptions and
are really trying to do completely
different things
okay the only thing they have in common
is that they're still trying to capture
the original data somehow I understand
that but I also learned the opposite
which is that they are really closely
related and are trying to do very
similar things yeah but their underlying
models are just so maybe that's actually
good point Michael so maybe a better way
of saying it is there's sort of
fundamental assumptions or different
even though they're trying to do the
same thing which is capture the original
data in some new transform space that is
somehow better but if you think about it
that way there have two different
optimization functions two different
fitness functions two different cost
functions so even though they're trying
to do the same thing reconstruction
keeping the data around their basic
assumptions about the way that data is
constructed are very different okay okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>