<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Boosting Tends to Overfit Quiz Solution - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Boosting Tends to Overfit Quiz Solution - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Boosting Tends to Overfit Quiz Solution - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Hp4gJjSFSYc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right Michael what's the answer all
right well let me start off with what I
think the answer isn't so the the last
one boosting tends to over fit if
boosting trains too long you just told
me a story about that not being true so
I'm I'm gonna eliminate that one from
consideration boots booty training too
long seems like not a good reason for it
to overfit you're correct all right
boosting tents overfit if it's a
nonlinear problem so that doesn't seem
right I mean I guess now this one just
doesn't seem right at all like I don't
see why why the problem being linear and
nonlinear has anything to do with
overfitting okay a whole lot of data is
the opposite of what you tends to cause
overfitting if there's lots of data then
you'd think that it would actually do a
pretty reasonable job of you know
there's a lot to fit there's a lot going
on there it's unlikely to overfit right
and in fact if a whole lot of data
included all of the data and you
actually could get zero training error
over it then you know you have zero
training zero generalization error ah
cuz it'll work on the testing data as
well because it's in there right all
right weak learner uses artificial
neural network with many layers in node
so I'm guessing that you wanted me to
think about that being something that on
its own is prone to overfitting because
it's got a lot of parameters sure so if
now we're doing boosting over that so we
fit a neural net and then we fit another
neural net and we fit another neural net
and we're combining all the outputs
together in the correct weighted way
it's not obvious to me that that should
be a good thing to do I'm not sure it
would over fit but it seems like it it
sure could okay so your so for now let's
put a little question mark to it you
think that might be the right answer
which you want to think about it yeah
look at the first one week learner
chooses the weakest output well I mean
boosting is supposed to work as long as
we have a weak learner and it doesn't
matter if it chooses the weakest or the
strongest all that matters is it does
significantly better than a half so like
I feel like the only one the only way
these choices that is likely to be true
is the second one and that is in fact
correct so let me give you an example of
when that would be correct so let's
imagine I have a big powerful neural
network that can represent any arbitrary
function okay got lots of layer
lotsa notes so boosting calls it and it
perfectly fits the training data but of
course over fits so then it returns and
it's got no error which means all of the
examples will have equal weight and when
you go through the loop again you will
just call the same learner which will
use the same neural network and will
return the same neural network so every
time you call the learner you'll get
zero training error but you'll just get
the same neural network over and over
and over again and a weighted sum of the
same function is just that function so
if it over fit boosting one over fit
interesting and not only will it over
fit but it'll just it'll kind of be
stuck in a horrible loop of error right
so that's why this is the sort of
situation where you can imagine boosting
roll over fit if the underlying learners
all over fit and you can never get them
to stop overfitting then there's really
not much you can do interesting now I do
want to have a little semantic argument
with you for a moment Michael you use
the word strongest at some point when
you were talking about choosing the
weakest output and I just want to point
out that that doesn't really mean
anything well so what's a strong what
would you call a strong learner one that
is far away from it if we clone or just
has to do a little bit better than a
half it seems like a strong learner
would be something that would be really
close to being accurate right of course
on the other hand if by that definition
all strong learners are also weak
learners because anything that does
better than a half is still doing better
than a half which is all it requires to
be a weak learner yeah but that's kind
of true of people too like a strong
person is also weak person well it
depends how you define it so if you say
that a weak person is someone who can
lift at least their own arms then strong
people are also weak people in that they
can lift their own arms yes if you
define it that way and if I define blue
to be purple then I can say blue is
purple but that's not how people define
weak people they define weak people by
saying they can't lift more than not
that they can lift at least as much I
see so it's this piece of terminology
that boosting uses that is in error not
to me that's one interpretation it's not
the one that I would use but it's one
interpretation when you say something
like a strong learner I mean it makes
sense to use that kind of term sort of
throw it around and say well by a strong
learner I mean someone who's or a
learner that
going to overfit or is going to always
do really well on the training data but
in kind of a technical definition it's
very difficult to sort of pin down so
don't get too caught up in what a strong
learning means if you want to write a
proof good point and also compare yeah
also that this is this whole notion that
strong is sometimes defined as not weak
and it is not the case that if you have
something that's not a weak learner that
it's then it's a strong learner in fact
it's no learn learn no learner at all
exactly so weak learners just defined in
a way that basically says it gives me at
least some information good let me just
throw one more thing in here and then we
can we can stop talking about this
there's another a couple of other cases
where boosting tends to overfit the one
that matters the most or comes up the
most is in the case of pink noise did
you say peak noise I said pink noise I
even wrote it in red which looks like
pink it's a strong pink as opposed to a
weak pink I'm sorry there's no way for
that to be obvious from what we've
talked about but as a practical matter
pink noise tends to cause boosting
overhead okay but this is not a term I'm
familiar with unless you're critiquing
the musical stylings of a particular
performer no although I did recently see
see them in concert but that's a whole
nother conversation okay so pink noise
just means uniform noise I thought wait
noise was uniform noise no white noise
is Gaussian noise okay so peak noise is
uniform noise and white noise is
Gaussian noise this is why Michael by
the way if you ever try to set up a
studio or a cool stereo system in your
house you want a pink noise generator so
that it covers all the frequencies
equally not just a white noise generator
but boosting tends to overfit in those
sorts of those sorts of circumstances
and you can read more about it in the
notes if you want to but the one that I
really want people to get is that if you
have an underlying weak learner that
overfits then it is difficult for
boosting to overcome that because
fundamentally you've already done all
your overfitting and it's there's really
not much for boosting to do okay got it
got it excellent it all ties back in the
margins and it's all one big story which
i think is the lesson of all of machine
learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>