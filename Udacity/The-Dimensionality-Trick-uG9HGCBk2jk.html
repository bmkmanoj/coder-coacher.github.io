<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Dimensionality Trick | Coder Coacher - Coaching Coders</title><meta content="The Dimensionality Trick - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Dimensionality Trick</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uG9HGCBk2jk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so here's the dimensionality trick all
right and the notation that I'm showing
you here it comes directly from a paper
Turk and penland I mentioned a lady that
does eigenfaces but it'll be clear let's
suppose that each this cap CI is a face
image so it's really long right d long
and by the way let's assume we've
subtracted the mean ok so we've actually
removed the mean from all the pictures
that we have and so we've got the mean
at the origin fine alright so now let's
define a new matrix called see which is
just the average of a of fee of the sum
of fifi transpose ok this is the sum of
all the outer products and in fact this
can be written as a a transpose if a
this is what it says here is just the
stacked up face images right so a
stacked up face images it's gonna be d
really big by M and is however many face
images I have so it's huge by reasonable
that's what a is and that means a a
transpose so if i take d by M times M by
D I get a what D by D all right D by D
that is a really big matrix ok so if d
was 10,000 this would be a 10,000 x
10000 matrix how big is that Megan a lot
as a lot exactly that's about 100
million ok so if it was a million long
vector a d by d would be a million times
a million which would be a million
million which is a big number that's a
thousand billion which is a trillion
that's a lot you wouldn't ever want to
do that ok the idea of trying to find
the eigenvectors of a 10,000 x 10000
matrix is painful even on today's
computers and of a million by million
matrix is even more painful but
fortunately you don't have to so C equal
to a a transpose is a huge matrix but
now instead of considering a transpose
consider a transpose a
that's only an M by M matrix right
because remember a is d by M so a
transpose a is M by M finally the
eigenvalues and eigenvectors of that
pretty easy well suppose I've got some
eigenvector VI that's an eigenvector of
a transpose a so if it's an eigenvector
of a transpose a that means when i
multiply a transpose a times VI i just
get some multiplier lambda a VI the
eigenvalue pre multiply that equation by
a when you do that now I've got a a
transpose and i made this blues to make
the difference clear avi and by the way
I just pulled a lambda out on the right
hand side it's just a scalar I can do
that look at that equation what do you
see what do you see what do you see what
you see is that the AV eyes are the
eigenvectors of a a transpose remember a
transpose is that big ugly matrix C so
all we need to do is to find the VIS
which the eigenvectors of the a
transpose a pre multiply them by the
matrix a and I now have the eigenvectors
of a a transpose alright so what this
basically meant was we took the Duke the
eigenvectors of an m-by-n matrix and
created the eigenvectors of a d by d
matrix without having to explicitly
compute the eigenvectors of d by d this
is much easier to compute</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>