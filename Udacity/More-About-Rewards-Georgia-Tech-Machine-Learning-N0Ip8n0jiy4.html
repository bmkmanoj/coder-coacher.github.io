<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>More About Rewards - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="More About Rewards - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>More About Rewards - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/N0Ip8n0jiy4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so I want to talk about two things
on this little slide here the first one
is kind of a general observation about
rewards and what makes the reinforcement
learning
MDP problem different from the
supervised learning problems that we did
before and that's this notion of not
just rewards but this notion of delayed
rewards so what do I mean by that well
so if you think about the way we've set
up this kind of problem like like we
have here where we're trying to start in
this little bottom left-hand Square and
wind our way up into this plus one
there's really this notion of sequences
of actions so in the reinforcement
learning context and all the things that
we're talking about at least in the
foreseeable future there's this sort of
problem where you take some action and
that puts you in some place and then you
take another action and that puts you in
some place and then you take another
action and that puts you in some place
and maybe it puts you at a place where
you get plus one or maybe it puts you at
a place where you get minus one and what
really is going on here is this idea
that you take actions that will set you
up for other actions that will set you
up for other actions and only then do
you know how good those particular
actions you took work so this reward is
not just an idea of getting a reward at
every state it's an idea of getting
delayed reward so you don't know how
your immediate action is going to lead
to things down the road so let me give
you a concrete example about that so
have you definitely chests sure so let's
say we played a long game of chess and
maybe it took 60 61 62 moves
um and then at the end I win the game so
what do you know about the way you
played that game that I probably made a
bad decision around the time when I
decided to play chess against you that's
possible but maybe you made a good
decision and you kept making good
decisions and you only messed up at the
very last move when you could have mated
me but you didn't I see well my
experience in playing chess is it
usually that's not what happens usually
it's not that I make a bad move and then
there's a problem it's usually I make a
move that I think is reasonable that
only later I discover put me in a
position where I can't possibly do
anything good right well that's actually
a the game that I played in New York
many many years ago that I lost was
exactly like that the game went on for
like literally one
moves I really lost the game on the
third move I made a mistake i transpose
two moves because it was a new opening
that I was just learning and I knew at
the time that I screwed up I played
beautiful chess from that point on but
the truth is the other player had had a
positional advantage from that point on
that I could never overcome so I lost
the game but not because I played poorly
for you know 80 moves it's because I
paid played poorly for one move and that
move happened to be fairly early so this
is this notion of delayed reward that I
played this long game of chess and maybe
it's I played well I screwed up in the
end maybe I played me in a mediocre game
but I had a couple of brilliant moves
and that's why I won or maybe I played
very well in the beginning poorly at the
end or the other way around and the
truth is you don't really know all you
know is that you take it a bunch of
actions you get rewards signals back
from the environment like I won the game
or I lost the game and then you have
this problem of figuring out of all the
actions that I took what was the action
that led to me ultimately winning or
losing or getting whatever reward that I
I got at the end of a sequence does that
make sense
yeah it seems like that could be really
challenging you probably need your own
like sportscaster listening and
commenting yeah and in fact the
sportscaster is listening and commenting
at least in the MDP world is in fact the
sequence of rewards that you get in the
sequence of states that you see right it
really is sort of a play-by-play in this
state so if this action got this reward
and you want to take all of that and
then figure out whether this was a good
action your first action or a poor
action but your second action was good a
poor and so on and so forth now contrast
that with supervised learning so in
supervised learning what you would be
getting is while I was in this state
this first state and then this was the
proper action I was supposed to take
let's say action 17 and you're golden is
just simply to learn a function from
States to specific actions that's how
the supervised learning problem set up
right yeah exactly in this particular
case you're in some state you take some
action and you get some reward for the
action that you took or maybe the state
that you ended up in or something and
you get a sequence of these state action
reward triples and then ultimately you
have to figure out for the given States
you were in what was the action you took
that helped to determine or actions you
to help to determine the ultimate
sequence of rewards that you saw perhaps
this one plus one or this minus one that
you got at the end this is a really
difficult problem it's got its own name
it's called the credit assignment
problem and in fact because we're
talking about a sequence of events over
time we typically refer to this sort of
problem as the temporal credit
assignment problem that make sense
Michael yeah that's really cool I know
it I guess its credit but also blame
right well credit and blame me blame is
just the minus of credit so without loss
of generality assume that creditor can
assume a negative or positive value and
yet when I go to the supermarket they
never say blame or credit well that's
just because they don't understand the
technical term got it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>