<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Three Approaches to RL - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Three Approaches to RL - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Three Approaches to RL - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bFPoHrAoPoQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right we're gonna drill down and
talk about a specific reinforcement
learning algorithm in just a moment but
I wanted to remind everybody about some
of the quantities that Charles
introduced in the lesson last time when
he's talking about MVPs and use them to
describe three different approaches for
solving reinforcement learning problems
so this first box here PI maps States to
actions and what did you call this
Charles a policy that's right and
reinforcement learning algorithms that
work directly on trying to find the
policy are called policy search
algorithms so the good thing about
policy search is you're learning the
quantity that you directly need to use
right you're learning the policy that's
supposed to be the output so that seems
like a really good thing unfortunately
learning this this function is very
indirect we don't get direct access to I
was in the state what action should I've
chosen right so this is what did you
call this Charles the the temporal
credit assignment problem right so the
data doesn't tell you what action to
actually choose right and this is why
it's not exactly like the way we've
formulated supervised learning in the
past well at least if we're trying to do
policy search that's right but what
we're gonna do is now consider well
maybe that's not the quantity that we
want to learn let's let's think about
learning this function U which you had
said Maps states to values so what was
this guy U as a utility yeah yeah the
true utility of the state sometimes I
think I called it the value of the state
yeah so so sometimes this is referred to
as a value function and learning methods
reinforcement learning methods that
target that as what they're trying to
learn directly are called value function
based approaches and let's say that we
that we try to learn this we're trying
to learn to map states to values well
the good news is at least if we're
acting in the world we're getting to see
okay I was in some state I took some
action data and I can I can observe the
values that actually result from that
and maybe use that to make updates so
you can kind of imagine learning this so
the the the learning is not quite as
indirect how do we use this to decide
how to behave so we need to turn it into
a policy and we didn't quite we sort of
talked about how to do that in terms of
looking at the bellman equations we're
gonna we're gonna turns out it's
actually kind of hard to use you
directly
to do that but we're going to talk about
a different form of the value function
that's going to make that easy so this
is actually it's it turns out it'll be a
relatively easy kind of arc max
operation once we have the right kind of
value function so there's some
computation we have to do and there's
some indirectness to the learning but
you know it's okay
all right so the other quantities that
you told us about were T which is I
think of it as the transition function I
think you had a different name for it I
just called it the transition model the
model right yeah so this is the model
and and there are is there is the
reinforcement function the reward
function and what do these things do
they take states and actions and the
transition function or the transition
model predicts next dates
maybe probabilistically and the reward
function tells you next rewards but just
rewards and so this is a model jointly
and I already mentioned this but we call
methods that learn this quantity learn
these these functions and then use them
to do decisions model based
reinforcement learners so how do we go
from TN r to something like you well if
we had those if we had T and we had R
and we could see the esses and we then
actions we took then we could do you
know something like the value duration
we did before to learn values right
which valuation was used to solve the
bellman equations right so that is a
somewhat heavyweight computation to do
but it's doable so what what happens
over here is we actually have fairly
direct learning why is that because when
we're trying to learn the transition and
reward function we get state action
pairs as input and when then we received
next state reward pairs as output so we
can solve this as a supervised learning
problem hmm so so learning is rather
direct the usage of this is a little bit
computationally complex because you
actually have to do the planning and
then the optimizing to actually develop
what you're you know the policy doesn't
come directly out of that but but this
kind of gives you a sense of three
different ways three different ways
places that you can target the pieces of
the MDP so you can do reinforcement
learning I like that now we're gonna
focus on this this middle piece partly
because you know it's kind of the
Goldilocks
situation it's um you know with the
learnings not so indirect and the usage
is not so indirect there's just been a
tremendous amount of work focused on on
value function based approaches and it's
you know remarkably simple it turns out
if you do it right and also very
powerful that there's lots of ways to
use these simple ideas to learn hard
problems so I think this is a good place
to focus okay
I buy that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>