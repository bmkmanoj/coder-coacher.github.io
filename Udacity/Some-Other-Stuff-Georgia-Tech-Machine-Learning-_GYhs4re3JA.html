<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Some Other Stuff - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Some Other Stuff - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Some Other Stuff - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_GYhs4re3JA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Michael so we talked a little about
the curse of dimensionality but I think
it's worthwhile to talk about some other
stuff that comes up we've been sort of
skirting around this and you know bring
it up in various ways throughout our
discussion so far but I think it's
worthwhile kind of writing them all down
on a slide and trying to think through
them for a little bit
so the other stuff that comes up in K&amp;amp;N
mainly comes up in these sort of
assumptions we make about parameters to
the algorithm so the one we've talked
about probably the most is our distance
metric you know a distance between some
X and some the query point Q and we've
explored a couple we looked at Euclidean
and we looked at Manhattan and we even
looked at weighted versions of those and
this really matters so I've said this
before but I really think it bears
repeating that your choice of distance
function really matters if you pick the
wrong kind of distance function you're
just gonna get very poor behavior so so
I have a question about these these
distance functions so you mentioned
Euclidean and Manhattan are there other
distance functions that the students
should know like things that might come
up or things that they should think of
first if they have a particular kind of
data um yeah there's a there's a ton of
them I think well first off it's
probably worth pointing out that this
notion of weighted distance is one way
to deal with the curse of dimensionality
you can weight different dimensions
differently and that would be one and
you might come up with sort of automatic
ways of doing that that's that sort of
worth mentioning but you'll notice that
both Euclidian and Manhattan distance at
least as we've talked about them are
really useful for things like regression
they're they're kind of assuming that
you have numbers in there subtraction
sort of makes sense but there are other
functions you could other distance
functions you might do if you're dealing
with cases like I don't know discrete
data right where instead of it all being
numbers its colors or it's something
like that right so your distance might
be mismatches for example or it might be
a mixture of those in fact one of the
nice things about KN is that we've been
talking about it with points because
it's sort of easy to think about it that
way but this distance function is just a
black box you could take arbitrary
things and try to decide how similar
they are based on whatever you know
about the domain and that could be very
useful so you could talk about images
right where you
you take pictures of people and you know
rather than doing something like a
pixel-by-pixel comparison you try to
line up their eyes and look at their
mouths and try to see if they were the
same shape you know things like that
that might be more complicated and
perhaps even arbitrarily computational
to determine notions of similarity so
really this idea of distance and
similarity tells you a lot about your
domain and what you believe about it
another thing that's worth what's
pushing on a little bit is how you pick
K well there's no good way to pick K you
just you just have to know something
about it but I want to think about a
particular case well what if we end up
in a world where K equals in well that
would be silly why would it be silly
well so if K equals n then what you're
doing is you're taking so in the case of
regression for example you're taking all
of the data points and averaging the Y
values together basically ignoring the
query so you end up with a constant
function ah but that's only if you do a
simple average what if you do a weighted
average weighted average so the near the
points that are near the query are gonna
get more weight in the average so that
actually will be different even though K
equals n it will be different depending
on where you actually put your query
down exactly that's exactly right so for
example if I have a little bunch of
points like this say well you notice it
kind of looks like I have two different
lines here so I could pick a query point
way over here all of these points are
going to influence me as opposed to
these points and so I'm gonna end up
estimating with something that looks
more like this because these points over
here won't have much to say but if I
have a query point that's way over here
somewhere these points are gonna matter
I'm gonna end up looking something looks
a little bit more like this than like
that
now I'm drawing these lines they won't
exactly look like lines because these
points will have some influence they'll
be more curvy than that but the point is
that near near the place where we want
to do the query it'll look to be more
strongly influenced by these points over
here or these points over here depending
upon where you are well that gives me an
idea
Oh what kind of ideas they give you well
what about instead of just taking a
weighted average what about using the
distance metrics to pick up some of the
points and then do
a different regression on that subset of
points right I like that so we can
replace this whole notion of average
with a more kind of regression thing so
it actually instead of using the same
value for the whole patch actually still
continues to use the input values yeah
so in fact if average is just a special
case of a kind of regression right right
so this actually has a name believe it
or not it's actually called locally
weighted regression yeah so this
actually works pretty well and in place
of this sort of averaging function you
can do just about anything you want to
you could throw in a decision tree you
could throw in a neural network you get
throw in lines do linear regression you
can do almost anything that you can
imagine doing yeah and that works out
very well and again it gives you a
little bit of power so here's something
that I don't think is very obvious until
it's pointed out to you which is this
notion of replacing average with a more
general regression or even
classification function actually allows
you to do something more powerful than
it seems so let's imagine that we were
going to do locally weighted regression
and we were going to do in fact linear
regression so what would locally
weighted linear regression look like
well if we go back to this example over
here on the left basically you take all
the points that are nearby and you try
to fit a line to it and so you would end
up with stuff that look pretty much like
this
while you're over here you would get a
line like this but while you're over
here you would get a line like this and
somewhere in the middle you would get
lines it started to look like this and
and you would end up with something that
kind of ended up looking a lot like a
curve so that's kind of cool because you
notice that we start with a hypothesis
space of lines and this locally weighted
linear regression but then we end up
actually being able to represent a
hypothesis space that is strictly bigger
than the set of lines so we can use a
very simple kind of hypothesis space but
by using this locally weighted
regression we end up with a more
complicated space that is complicated
that's made more complicated depending
upon the complications that are
represented by your data points so this
results sort of reveals another bit of
power with kN which is it allows you to
take local information and build
functions or build concepts around the
local things that are similar to you and
that allows you to make arbitrarily
complicated functions neat okay cool
all right so you got all that think so
yeah okay cool so then I think we should
wrap up nice nice</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>