<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>TD What Have We Learned | Coder Coacher - Coaching Coders</title><meta content="TD What Have We Learned - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>TD What Have We Learned</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Tg2c-zSXisw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Charles so we've been talking about
temporal difference learning and in
particular various flavors of the TD
lambda algorithm can you remember some
of the important things that we touched
on well I think you you just said the
big one that this all this discussion
has really been about temporal
difference learning and in a nutshell
what's a temporal difference well it's
the difference between things you see on
different times its subsequent time
steps differen what kind of things in
different time sets reward so some sort
of the the difference in value estimates
as we go from one step to another right
that's exactly right because value is
just a nice summary statistic for long
term reward good all right but actually
I but actually I think it was important
that you know you started off talking
about this is the thing that we wanted
to do but first you put it in context by
giving by talking about different ways
that we could solve the reinforcement
learning problem and as I recall there
were sort of three ways to think about
it model based learning value based
learning and then let's say policy based
learning that's right and TD methods
kind of fall where the value based all
right and then we drove into the the
update rules themselves so what do we
talk about there well we actually do
road it derived that seems either one of
those things it's just semantics you
know this kind of incremental way of
building up estimates of the values and
I think you called it outcome based and
I suppose but and all that really means
of course is that you know you actually
look at the outcomes of what you
experience from different trajectories
and episodes through the space and you
use that to build up your your sort of
estimate of what the values of various
states are yeah and in particular that
the idea of an outcome based says well
you can kind of treat reinforcement
learning as a kind of supervised
learning just wait for time to stop and
see what happened and use that as a
training example but we can't really
wait for time to end because it's kind
of too late to use any of the
information there so what we want are
incremental methods methods that can
actually do updates along the way that
would be equivalent to waiting to the
end but actually gives us results sooner
right and there was a technical point
that you made that I guess is something
that we learned about which we had
talked about in the
machine learning course but we didn't go
into any detail about which is this sort
of incremental update by making it look
like a supervised learning rule just
look like the normal you know perceptron
update rule and it had a learning rate
and in order for us to be sure that we
were going to learn something we wanted
to learn that learning rate had to have
certain properties right that's exactly
right and in a nutshell what were these
properties that the learning rate had to
have well let's see that the sum over
all the learning rate values so let's
call that alpha T for time needed to be
infinite and that the sum of the
learning rates squared needed to be
finite yeah and so intuitively we wanted
learning rate sequences that would allow
us to move the value to wherever it
needed to go to converge but would
dampen over time decay so that so that
the estimates wouldn't keep bumping
around as as a property as a function of
the noise right I think that's great
well anyway I think that pretty much
brought us to the end I mean we took a
lot of slides to get there but it turned
out the the algorithm that we drove it
looked a lot like something we call TD
one gave it a nice little name and TD
one was very nice except it was
inefficient in the way that it used data
yeah I don't know that everybody would
agree exactly what that way of
characterizing it but it is true that it
had this sort of weird property that it
was there was a lot of variability in it
there was a very high variance because
if we had one run you know we had a
value that we're estimating from one
place and the estimate was based on a
very noisy example then we're gonna have
a very noisy estimate it wasn't going to
be able to use the rest of the examples
that it seen to kind of smooth this out
right it just sort of didn't get it
didn't use information along the way it
using information sort of at the end of
every episode and that's what made it
sort of data inefficient and I guess in
the end that gives you high variance but
we came to the rescue and when I say we
I mean you came to the rescue with TD
zero which has the nice property that it
gives you the maximum likelihood
estimate I prefer to think of it as the
Michael Wittmann estimate yeah I know
you do you know you snuck that in
and then I got us to the very end which
is good because you're running out of
space on the slide we generalize T t1
and t2 zero and there's something we
call TD lambda right that has the
property that when lambda 0 we get TT 0
when lambda is 1 we get TD 1 and what
about the intermediate values well then
that just gives you a dubstep wait what
gives you dubstep you get the dubstep
look ahead multiple steps of updates I
see what you mean by dubstep like double
double the number of steps oh yeah I
like that that sounds like I meant to do
that yeah let's go with that and there
you go I think that's just about
everything oh one more thing is that you
talked a little bit about different
values of lambda and why you might
choose one value over the other and it
turns out that values of lambda between
you know point three and point seven
empirically seem to work pretty well
yeah because it somehow is making a nice
trade-off between giving the you know
estimating the right kind of quantity
and estimating it using that you know
the values that you get from looking
ahead right and that's that's kind of a
first off that's neat because otherwise
you wouldn't need to have TD lambda but
it's in some ways a little surprising
why is that well because it just kind of
works out the way that you would want it
to work out you came to it you went
through all this trouble to kind of
generalize between things you have this
kind of highly nonlinear way of
combining all the different case step
look Ahead's and it turns out that
combining them so it's not quite 0 it's
not quite 1 is better than either 0 or 1
and often that's not how it works right
I mean often it's like a line from 0 to
1 as Betty you know it's like a line or
it you know it gets really better than
it it sort of flattens out towards the
end like you said but in this case it
just gets better and better and better
and then it starts to get worse when you
go from one end to the other yeah that's
right so often when you combine two good
things you get the worst of both and in
this case it does somehow manage to get
the best of both yeah pretty cool all
right so what I would like to dive into
next is a deeper understanding of how we
could show that these kinds of methods
actually converge and they actually
produce optimal estimates and optimal
behavior
in the limit and so we're gonna take a
step back and look at the bellman
equation again to do that oh I like it I
love the bellman equation alright see
you then then alright see ya bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>