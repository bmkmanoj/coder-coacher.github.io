<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Filters - Artificial Intelligence for Robotics | Coder Coacher - Coaching Coders</title><meta content="Filters - Artificial Intelligence for Robotics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Filters - Artificial Intelligence for Robotics</b></h2><h5 class="post__date">2012-05-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bjZy-RVms_8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let me ask you a few questions we had
measurement updates and motion updates
in the measurement update the computed
posterior overstate given the
measurement and there was proportional
to up to a normalization of probability
of the measurement given the state times
P of the state itself in a motion update
to compute a posterior of a distribution
one time step later and that is the
convolution of the transition
probability times my prayer now those
formulas don't look familiar
this is exactly what we implemented you
might not know we implemented this let
me explain to you how you implemented it
this distribution was a set of particles
a thousand particles together
represented your prior X these very
important weight intently speaking the
particles with the importance weights
are a representation of distribution but
we wanted to get rid of the importance
weights so by resampling we worked the
importance weight back into the set of
particles so the resulting particles the
ones over here would represent the
correct posterior you've implemented
this I'm just telling you what the math
is behind this this you also implemented
this was a set of particles again and
you sampled from the sum by taking a
random particle over here and applying
the motion model with the noise model to
generate a random particle X prime as a
result you get a new particle set that
is the correct distribution after the
robot motion so you recognize the math
and hopefully you understand how your
code implements this math you can prove
all kinds of interesting facts about
this math for example you can prove
convergence if the number of particles
goes to infinity is obviously
approximate particles are not an exact
representation and it was amazingly easy
to program so when you go over your
particle code you realize you
implemented a fairly involved piece of
math that is actually the same for all
the fields we talked about so far the
same math underlies our histogram filter
we talked about in class number one in
the same math for gaussians is the
common filter we talked in class number
two so let me ask you
and interesting question which of the
three filters did Sebastian use in his
job talk at Stanford histogram filters
common filters particle filters or none
of the above check one or all that apply
and of course you can't really know
unless you you Google me and look at my
home page then you might find out some
evidence so just take a random guess and
I tell you the answer in a second I
should say I was hired by Stanford in
2003 into a tenured associate professor
position so obviously my job talk wasn't
bad bad</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>