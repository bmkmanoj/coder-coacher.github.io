<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Principal Components Analysis - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Principal Components Analysis - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Principal Components Analysis - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kw9R0nD69OU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so the first linear transformation
algorithm we're going to discuss is
something called principal components
analysis okay Michael sure now just to
be clear here the amount of time it
would take me to derive principal
components analysis and work it all out
in its gory detail would take forever
and so I'm not going to do that I'm
going to leave that to a reading but I
want people to understand what principal
components analysis actually does and
what its properties are okay yeah
okay so principal components analysis
has a long history it's a particular
example of something called an eigen
problem okay
it's a particular example of something
called an eigen problem which you either
already know what that means or you
don't if you don't then it means you
haven't read the material that we've
given you so I'm going to ask you to do
that but whether you have or have not
let me just kind of give you an idea
what principal components analysis
actually does and I think the easiest
way to do that is with a very simple two
dimensional example so here's my very
simple two dimensional example alright
so you see this picture Michael yep so
this is a bunch of dots sampled from
some distribution that happens to lie in
a two dimensional plane like this okay
yep now what so this is in fact two
dimensions so we have two features here
we'll just call them X and y we could
have called them one and two it doesn't
really matter this is just the XY plane
and let me tell you what principal
components analysis actually does what
principal components analysis does is it
finds directions of maximal variance
okay does that make sense a variant of
what the variance of the data so if I
had to pick a single direction here such
that if I projected it onto that
dimension onto that direction onto that
vector and then I computed the variance
like literally the variance of the
points that were projected on there
which direction will be maximum I would
think it'd be the one that is sort of
diagonal it kind of blobs along that
particular direction right that's
exactly right and to see that imagine
that we projected all of these points
onto just the X dimension that's the
same thing as just taking that
particular feature well if we projected
all of them down we would end up with
all of our points living in this space
and when we compute the variance the
variance is going to be something that
captures the district
in between here and here does that make
sense yep similarly if we projected it
onto the y-axis which is the equivalent
of those are examples of feature
selection
yes exactly wait it's a feature
selection its equivalent of just looking
at the second feature here why I'm going
to end up computer having a variance
that spans this space between here and
here by contrast if we do what you want
to do Michael and we pick a direction
that is about 45 degrees if I drew this
right we would end up projecting points
between here and here now it's not as
easy to see in this particular case but
the variance of points as they get
projected onto this line will have a
much higher variance than on either of
these two dimensions and in particular
it turns out that for data like this
which I've drawn as an oval that you
know sort of has an axis that's 45
degrees this direction or axis is in
fact the one that maximizes variance so
principal components analysis if it had
to find a single dimension would pick
this dimension because it is the one
that maximizes variance okay you got it
sure
okay now what's the second thing
what's the second component that PCA or
principal components analysis would find
you know I don't know what you mean by
second this is now a direction that had
that has high variance not the first one
yes it seems like you know either the X
or the Y is pretty high or something
that looks just like that red line but
it's just a little bit tilted from it
would also be very very high right
that's exactly right so in fact what
principal components analysis does is it
has one other constraint that I haven't
told you about and that constraint is it
finds directions that are mutually
orthogonal so in fact since there are
only two dimensions here the very next
thing the principal components analysis
would do is it would find a direction
that is orthogonal or we think about it
in two dimensions as perpendicular to
the first component that it found
I see so there really is only really
only is one choice at that point that's
right or you know there's two choices
because you doesn't matter which
direction you pick in principle</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>