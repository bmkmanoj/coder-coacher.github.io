<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Regressão linear com gradiente descendente | Coder Coacher - Coaching Coders</title><meta content="Regressão linear com gradiente descendente - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Regressão linear com gradiente descendente</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/j5gs0m001Ak" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">when performing linear regression we
have a number of data points let's say
that we have 1 2 3 and so on up through
M data points each data point has an
output variable Y and a number of input
variables x1 through xn so in our
baseball example Y is the lifetime
number of home runs and our x1 and X and
our things like height and weight our 1
through m samples might be different
baseball players so maybe data point 1
is Derek Jeter data point 2 is Barry
Bonds and data point M is Babe Ruth
generally speaking we are trying to
predict the values of the output
variable for each data point by
multiplying the input variables by some
set of coefficients that we're going to
call theta 1 through theta n each theta
which will from here on out call the
parameters or the weights of the model
tell us how important an input variable
is when predicting a value for the
output variable so if theta 1 is very
small x1 must not be very important in
general when predicting Y whereas if
theta n is very large then xn is
generally a big contributor to the value
of Y this model is built in such a way
that we can multiply each x by the
corresponding theta and sum them up to
get Y so that our final equation will
look something like the equation down
here theta 1 plus x 1 plus theta 2 times
X 2 all the way up to theta n plus xn
equals y and we'd want to be able to
predict Y for each of our M data points
in this illustration the dark blue
points represent our observed data
points whereas the Green Line shows the
predicted value of y for every value of
x given the model that we may have
created the best equation is the one
that's going to minimize the difference
across all data points between our
predicted Y and our observed Y what we
need to do is find the Thetas that
produce the best predictions that is
making these differences as small as
possible
if we wanted to create a value that
describes the total errors of our model
we'd probably sum up the errors that is
sum over all of our data points from I
equals 1 to M the predicted Y minus the
actual Y however since these errors can
be both negative and positive if we
simply sum them up we could have a total
error term that's very
close to zero even if our model is very
wrong in order to correct this rather
than simply adding up the arrow terms
we're going to add up the square of the
error terms this guarantees that the
magnitude of each individual error term
Y predicted minus y actual is positive
why don't we make sure the distinction
between the input variables and output
variables is clear</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>