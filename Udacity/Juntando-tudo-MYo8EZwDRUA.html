<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Juntando tudo | Coder Coacher - Coaching Coders</title><meta content="Juntando tudo - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Juntando tudo</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MYo8EZwDRUA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so Sara's just shown you the code and
one of the nice things about using
Hadoop streaming is that it's really
easy to test your code outside of Hadoop
so let's take a look at how to do that
our mapper takes input from standard
input so in order to test it we can just
run it from the command line and type
data in to test it here I'm just typing
a standard input a couple of lines of
sample data six fields separated by tabs
and then when I hit control D to
simulate the end of input you can see
that the mapper outputs the results just
as I'd expect even better we can just
build a small sample data file and pipe
that into the mapper so let's do that
I'm going to just take the first 50
lines of purchases txt and save those
into a test file which I'll call test
file now I can just pipe that to the
mapper by saying cat test file pipe that
to mapa dot py and that gives mapper
that data as standard input and as you
can see the mapper produces its output
again to the command line if we have
problems we could just go back and edit
the mapper until it worked it's really
nice and fast to be able to do this
without having to run a complete Hadoop
job every time during the development
phase we can do a similar thing with the
reducer it's expecting a set of lines
each of which looks like the store name
then a tab then the value so again we
can create a sample file which looks
like that and pass it into the reducer
but even nicer we can test the entire
pipeline remember that the mappers
output is sorted by the Hadoop framework
and then pass to the reducer so we can
simulate the entire thing on the UNIX
command line like this I cat test file I
pipe it to the mapper I then pass that
to the UNIX sort command and pass that
output to the reducer when I run that
that simulates the entire map followed
by shuffle and sort followed by reduced
phase and as you can see I've got the
output from the reducer so now that we
tested this on the command line we can
test it on the cluster
the best practice when you're developing
MapReduce jobs is first test locally
with a small data set before you run
your code on the entire huge set of data
so now that we've tested on the command
line we can test our code on the cluster
best practice when you're developing
MapReduce jobs is to first test with a
small data set before you run your code
on your entire huge set of data but
we're already pretty confident here so
let's just run the thing on the whole
purchases txt file will use the HSA leus
to save some typing I specify my mapper
my reducer my input directory and a new
output directory which we'll call output
to and offered it goes it starts running
the job it turns out that we can see
what's going on on the cluster by taking
a look at the Hadoop job tracker web
user interface so you point your web
browser at the job tracker which on our
machine is just local host on port five
zero zero three zero and here you can
see that there's just one running job if
we click on the job name then that takes
us to a page which shows us the progress
of the job and a lot of other
interesting information as well we can
drill down and look at the individual
map or reduce tasks just by clicking on
the words map or reduce and here we can
see that two tasks have completed two
tasks are still running if I click on
one of the completed tasks I can even go
as far as to look at the logs from that
task so here's our job we're now 25% of
the way through our reducers as you can
see we get graphs at the bottom of this
page to show us exactly what's happening
with the job so we're nearly finished
with the job when the jobs finished
Hadoop FS minus LS of output 2 shows me
that just as I'd expect there's a part -
zero zero zero zero zero file in there
and just as we did before we can Hadoop
FS - cat that file to see our actual
results</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>