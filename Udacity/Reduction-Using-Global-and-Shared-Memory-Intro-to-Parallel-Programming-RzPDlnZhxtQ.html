<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Reduction Using Global and Shared Memory - Intro to Parallel Programming | Coder Coacher - Coaching Coders</title><meta content="Reduction Using Global and Shared Memory - Intro to Parallel Programming - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Reduction Using Global and Shared Memory - Intro to Parallel Programming</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RzPDlnZhxtQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so now we've done all the preliminaries
so let's turn to some actual code we're
going to implement this twice with a
similar strategy each time in both we're
going to implement a sum of a million
actually two to the twentieth elements
and we're going to do this in two stages
in the first stage we're going to launch
1024 blocks each one of which will use
1024 threads to reduce 1024 elements
each of those will produce one single
item so we're going to have 1024 items
left when we're done and we're going to
launch one block to reduce the final
1024 elements into one single element so
we'll post all the code of course but
the CPU side code is straightforward
instead we're just going to take a look
at the kernel let's see how that works
so each block is going to be responsible
for a 1024 element junk of floats and
we're going to run this loop within the
kernel on each iteration of this loop
we're going to divide the active region
in half so on the first iteration where
we start with a thousand 24 elements
we're going to have to 512 element
regions then each of 512 threads will
add its element in the second half to
its element in the first half writing
back to the first half now we're going
to synchronize all threads the sync
threads call right here to make sure
every one is done we've got 512 elements
remaining and so we're going to loop
again on this resulting region of 512
elements now we'll divide it into two
256 element chunks using 256 threads to
some these 256 items to these 256 items
and we're going to continue this loop
cutting it in half every time until we
have one element remaining at the very
end of ten iterations and then we'll
write that back out to global memory so
this works we can run it on a computer
in our lab so we're doing that now and
we notice that it finishes in 0.65
milliseconds less than a millisecond
that's pretty great but it's not as
efficient as we might like specifically
if we take a look at the code again
we're going to global memory more often
than we'd like on each iteration of the
loop we read in items from
global memory then we write back in over
two items then we read those in over two
items back from global memory and so on
in an ideal world we do an original read
where we read all of the 1024 items into
the thread block do all the reduction
internally and then write back the final
value and this should be faster because
we would incur less memory traffic over
all the CUDA feature we use to do this
is called shared memory and will store
all intermediate values in shared memory
where all threads can access them shared
memory is considerably faster than
global memory so let's take a look at
the kernel it's going to look very
similar and in this kernel we're going
to have the exact same loop structure
what's going to be different though is
this little part right here we have to
first copy all the values from global
memory into shared memory and that's
done with this little block and then all
the further accesses here are from
shared memory this s data as opposed to
from global memory which we did last
time and when we're done we have to
write this final value back to global
memory again the only other interesting
part of this code is how we declare the
amount of shared memory we need and we
do that here we're declaring that we're
going to have an externally defined
amount of shared data now we haven't
actually said how much we do so to do
that we're going to have to go down
before we actually call the kernel so
when we're calling the reduced kernel
using the shared memory we call it with
now three arguments inside the triple
chevrons the normal blocks and the
threads but then we say how many bytes
we need allocated in shared memory in
this case every thread is going to ask
for one float stored in shared memory so
the advantage of the shared memory
version is that it saves global memory
bandwidth to good exercise to figure out
how much memory bandwidth you'll save so
I'll ask that is a quiz the global
memory version uses how many times as
much memory bandwidth as the shared
memory version round to the nearest
integer</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>