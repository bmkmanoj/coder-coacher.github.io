<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Trajectories as TTD MDPs | Coder Coacher - Coaching Coders</title><meta content="Trajectories as TTD MDPs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Trajectories as TTD MDPs</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2LG1QqL7ujA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so in order to deal with those problems
we're going to introduce an extension to
Markov decision process is called TDD
MDPs as opposed to plain old MVPs TTD
and EDM dp's because it rolls off the
tongue what does it mean well it means
targeted trajectory distribution Oh
obviously and that actually you can see
that when you think about how we go from
an MVP to a TDD MVP so what we try to do
is we want to make explicit those things
we said before so instead of having
states we now have trajectories and
that's just all partial plot sequences
that we can't have that we can reach
that we can get to so it's the sequence
so far it's the story so far I try to go
back and forth between trajectories and
sequences as opposed to stories because
this is actually more general than the
story example that we're using actions
are exactly the same as they were before
we have a model but now because we have
trajectories instead of states we have a
model that says well if I'm in a
trajectory and I take a particular
action what's the next trajectory that I
will see hmm but it's still basically
the same thing except it really it
elevates this notion of trajectories to
sort of to a first class citizen and
that we're reasoning over trajectories
now and now instead of having a simple
reward we have a target distribution now
here I've made a sort of capital T
instead of a lowercase T and that's
because the lowercase T here was sort of
standing for trajectories the capital T
stands for final trajectories yeah that
is ends of stories so you don't have a
distribution or you guy guess you could
say have a probability of 0 for all
partial addition but all what you really
care about is stories that we care about
so things that have higher probability
our stories that you might consider good
and things that have lower probability
stories that you might consider bad or
less desirable does that make sense I
guess so but why not just pick the one
that we want and just and call that I
guess I guess the distribution where the
right story has probability of one and
everything else is 0 seems to make sense
right so that's the sort of strict
generalization of what we were talking
about this is a strict generalization of
what we were talking about before so
this allows us to say some things are
better than others and it turns out by
relaxing this hard constraint that I'm
trying to find I'm trying to maximize
reward but instead I'm trying to match a
distribution it allows more
replayability allows players to do a few
more
things because now our goal is not to
find a policy that maps states to
actions in order to maximize long-term
reward it's defined a probabilistic
policy that is a probability
distribution over actions okay and what
is the optimal policy it's the one that
if i use this probability distribution
would lead to be matching the target
distribution i sing right now and just
just so that I'm still on board here the
actions here are the things that the
story can do write the story or the
story manager or the system can do and
that and within the uncertainty for that
thing the uncertainty is well what is
the player actually going to do we don't
know right okay so it's not the player's
actions it's the stories act right right
exactly it's its story actions or system
actions or manager actions are lots of
ways to sort of think of things like
we're gonna make a wolf appear right
about now whoo yes exactly that was me
doing a wolf oh shoot I just it's really
scared or I'm going to make a key up
here well i'm going to unlock a door i'm
going to lock a door because i don't
want you to go in this room got it right
so these are the kind of actions that
you might be able to take in the system
the model says well if I'm in some
trajectory and I take one of these story
actions what plot point will I end up in
next what's reject we will I see next
that depends upon the player and so all
of your uncertainty here sort of
probably comes from the player that's
where all your entropy comes from okay
and I'm trying to get some stories in
some stories i want to happen more often
others but this allows the player agency
rendering this sort of probability
distribution now there's a lot of map
that i could do here but at the end of
the day what you might think is that
well now that I no longer have this sort
of simple constraint of maximizing my
long-term expect a reward but I have
probabilities and randomness and
probabilistic policies this is going to
be harder much harder to deal with oh
but it turns out it's not as is often
the case in machine learning in AI if
you relax a hard constraint and make it
a soft constraint you make it something
that's more about probabilities in
probabilistic sort of search you
actually get more power out of it and
you're able to solve hard problems so
without going into the details I can
point you to a paper that you can
actually see
the papers it can read here it turns out
that by going from a hard constraint to
probability distributions you can
actually solve a TDM DP in linear time
linear in what linear in the length of a
story that seems unlikely well I could
prove it to you but instead what I'm
doing have you read the read the paper</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>