<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hmmmm | Coder Coacher - Coaching Coders</title><meta content="Hmmmm - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hmmmm</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fBtJPEmjsRE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">So, or hm, that's a joke.
Did you get that joke, Megan?
You got the joke.
She's quiet today,
which is an unusual thing for Meg.
Here's how it works,
if we have the HMM, so
if we know the HMM,
we know a and b and pi.
We can use the forward algorithm and
the backward algorithm, and
this is, part I'm not going to show you.
To estimate, that's what's cap E.
The distribution of what state I'm
likely to be in a time t, okay?
because one has to do with
the sequence is forwards,
one has to do with
the sequence is backward.
I can evaluate the probability of all,
of the whole sequence.
That allows me to estimate,
what's typically called gamma i of t,
the probability that I'm in state one
at time t, state two, state three,
all right?
I estimate that distribution.
Well, if you give me that distribution,
and I've actually seen
at every time t what was observed, then
given the likelihood that I'm in each of
the states, probability that I'm in each
of the states, and the observed data,
I can figure out what
the most likely b sub j of k.
That is, what do I think
the emission probabilities are?
And I picked the ones that
would maximize the total
observation probability.
It's no different than saying maximum
likelihood, you know, if, if I, if I'm
drawing something, and I pull, you know,
seven, I pull ten things out, and seven
of them are black, and three are red, I
say there's a 70% chance of being black.
That value is the one that
would've maximized the likelihood
of pulling out seven of them.
It's a maximization procedure.
So I can choose the b sub k's that
maximize the total observation,
given those probabil,
the idea which state I'm in, okay?
Also, given that distribution,
I can also say well, if I think there's
a 75% chance that I'm in
state one at this time, and
a 90% chance that I'm in state two at
the next time, that tells me that well,
you know, the likelihood of going from
one to two must be kind of high, right?
And basically by looking over all of
time and looking at my guess about
the states, I can come up with the best
possible transition probabilities.
And when I say best, is when, again,
the ones that maximize the probability.
That gives me my new a i j's,
and I've got new b k's.
I can also guess new pi's,
but those never matter.
That's my new machine, right?
So after I do the end step, called the
maximization step, of getting the new
A's and B's, I now have a new machine,
I can go back and do the E step again.
I can estimate the probabilities of
being at a particular state of time,
T, and I redo that process.
And that iterative process,
it's called expectation maximization.
The particular algorithm for HMMs
was called the Baum-Welch algorithm.
Like I said, there's plenty of tutorials
out there for you to look at it.
The fundamental idea is that by using
those recursive algorithms, and
then estimating the probability of being
at a particular state at a particular
time, I can re-estimate the machine.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>