<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What Have We Learned - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="What Have We Learned - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What Have We Learned - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Q5v-HAWXBEI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so actually that was all we were going
to talk about in this lesson about
computational learning theory so let's
just recap where we went so far okay so
uh what you want me to do it yeah that's
been our technique all along I find so
here you're the teacher and I'm the
student I get that which is actually one
of the things that that we talked about
aha we talked about what it would mean
to be a learner versus being a teacher
and and how teachers and learners
interact to make learning happen faster
or not okay but that was actually in a
larger context which I thought was kind
of cool which was this sort of notion of
trying to understand what's actually
learn about right and I think the
comparison that made sense to me was
that we were trying to do the equivalent
to what we do in computer science with
complexity theory and algorithms we're
here we were bouncing up from a specific
algorithm like decision trees or KN and
asking a question about how
fundamentally hard is the problem of
learning good yeah like that and we
focused on a particular measure of
difficulty which I guess drove
everything else we talked about which
was sample complexity how many examples
how many samples do we need in order to
learn some concept good yeah it's a
really powerful idea because it's a
different resource than what's normally
studied in computer science things like
time and space this is now how much data
do we need right which makes sense
because we do machine learning and
machine learning people what we care
about is data I saw a t-shirt recently
that says data is the new bacon mmm so
you're saying data is delicious yeah I
think we like date a lot I love data
okay so that ties us back into
discussion about teachers and students
because what we talked about was how if
the relationship it to the teacher and
the student was one way versus another
way we might get different answers about
sample complexity
so in particular we talked about what
would happen in a world where the
learner had to ask all the questions and
that's powerful because the learner
knows what the learner doesn't know but
the learner doesn't know what the
learner needs to know so that is
somewhat powerful but it may be useful
for the teacher to be more involved
right so that's the other thing where
the teacher gets to actually pick the
questions
great and then the third sort of case
was where the teacher didn't really pick
the questions or the teacher didn't have
an intent to pick the questions but the
teacher was in fact nature so like a
fixed distribution yeah good right and
some of those are you know easier to
deal with than others like the teacher
since the teacher knows the answer can
ask exactly the right set of questions
that get you there very quickly versus
say when the teacher is just nature and
you know you get it according to
whatever distribution there happens to
be sort of oblivious maybe is a better
word I think unfeeling nature just
doesn't care about me I think nature
cares about you just as much as nature
cares about everyone else that's exactly
what I was afraid yeah okay so let's see
what else do we cover so we talked about
mistake bounds as a different way of
measuring things how many mistakes do
you make as opposed to how many samples
do you need that was kind of neat there
was some tie in there and then the bit
that I like a lot is we started talking
about version spaces and pac
learnability and what really worked for
me with that was this distinction
between training error which we talked
about a lot test error which is how
we've been thinking about all of the
assignments we've been doing and true
error and true error in particularly are
connected back to this this notion of
nature right the distribution D right
and then you introduce the notion of
epsilon exhaustion aversion spaces and
gave us an actual sample complexity
bound for the case of distributions in
nature and a sample complexity bound is
pretty cool because it depends
polynomial e on the size of the
hypothesis space and the target error
bound and the failure probability so
actually that reminds me I had two
questions about this model uh-huh so the
first question was that equation m
greater than or equal to one over
epsilon times the quantity natural log
hypothesis face plus natural log 1 over
Delta close quantity assumed that our
target concept was in our hypothesis
space didn't it yes that's true so what
happens if it isn't then we have a
learning scenario that's referred to in
the literature as agnostic that the
learner doesn't have to
have a hypothesis that is in the target
space and instead needs to find the one
that fits nearly the best of all the
ones in there so it doesn't have to
actually match the true concept it has
to it has to get close to the best in
its own collection okay well so do we
get the same bounds it's very similar
bound I think I think maybe there's an
extra epsilon there's an extra squared
on the epsilon hmm okay I think there's
maybe slightly different constants in
here so it's it's a very similar form
it's still polynomial it is worse though
because the learner has kind of less
strength to depend on okay that's fair
okay so then my second question was I
just realized staring at this now since
you wrote it in red that the bound
depends upon the size of the hypothesis
space indeed so what happens if we have
an infinite hypothesis space well
according to this bound to technical
term is your hosed those ever the H
stands for yes hmm
so M would be greater than one over
epsilon times the natural log of
infinity which I'm pretty sure is
infinity yeah even with the even once
you multiply it by 1 over Epsilon
so yeah you know this is a really
important issue and I think it really
deserves its own lesson so let's let's
put this off to lesson eight you're
right that infinite hypothesis spaces
come up all the time they're really
important they almost everything we've
talked about so far in the class like
actual learning algorithms deal with
infinite hypothesis spaces we would
really like our bounds to deal with them
as well yeah we're like that so I know
anything else to talk here or should we
say without further ado let's move on to
the next lesson
I think we should say without further
ado let's move on to the next lesson
all right then without further ado let's
move on to the next lesson okay well
then I will see you next time Michael
sure yeah thanks thanks for for
listening oh you know I enjoy doing it
so much bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>