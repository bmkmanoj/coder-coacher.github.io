<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bayes Filters | Coder Coacher - Coaching Coders</title><meta content="Bayes Filters - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bayes Filters</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qDvd5lu80bA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what I want to do is work through the
the slides here on the Bayes filter
getting us to how we come up with our
new belief okay so this was just our
definition before that the belief is the
probability about the current state
given everything including the current
measurement so we're gonna use the Bayes
rule that we just showed that says we
can swap that around by putting in the
likelihood and the prior and the prior
is just XT without the ZT over here
right that was our prediction about what
XT was and this is this likelihood model
right this is just our sensor likelihood
model what's the likelihood of getting
that value
all right well our sensor independence
that we talked about before says if you
have the state XT then none of this
stuff matters and it's just P of ZT
given XT that's sensor independence all
right so here we are so the next thing
we can do is remember we talked about
this before this notion of total
probability the way we do a prediction
okay for any given XT what we have to do
is say well what's our prediction of all
the possible XT minus 1 and what Y there
many of them well all the possible X's
at t minus 1 if we take all the
probability okay that we would we sum up
over all those possibilities and here
you notice what we said was what's the
probability of going to XT from XT minus
1 times this probability of XT minus 1
summed up over all of those XT minus
ones that will be the the prediction
that will be the prior but this value
actually can get made much smaller
because notice that it has this term in
that term because remember the Markovian
property says that if we have XT minus 1
and we want to predict XT all this other
stuff doesn't matter except the action
right so basically if you give me where
I thought I was before and what action
you did I can tell you about where I
I am now and that's what this Markovian
step is right there one more time flies
up Wow beautiful okay keep going all
right well now actually we're pretty
much done and now everything is
beautiful and color-coded all right we
start here right this value right here
that's just our belief about all the X's
at the last time step so that's written
here so now we're already looking at a
recursive algorithm right because on the
left hand side we have the belief of X T
on the right hand side we have the
belief of XT minus one so this is a
induction remember we talked about
tracking his induction this is an
induction formula that goes from your
belief at t minus 1 to your belief at t
when you integrate or sum over all the
possible XT minus ones these are your
predictions before taking the
measurement so this is just like we did
with the kalman before to make your
predictions what you do is you take your
belief about where things were you pump
it through your dynamics and you add it
onto your uncertainty and then what this
is showing is to get our new belief okay
we just multiply that by the likelihood
of the measurement remember it's not the
it's not your probability of where you
are given the measurement it's how
likely would this measurement have been
if that had been the particular X and
then we can worry about this
normalization later that's how we get
the belief at new time t</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>