<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ecossistema Hadoop | Coder Coacher - Coaching Coders</title><meta content="Ecossistema Hadoop - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ecossistema Hadoop</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4sZ7n-Wg9Dc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we've been talking about core Hadoop
which consists of HDFS and MapReduce but
since the project was first started an
awful lot of other software has grown up
around it and that's what we call the
Hadoop ecosystem some of the software is
intended to make it easier to load data
into the Hadoop cluster
well lots of it's designed to make
Hadoop easier to use for example as
you'll see in the next lesson
writing MapReduce code isn't completely
simple you need to know a programming
language such as Java Python Ruby or
Perl but there are lots of folks out
there who aren't programmers but can
write SQL queries to access data in a
traditional relational database system
like sequel server and of course lots of
business intelligence tools want a way
to hook into Hadoop for that reason
other open source projects have been
created to make it easier for people to
query their data without knowing how to
code two key ones are hive and pig
instead of having to write mappers and
reducers in hive you just write
statements like this which looks very
much like standard SQL the hive
interpreter turns the SQL into MapReduce
code which then runs on the cluster an
alternative is Pig which allows you to
write code to analyze your data in a
fairly simple scripting language rather
than MapReduce again the code is just
turned into MapReduce and run on the
cluster hive and pig are great but
they're still running MapReduce jobs
which as you'll see can take a
reasonable amount of time to run
especially over large amounts of data so
another open source project is called
Impala Impala was developed as a way to
query your data with SQL but which
directly accesses the data in HDFS
rather than needing MapReduce Impala is
optimized for low latency queries in
other words Impala queries run very
quickly typically many times faster than
hive while hive is optimized for running
long batch processing jobs
another project used by many people is
scoop scoop takes data from a
traditional relational database such as
Microsoft sequel server and puts it in
HDFS as delimited files so it can be
processed along with other data on the
cluster then there's flume
which ingest data as its generated by
external systems and again puts it into
the cluster HBase is a real-time
database built on top of HDFS and
there's more Hugh is a graphical
front-end to the cluster Guzzi is a
workflow management tool mahou is a
machine learning library in fact there
are so many ecosystem projects that
making them all talk to one another and
work well can be tricky to make
installing and maintaining a cluster
like this easier
Claude era the company we work for has
put together a distribution of Hadoop
called CDH CDH or the cloud era
distribution including Apache Hadoop
takes all the key ecosystem projects
along with Hadoop itself and packages
them together so that installation is a
really easy process and the components
are all tested together so you can be
sure there's no incompatibilities
between them of course it's free and
open source just like Hadoop itself
while you could install everything from
scratch it's far easier to use CDH and
that's certainly what we'd recommend in
the next lesson in fact you'll be
downloading and running a virtual
machine which has CDH installed for more
information on the Hadoop ecosystem and
how each of these components works see
the instructor notes</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>