<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>OS for Parallel Machines - Georgia Tech - Advanced Operating Systems | Coder Coacher - Coaching Coders</title><meta content="OS for Parallel Machines - Georgia Tech - Advanced Operating Systems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>OS for Parallel Machines - Georgia Tech - Advanced Operating Systems</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ph-eUJ76ZVY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">modern parallel machines offer a lot of
challenges in converting the algorithms
and the techniques that we have learned
so far in the scalable implementations
now what are some of these challenges
well first of all the size bloat of the
operating system and the size bloat
comes because of additional features
that we have to add to the operating
system and so on and that results in in
system software bottlenecks especially
for global data structures and then of
course we already have been discussing
this quite a bit that the memory latency
to go from the process in a memory is is
huge all the cores of the processor are
on a chip and if you go outside the chip
to the memory that latency is huge 100
to 1 ratio is what we've been talking
about and that latency is only growing
the other thing that happens when
parallel machines is the fact that this
is a single node and we're talking about
the memory latency going from the
processor to a memory but in a parallel
machine it's typically constructed as a
non-uniform memory access machine and
that is you take individual nodes like
this that contains a processor and
memory and put all of them together and
connect them through an interconnection
network and what happens with this Numa
architecture is that access there is
differential access to memory whether
this processor is accessing memory that
is local to it or it has to reach out
into the network and access a memory
that is farther away from where it is in
addition to the numerous aked there is
also the memory hierarchy itself itself
is very deep we already talked about the
fact that a single processor these days
contains multiple levels of caches
before it goes to the memory and this
deep memory hierarchy is another thing
that that you have to worry about in
building the operating system for a
parallel machine and there is the issue
of false sharing and false sharing is
essentially saying that even though
programmatically there is no connection
between a piece of memory that is being
touched by a particular thread executing
on this core and other thread that is
executing on
this core the cache hierarchy may make
the block that contains the individual
memory touched by different threads on
different course to be on the same cache
block so programmatically there's no
sharing but because of the fact that the
memory that is being touched by a thread
on this core and the memory that is
being touched by a thread on this core
happened to be on the same cache line
they appear to be shared that's what is
false sharing false sharing is
essentially saying that there is no
programmatic sharing but because of the
way the cache coherence mechanism
operates they appear shared and this is
happening more and more in modern
processes because modern processors tend
to employ larger cache blocks why is
that well the analogy I'm going to give
you is that of a handyman if you're good
at doing chores around the house then
you might relate to this analogy quite
well you probably have a toolbox if
you're a handyman and if you want to do
some work let's say a leaky faucet that
you want to fix what you do is you put
the tools that you need into a tool tray
and bring it from the toolbox to the
site where you're doing doing the work
and basically what you're doing there is
you know collecting the set of tools
that you need for the project so that
you don't have to go running back to the
tool tray all the time that's the same
sort of thing that's happening with
caching and memory memory contains all
this stuff but what I need I want to
bring it in and the more I bring in from
the memory the less time that I have to
go out to memory in order fetch it that
means that I want to keep increasing the
block size of the cache in order to make
sure that I take advantage of spatial
locality in the cache design and that
increases the chances that false sharing
is going to happen the larger the cache
line the more chances are that memory
that is being touched by different
threads happen to be on the same cache
block and that results in false sharing
so all of these effects the Numa effect
the deep memory hierarchy and increasing
block size leading to fault sharing all
of these are things that the operating
system designer has to worry about in
making sure that
the algorithms and the techniques that
we've learned when it is translated to a
large-scale parallel machine it remains
scalable so that's really the challenge
that the operating system designer faces
so some of the things that the OS
designer would have to do is work hard
to avoid false sharing work hard to
reduce right sharing the same cache line
because if you right share the same
cache line then it is going to result
among different cores of the same
processor then it was gonna result in
that cache line migrating from one
processor to another and even within the
same core and even within the same
processor multiple cores and across
processors that are on different nodes
of parallel machine connected by the
interconnection network</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>