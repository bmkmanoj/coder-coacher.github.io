<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Gradient Descent - Artificial Intelligence for Robotics | Coder Coacher - Coaching Coders</title><meta content="Gradient Descent - Artificial Intelligence for Robotics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Gradient Descent - Artificial Intelligence for Robotics</b></h2><h5 class="post__date">2012-03-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/umAeJ7LMCfU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so we saw a lot of confusion in the
forums about what was going on with this
gradient descent the equations that were
given to you were sort of presented as a
black box and I'd like to open up that
black box and see what's going on inside
so for those of you who looked at
Wikipedia to try to understand what's
going on with gradient descent the first
formula you would have encountered was
this one and this equation describes
gradient descent now this is a little
intimidating at first what do all of
these parameters mean B equals a minus
gamma times the gradient of the function
evaluated a let's figure out what's
going on here so before we get into the
gradient descent let's first answer the
question what is a gradient and to help
me answer this question I'm going to get
the help of my blindfolded hillclimber
so here's our blindfolded hillclimber
and he's climbing this rounded hill he
wants to get to the top and he wants to
do it in as few steps as possible
what is his method going to be well as
long as the hill is steep he knows he
can afford to take a really big step
because he's still not really close to
the top as the hill smooths out and
becomes more level he's going to want to
take smaller and smaller steps because
he's going to be afraid of overshooting
the top and winding up on the other side
of the hill so how can we describe this
mathematically and the way we're going
to do that is by using the gradient and
when we do that our climber is going to
get to the top of the hill so let's look
at this hill from a top-down view
instead of from the side and I'm going
to draw these isoclines so this these
lines all represent lines of the same
altitude and we can see they come
together they're really close together
in the beginning and become further and
further apart as we go inside and that's
because the hill is getting less and
less steep so at any point the gradient
is a vector if I took the gradient at
let's say this point I would get a
vector pointing in the direction of
steepest descent and the length of that
vector would depend on exactly how steep
the hill is so at this point which maybe
corresponds to somewhere down here on
the hill the hill is quite steep so our
vector is quite long and it points
directly
uphill if I was over here and taking the
I would still be pointing uphill but the
vector won't be quite as long now
because now we're up here and the slope
isn't so great and with a reduced slope
comes a reduced gradient and a reduced
step size for a blindfolded Hill Climb
gradient descent is very similar to
blindfolded hill climbing now instead of
climbing up a hill we're climbing into a
valley and trying to find the bottom of
it because we want to minimize a
function that minimization is encoded in
this minus sign so instead of adding the
gradient we're going to subtract it and
let's talk about what each of these
variables mean so a is our current
position if we were the hill climber it
would be right here where he starts off
B is going to be his next position so
this is how the hill climber decides
where to go next
this gradient term tells us the
direction of steepest ascent and this
minus sign flips that around and says
okay let's actually find the direction
of steepest descent and this gamma is
just a weighting factor so that's all
this formulas doing it's saying let's
take a step down the hill and use that
as our new equation or nua position
sorry
and now to elaborate on this we're going
to need to use a little bit of calculus
so if you don't know calculus don't be
intimidated try best to follow along and
if you can't that's okay too I think
this kind of captures the essence of
what's going on with gradient descent so
first we have to figure out what exactly
is the function we're trying to minimize
for the purposes of the lecture the
function we were trying to minimize was
a function of Y I and it was equal to
some weighting alpha times X I minus y I
squared plus some weighting beta times y
I minus y I plus 1 squared so the next y
coordinate and we also do the same for
the previous y coordinate so Y I minus 1
so we want to minimize this and we're
going to use gradient descent
well gradient descents is that we should
just iterate over this process until we
get to a sufficiently shallow slope that
we're confident or in the bottom we've
really found a minimum so at each step
we're going to follow this be our new
location becomes Y I prime that's going
to equal our old location Y I and then
minus this gradient and the gradient
here is just
going to be the derivative with respect
to Y I if you don't know calculus again
don't worry about it and if you do this
is a fairly simple derivative so I'm
going to write it simplified I'm not
going to show you all the sub
intermediate steps but once we do this
out we find that Y I prime equals y I
plus 2 alpha times X I minus y I plus 2
beta times y I plus 1 plus y I minus 1
minus 2y I and this almost almost looks
like the equation that you're given in
class there's a little problem and
that's these twos and I'm going to do
something where I just erase them and
you may be saying hey you're not allowed
to do that but let's just pretend I'd
originally called these parameters alpha
over 2 and beta over 2 then everything
will work out just fine
so that's gradient descent that's where
this equation comes from in our update
step this would be simultaneous update
another good way of using gradient
descent it's not how we implemented it
in lectures but that's ok and I hope
this was helpful good luck</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>