<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Use Of Ply Library - Programming Languages | Coder Coacher - Coaching Coders</title><meta content="Use Of Ply Library - Programming Languages - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Use Of Ply Library - Programming Languages</b></h2><h5 class="post__date">2012-06-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OV7qofw92MM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to the unit to office hours and
one of the first questions I'd like to
address deals with the PL y library that
we're using under the hood for this
class to help us out with lexing and
parsing in the lectures we talked about
how to write down a token definition you
define a function t underscore or
something like a number and then you'd
write out the regular expression 0
through 9 plus then you might do a
little work modifying the token value
may be converting it from a string to an
integer and then eventually you return
the token and the basic question from
students is how does that work what's
going on so after you write down a
number of these token definitions
there's a library behind the scenes part
of the grading scripts that we're using
but also available for you to download
if you'd like to try it out on your own
that gathers up all of your token
definitions one way to do this is using
a technique called reflection which
actually sounds very philosophical but
is a way for a computer program to look
at itself and all of its own
capabilities in essence our Python
program asks has anyone defined any
functions recently that start with T
underscore if so the next thing we have
to do is get that regular expression out
of them and it turns out that Python
functions allow you to write
documentation or a brief explanation at
the beginning of any function this is
sometimes a good software engineering
practice our Python parsing library in
our Python lexing library we use this
power we're writing down this regular
expression and the library is treating
it as if it were documentation we use
that to get access to the regular
expressions you've written down so step
one you write down a bunch of these
token definitions and then we look using
reflection for example to find all of
them step two we go through all of them
and ask do they have any strings at the
beginning anything that looks like
documentation the answer is yes but for
us it's not documentation it's a regular
expression specification the next thing
to do is can
very each of those regular expressions
to a finite state machine now I hinted
at this in class didn't give a full
formal proof on how to do it but it
turns out that every one of the regular
expressions plus star disjunction
concatenation can be written out in a
finite state machine for every regular
expression there's at least one and in
fact typically infinitely many finite
state machines that accept exactly the
same language so we'll just apply that
conversion in the background but if you
have a bunch of different token
definitions 14 number 14 string one for
some key words in your language like if
then or else we're going to end up with
a bunch of different finite state
machines so now we have to combine them
all together and if you're willing to
humor me with non deterministic finite
state machines we could actually do that
just by putting a special state at the
beginning special new start state and
having an epsilon transition go to the
beginning of each of our old states will
blew them all together into sort of a
Frankenstein's monster and amalgamation
of everything we've looked at we're
almost done except that it's not enough
for a lexer to know this string is it
token we have to know which one it is so
real life flexor will have one more bit
of information that we didn't talk about
in class a state isn't just an accepting
state it's an accepting state with a
little label if you accept here it was a
string if you accept in this over there
it was the token then if you accept in
this state down here it's a number so
instead of just knowing what the
accepting states are we need to know
what the accepting states are and what
token each one corresponds to all right
so you did all your token definitions we
found them all each one had a regular
expression we found that we converted
each of those down into finite state
machines we joined them all together we
labeled all the accepting States and now
we represent that big finite state
machine internally as edges this is
sometimes formerly called a transition
table but it's just like the edges
encoding that we used in class and now
when it comes time to actually do lexing
to break a string down into import
words and tokens you just feed the
characters of the string one at a time
into that big finite state machine it's
exactly the same as the fsm sim
procedure we went over in class it's
just that they're finite state machine
is much bigger but in fact the fsm sim
code that we wrote would work just as
well even on bigger finite state
machines I just didn't show it because
they're harder to draw on screen so
under the hood this library is basically
just doing a bunch of grungy details
chores busy work in the background we
did exercises with one or two regular
expression definitions the library
gathers them all up together in one
place so you're already familiar with
all of the key concepts in making a
lexical analysis library the library
just does a lot of the busy work for you
and that's the explanation</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>