<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Multi Threading Performance - Georgia Tech - HPCA: Part 5 | Coder Coacher - Coaching Coders</title><meta content="Multi Threading Performance - Georgia Tech - HPCA: Part 5 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Multi Threading Performance - Georgia Tech - HPCA: Part 5</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZpqeeHFWxes" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so let's discuss the performance of
multi-threading and how can it be better
than simply running things one at a time
so if we have a processor with no
multi-threading support then this is how
two threads might run these are cycles
going in this direction so each knotch
here is a cycle and we have four
instructions that can execute in this
cycle so this is what might happen a
green thread might be using this and
this slot in this cycle and the other to
issue slots are not used because the
processor has no read instructions to
dispatch to execution in this cycle
let's say there is a data dependence on
next cycle one instruction can wake up
and then let's say that in the next
cycle we can have three instructions go
but then let's say that all of the
instructions are now waiting for
something to happen so let's say for a
few cycles we don't get much happening
then when it happens we get to execute
let's say all four and then maybe
another cycle with two etc so things can
be like this basically much of the time
we are running with a less than fully
utilized processor pipeline let's say
that at this point we get an interrupt
and because there is another thread that
is ready to run we now get many cycles
of the operating system doing its thing
so it's also going to utilize the
pipeline in some way but it spends many
cycles basically figuring out which
thread to run next eventually decides
that it's going to be the blue thread
and begins executing instructions from
that thread on the processor so this is
where we return from the interrupt and
start executing a new thread because
let's say we have these two threads
ready to go so the blue thread in a
similar way will be using the processor
partially let's say it has a level 1
miss takes a while to get the thing from
level 2 meanwhile maybe it's able to do
something for a while but eventually
runs out of things to do and then for
example starts doing something again and
let's say this whole cycle is used so
this is what might happen in a processor
with no multi-threading support in that
case really
the overhead that you created by having
to switch between threads is probably
larger than the gains we are getting
from running them we were better off
just running a single thread that does
the work of both of these threads so
really a single processor with no
multi-threading is capable of doing this
with a good operating system not because
it's better for performance to break
work into multiple threads but because
we want to be able to simultaneously run
several applications and make progress
in them so that for example if you're
playing games and playing music at the
same time we alternate between the game
and the music player fast enough for you
to not be able to tell that they are
really executing one at a time on your
one core if we have a chip
multiprocessor we get to do something
like this now we have two cores each of
which pretty much gets to execute one
thread so it's faster of course than
this and we can benefit truly from being
able to write multi-threaded programs
but you have to have two cores for it so
the cost is about twice the cost of a
single core here so with fine grain
multi-threading what we have is really
one core but with separate sets of
registers for these threads and some
scheduling logic with fine grained
multi-threading every cycle we can
switch between threads so what happens
is the green thread gets exactly same
behavior in the first cycle in the
second cycle we do the blue thread so we
get the behavior of its first cycle here
then we do some of the work from the
green thread again then from the blue
thread the red cycle of the green thread
the red cycle of the blue thread and now
we would be running the green thread
except that there is nothing to run
there but what happens is the blue
thread has something to do so we spend
another cycle on the blue thread and
then the green thread is still somewhere
here there is nothing ready to run but
these operations are going on so the
blue thread gets one more cycle here at
this point the blue thread has nothing
to do but the green one now has
something to do
so we end up doing the work of the green
thread and then for another cycle then
let's say neither of the threads can do
something but then the blue thread is
able to do this because it's been three
cycles since here and then again and as
you can see we get done faster than here
because we are able to eliminate some of
the cycles where we were not able to do
anything so fine grained multi-threading
benefits from the fact that when there
is a long period of idleness in one
thread for example due to a cache miss
you might be able to run still
instructions from the other thread and
finally let's see what happens in SMT in
SMT we are able to mix instructions from
different threads in the same cycle so
what we will get is the green one gets
this the blue one also wants to use the
bottom slot and let's say that there is
a unit there for example the load unit
that you have to use here but we can
still run this in the same cycle next
cycle the green thread does this the
blue one wants to know do this this and
this and we can let's say do these two
but let's say there is a dependence from
this to this so we can only do this one
next cycle the green thread can do this
and from the blue thread we can maybe do
this instruction or maybe not so let's
say we can't because maybe this one
depends also on the one that we haven't
done or the next one now however there
is some period of idleness in the green
thread and now the blue thread gets to
make up for missing out on some things
so it's able to maybe do this and this
dependent instruction and at this point
the green thread is ready to run
something so we can choose whether to
run the green or the blue one let's say
that the green thread has a higher
priority so we always try to do
instructions from the green thread and
only fill the gaps with the blue ones
now we've done this next cycle we still
do whatever we can from the green thread
we now can do this and now we only have
the blue thread left to do if the green
thread has more stuff to do we can do
this so as you can see with SMT we get
to populate unused slots in what
would otherwise be a full-speed
execution of the green thread so that
the blue thread gets to make progress
while the green thread really is not
suffering from it and we can do that for
more than just two threads so for
example if we if we had four threads we
might be able to actually do another
thread at a slightly slower pace than
blue while we are also doing this
so with SMT what we are doing we are
really using underutilized issu slots in
our out of order superscalar processor
the hardware cost to do this is very
small
our fetch stage needs to be slightly
more complicated to allow us to fetch
from one or the other PC and the
register file needs to be slightly more
complicated because really there are no
two sets of architectural registers but
other than that the whole complicated
out of order engine stays the same so
let's say this is going to cost
something like five percent of the cost
of a know multi-threaded processor this
here again after we do register renaming
and everything else the process
scheduler stays just the same we just
need to put instructions from multiple
threads in the instruction window so
that they could be scheduled from there
and now also when we commit we have to
figure out which instruction belongs to
which commit and so on so the cost is
slightly larger than fine grained but
still not very large because again most
of the complicated reservation stations
and execution units and everything else
stays pretty much unmodified so this is
going to have performance that is maybe
very close to a CMP two threads here two
threads here maybe making almost as much
progress but at a very small fraction of
this cost</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>