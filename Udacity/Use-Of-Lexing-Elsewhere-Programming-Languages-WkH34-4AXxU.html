<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Use Of Lexing Elsewhere - Programming Languages | Coder Coacher - Coaching Coders</title><meta content="Use Of Lexing Elsewhere - Programming Languages - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Use Of Lexing Elsewhere - Programming Languages</b></h2><h5 class="post__date">2012-06-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WkH34-4AXxU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it was a very interesting and
wide-reaching question on the forums
about the uses of lexical analysis and
regular expressions and this general
sort of string processing outside of
what we're doing in this class the
essence of the question was what else is
this good for in the real world is it
used to do anything interesting and the
question is actually a resounding yes
there are a large number of great uses
of regular expressions in lexical
analysis beyond just building web
browsers or writing language
interpreters one of the most basic is in
electronic commerce I hinted at this in
the beginning of unit 1 but things like
phone numbers and credit card numbers
are recognized and processed using
regular expressions essentially using
lexical analysis break down a big input
string from the user into something that
you want to deal with for example users
often enter their phone numbers or
credit card numbers with hyphens in the
middle but maybe you want to get rid of
all of the hyphens that you can just
treat it as one big number and ask about
it in your database however quite a few
other things make use of lexical
analysis let me give you one you
probably haven't thought of virus
detection these days there are a number
of adversaries out there in the world of
computing people who are interested in
taking over your machine remotely
perhaps for economic reasons maybe
they'd like to take over your machine
and use it to send spam or do something
else malicious delete your files
something fun like that it turns out
that the virus software you probably
have running on your computer now that
is the antivirus software the good stuff
designed to keep away the evil attackers
is based on regular expressions in
lexing in essence in order to be
relatively interesting a virus typically
has to have some sort of important
payload apart where it replicates and
takes over another program writing out
something to the disk or changing data
structures in memory a virus definition
file which you may have seen your
antivirus software talk about updating
is actually just a big list of tokens
for every virus out there we write a
regular expression corresponding to
the machine code corresponding to this
payload that it uses to take over or
infect other files and a virus scanner
is actually just like a lexer except
that if you match any of those tokens
it's a bad scene a virus scanner runs
all of those finite state machines over
executable programs before you run them
or attachments you downloads in the
internet and if they find any of those
patterns they stop and say oh I think
this file may be malicious let's do
something special with it so virus
scanning or virus checking also uses
regular expressions here's a third use
string matching in general is commonly
used for dealing with computational
biology like DNA sequencing or protein
folding when we get pieces of DNA from
biological laboratories you can view
human DNA or animal DNA in the world as
basically just a long string over a very
simple alphabet of four characters gcat
corresponding to a special objects or
special structures in the world of
biology and biochemistry and everything
about us we believe in the physical
world is based on the interactions are
the unrolling or instructions gleaned
from DNA perhaps through DNA or RNA
transcription I'm not much of a
biologist but I know that these days a
lot of biology research is done with the
aid of or enabled by computers so for
example if I want to check to see if two
particular DNA sources have a relatively
high overlap but when they come out of
the biology lab these strings are a bit
scrambled because they've gone through a
physical process what I essentially want
to do is say Oh could this blob of text
over here match up with this DNA text
from some other person can I find the
best matching or the best alignment and
that task uses exactly the same sort of
lexical analysis string matching that
we've talked about here but I can take
it one step further you might have
wondered at some point how do we come up
with new drugs how do I make a better
aspirin how does that sort of thing go
and while there are definitely in lab
scientific techniques involved you might
imagine oh maybe I take some sort of
test subject I and
them with my candidate medicine and see
if they get better that sort of trial is
very expensive it would be really nice
if we could use mathematical models to
help narrow down the search space to
help get an idea for how drugs might
interact in your body how they might
behave and it turns out that a lot of
chemical interactions at that level are
governed by protein folding how various
proteins will sort of rub against each
other and interact or not we can use
computers to simulate protein folding
and thus get a better handle on
candidate drugs we're designing maybe
you can use the computers to simulate a
bunch of possibilities come up with the
best ten present those to the scientists
and then those are the ones that we try
on live subjects that sort of protein
folding experiment involves a number of
the same sort of techniques that we're
learning about here in lexington's do
big simulations and in fact if you're
curious about this sort of genome
sequence alignments string matching
protein folding sort of world you might
want to check out blast BL ast all
capital letters which is a common
software project for doing just that
it's not necessarily the fastest but it
is one of the most popular and then the
final thing I would mention is there are
a lot of very high level readability
metrics both for software and also for
natural language text that are based on
the sort of lexical analysis or token
definition principles that we've come to
know for example one of the least
expensive ways of figuring out the
appropriate grade level for a book is to
measure very simple things like the
number of words in a sentence and the
number of syllables per word then
through some division and multiplication
you can arrive at whether that should be
given to a ten-year-old child or 14
year-old child or what not to read based
on expected reading norms that sort of
readability metric can be easily
computed using exactly the sort of break
a string up in two words kind of lexical
analysis that we've been focusing on and
that segues into final part of this
question students were asking about
natural language processing with a
lexing and parsing the lexical analysis
and st. tactical analysis that will
cover in this class
do they carry over to parsing real world
languages like Japanese or English or
German and the answer is to some degree
yes linguists or computation linguists
are often interested in modeling real
world natural languages using the same
sorts of formal grammars that we're
going to introduce in unit 3 stay tuned
fun stuff and this question of breaking
a sentence up in two words is
legitimately tricky in languages other
than English I hinted at this in some of
the in some of the lecture material but
in languages like a Japanese or in Latin
where spaces might not be written
explicitly we really have to think hard
about how to break down an utterance
into its substructures however natural
language processing is actually in some
sense significantly harder than the
unnatural language processing that we do
here real world languages like English
are not well behaved compared to
javascript javascript is very artificial
it's very regular we know just how it
will go what the nouns are and what the
verbs are in language like English
that's very hard to figure out so
natural language processing is currently
to some degree still in its infancy
there is still a lot of tests or a lot
of tasks that we would like to be able
to carry out on natural language that
are hard to do if you're interested in
natural language processing writing
computer programs that look at human
written documents and do something with
them a common task to start with is
document summarization for example you
might go to a news aggregator site on
the web and there's a really long
article wouldn't it be nice if you could
say something like summarize this for me
what's the gist of this boil it down to
just a few sentences that's one of the
common tasks that natural language
processing researchers try to solve and
it turns out that that task is very
difficult a sort of a straw algorithm a
cheap way to do it might be to just take
the topic sentence from each paragraph
and put them together counting on the
human who wrote it to have used a
special structure where each paragraph
is introduced by a declarative topic
sentence it turns out that that simple
approach is actually relatively hard to
beat because understanding natural
language matcher language requires your
computer program to have in some sense a
model of semantic meaning or the rest of
the world people will sometimes jokingly
say that natural language processing is
AI complete where complete has a special
mathematical meaning we may get to deal
with jokes and puns like that in a later
class on theory suffice it to say for
now to some degree I'm actually glad
that i work in easy world of unnatural
language processing it's significantly
harder to deal with the utterances we
use day to day when talking to other
people</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>