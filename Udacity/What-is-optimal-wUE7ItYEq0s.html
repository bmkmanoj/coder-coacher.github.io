<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What is optimal | Coder Coacher - Coaching Coders</title><meta content="What is optimal - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What is optimal</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wUE7ItYEq0s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we didn't go into enough detail about
what it is we're trying to optimize here
I just said something vague like we want
to maximize the sum of our reward well
it's not so simple in fact here's a
great story to illustrate that there's a
great russian comedian Yakov Smirnoff
you may remember him or not but he told
this joke once that I really loved he
said have you heard about the Soviet
lottery it's a million rubles if you win
one ruble a year for a million years so
the point is and if you recall from one
of our earlier lessons that one dollar
or one ruble delivered to us a million
years in the future is really not as
valuable as a dollar or ruble that we
get now and so for instance if we think
about our robot living forever I might
do something just mundane together a
dollar a year that's an infinite amount
of money but in practice it doesn't
really work that well so to consider
that and illustrate that I'm going to
show you a little maze problem here and
we'll think about what the robot ought
to do that would be optimal in this maze
so here's our robot and here's the
challenge for our robot we have a reward
here of $1 and a reward over here of
$1,000,000 so if the robot comes over
here and gets us $1 it's a special and
that each time he touches it he gets the
dollar and it goes away but then it
comes back so the robot could come here
and go back and forth and get a dollar
each time it moves here this one once
the robot tags it it's gone but clearly
it's worth a while to come over here and
grab it now this red area is obstacle it
can't go there and here I wrote some
rewards that the robot and in fact
negative one is a penalty but the
penalties the robot would get as it went
this way and zero penalty that way now
if we say that what we want to optimize
is the sum of all future rewards
and it doesn't matter whether we go this
way and just get that dollar over and
over and over again or if we go this way
get the million dollars come back and
then that get that $1 over and over and
over again there's no difference because
they both sum to infinity over time now
what if we say okay I want to optimize
my reward over three moves so I've got a
finite horizon let's consider the
rewards would get with a finite horizon
of three if we go this way versus this
way so if we go this way we're gonna get
rewards of negative one negative one
negative one and if we go this way we
get zero $1 and then we have to move
down here and get another zero so
clearly starting here with a finite
horizon of three the best thing to do is
go up there now if we extend the horizon
a little bit further say out to eight we
would find that this is the best thing
to do so if we go this way we get
negative one negative one negative one
until we hit the jackpot here and get a
million clearly if you sum this up it's
a pretty good prize if we go this way
and touch that $1 over and over again we
get this so clearly as we expand our
finite horizon trivially out to say
eight steps going this way and tagging
that 1 million is the best thing to do
if we carried it even further
we'd discover that then we should come
back this way and go to that dollar and
tag it over and over and over again
let me formalize these a little bit with
the infinite horizon what we're trying
to maximize is the sum of all rewards
over all of the future so it's the sum
of each of these rewards for I equals 1
to infinity the finite horizon is very
similar it's just we don't go to
infinity so for optimizing over a
horizon of 4 steps in would be 4 we're
just trying to maximize the sum of the
reward for the next 4 steps now there is
yet a
another formulation that if you think
back to that lecture a while back about
what's the value of a future dollar we
can dig that up and it makes a lot of
sense in terms of reinforcement learning
so remember that if it takes us say four
years to get a dollar that dollar is
less valuable than say if it takes one
year and in the same way if it takes say
eight steps to make a dollar that dollar
is less valuable than a dollar I can get
just in one step and the way we
represent that is very simple just like
we represented the sum of future
dividends and it looks like this it's
called discounted reward so instead of
just summing up the arse of eyes we
multiply it by this factor gamma to the
I minus 1 such that our immediate reward
the the very next one we get whatever
gamma is when it gets raised to the
zeroth power is just 1 so that means for
a very next step we get our but for the
step after it its gamma to the one so it
D values that reward a little bit gamma
is a value between 0 &amp;amp; 1 the closer it
is to 1 the more we value rewards in the
future the closer it is to 0 the less we
value rewards in the future in fact if
gamma is set equal to 1 this is exactly
the same as the infinite horizon but
gamma relates very strongly to interest
rates if you recall
so if say gamma were 0.95 it means each
step in the future is worth about five
percent less than the immediate reward
if we got it right away this is the
method that we use in q-learning
one reason is that the math turns out to
be very handy and it provides nice
conversion properties</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>