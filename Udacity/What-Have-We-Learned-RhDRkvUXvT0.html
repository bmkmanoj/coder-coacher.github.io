<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What Have We Learned | Coder Coacher - Coaching Coders</title><meta content="What Have We Learned - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What Have We Learned</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RhDRkvUXvT0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so that actually brings us to the end of
the the pound EP topic so you know we're
at the end of the problem view topic we
have high probability of believing that
we're at the end of the palm DP topic
because the observation that we're
making is that what have we learned
slide so what have we learned well we
learned about palm dp's partially
observable markov decision processes
well we have been exposed to these
things before but we really kind of dove
into sort of some detail about what they
are and how to solve them and in
particular we've talked about belief
states mmm it's kind of a representation
which represents the states that we
might be in and how much we believe that
we're in them we talked about Palm DPS
as a strict generalization of MVPs
mm-hmm which is sort of important and
that if we can solve a problem TP that
happens to be an MVP we will solve the
underlying MVP one of the unfortunate
consequences of all that is that dealing
with palm dp's is hard they're they're
hard to solve their just sort of
fundamentally difficult I can't remember
the exact result now but there was
something in there about undecidability
and and that just that was difficult
that was hard for me to deal with hard
for you to accept hard to accept right
despite the fact that it's hard we did
come up some algorithms for solving mdps
like a value iteration I remember
something about piecewise linear and
convex mmm right into representing value
functions as piecewise linear and convex
functions or poke and one of the things
that was nice about that is that you
showed that you could build up these
sets of kind of linear functions and
because you only cared about the max in
a given point you could throw away a
bunch of them that might be unnecessary
and sort of keep the hard problem
possibly manageable good that was really
nice then we went from the solving of
palm vp's to reinforcement learning mmm
with palm DPS and if solving palm dp's
is hard then RL for prom teepees
definitely has to be hard and that turns
out to be true but we tried to do it
anyway let's see we did memoryless
mm-hmm first we talked about a model
based method all right model bismuth
where you try to actually learn the palm
DP from from an experienced wandering
around in it right and what was do you
remember how we how we did that learning
with a great difficulty well sure we
talked about expectation-maximization as
a way of trying to organize that
computation sure that's a favorite of my
expectations mm-hmm you're maximally
favorite yeah I'm actually favorite one
we got to go all the way back and try to
remind ourselves how AM worked yes that
was fun and then we did memory list yeah
and we needed in the memory of this case
we might need to be random so again does
like grad school that's true you're
saying that if you're in grad school but
you don't remember what where you are in
grad school then it helps to act
randomly well it certainly explains the
behaviors on my grad students okay then
we had a we probably did more stuff
there but I can't remember it ah and so
I do remember though that we started
talking about a beige in our yes yes and
and the cool thing about Bayesian RL
being that it blurs the line between
planning and learning so and you know
bayesian stuff does that in general this
sort of notion that well all learning
really is is estimating probabilities of
things it's not actually you know
learning so just keeps changing your
posterior and so decision making in the
Bayesian RL setting ends up becoming
just a kind of version of planning in a
continuous ugly kind of palm BP speaking
of which if we have beige in RL do we
have frequent tutorial well sure I mean
bayesian and frequent tests are kind of
you know rivals in a sense so anything
that's not bayesian maybe it's
frequentist okay so all the other
reinforcement learning that we did like
q-learning and whatnot as it certainly
has a frequentist flavor in that you
know what is what is frequentist
statistics it's about it's about you
know counting up how many times
something happens and divided by the
number of times you asked and that is
you know definitely q-learning has that
flavor model-based RL where you're
trying to estimate the parameters the
model by from experience they tend to
have that flavor okay so good so then we
have been learning about frequentist RL
and now that we have beige in RL it
reveals the frequentist things that
we've been doing in the past so i don't
care that yeah and the bayesian RL is
very nice because like it doesn't make
an exploration exploitation distinction
anymore you're always acting given your
knowledge of what the truth is and
you're acting in a way that
maximizes your reward given that
knowledge right and so exploration is
exploitation planning is learning
exploit exploration is exploitation war
is peace and we have always been at war
with supervised learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>