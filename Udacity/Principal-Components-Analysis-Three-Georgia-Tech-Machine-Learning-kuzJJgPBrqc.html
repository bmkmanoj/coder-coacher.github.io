<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Principal Components Analysis Three - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Principal Components Analysis Three - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Principal Components Analysis Three - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kuzJJgPBrqc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay wait so let me see if I can echo
some of that back so it's almost as if
what we're doing here is we're doing a
transformation into a new space where
feature selection can work exactly and
in fact here's something kind of
interesting for you it turns out that if
the eigenvalue of some particular
dimension is equal to zero then it means
it provides no information whatsoever in
the original space so if I have a
direction if I have a dimension that has
an eigen value of zero I can throw it
away and it will not affect my
reconstruction error I'm trying to
remember if that makes it irrelevant or
useless well it certainly makes it
irrelevant
there's no variance that's the same
thing as saying it has zero entropy
because it never changes so it's
irrelevant now whether it's useful or
not well probably not but it might be
useful for something like our simple
perceptron example that we use last time
gotcha okay
so does this all make sense so one
question I have is in the example that
we just kind of worked through there was
a blob of data and when we drew the red
line through the maximum variance
direction it went through the XY origin
mm-hmm does it have to is it necessarily
the case that it's going to or you don't
do it is the algorithm restricted to
have to put things through the origin
well so my answer to you is that that is
that's actually a very complicated
question and in principle so to speak it
doesn't really matter but in practice
what people do when they're doing
something like PCA is they actually
subtract the mean of the data or the
centroid of the data from all the data
points so it ends up being centered
around an origin or the origin I mean
what this means is that you can then
interpret what we're doing is finding
maximal variance is capturing
correlation okay that's helpful
otherwise if you don't do that what you
end up with is effectively a principal
component that at least intuitively kind
of captures the notion of where the
origin should be and it's not terribly
helpful for what it is we're trying to
do so my answer is no it doesn't and yes
it does okay sure
okay so that's that's basically PCA it's
got all these neat properties let's sort
of summarize them again it's well
actually let me add a couple beyond
these it is a global algorithm it does
give you best
genera which seems like a very nice
things thing to have and it tells you
which of the new features that you get
out are actually important with respect
to this notion of reconstruction by
simply looking at their corresponding
eigenvalues they also have a couple of
other practical properties in particular
it's very well studied and what that
means in this case is that there are
very fast algorithms that do a very good
job even in weird cases even with large
datasets at least if they're
appropriately sparse of being able to
compute these things quickly people have
been working on this problem again since
before we were born Michael and they've
gotten really good at finding principal
components even for what would be very
difficult spaces but this does leaving
too a question though which is another
practical question which is ok so you've
got an algorithm that provably gives you
the best reconstruction but what does it
have to do with classification this is
kind of like the question we had before
about relevance versus usefulness so we
find a bunch of projections which are
relevant in the sense that they allow
you to do reconstruction but it's not
clear that if you threw away some of
these projections the ones with low
eigen value that even though you'd be
able to reconstruct your original data
it's not clear that that would help you
to do classification later can you see
how that would work out sure I mean like
just could be that that's not where the
information is about what the what the
labels ought to be right so imagine for
example that one of your original
dimensions is in fact directly related
to the label and all the rest are just
you know say Gaussian noise but that the
variance of that particular direction is
extremely small it might end up throwing
it away it'll almost certainly end up
throwing it away which means you'll end
up with a bunch of random data that
doesn't actually later help you do
classification and that's because this
looks a lot like if I can do the analogy
a filter method I was going to say that
yeah so this is kind of like filtering
and in fact it's going to turn out that
the other two actors were looking at two
are like filtering although their
particular criterion might be more
relevant we'll see ok all right so do
you understand principal components
analysis again I'm not asking you to
understand exactly how you would run a
principal components analysis algorithm
that stuff's in all the
tax but just to sort of understand
exactly what is it's trying to do what
it's trying to accomplish I think so
it's it's taking the data it's finding a
different set of axes that are just like
regular axes in that they're mutually
orthogonal but it lines up the variance
of the data with those axes so that we
can drop the least significant ones and
that gives us a way to do feature
selection but the whole thing is a
feature transformation algorithm in the
sense that it first moved the data
around to be able to do that that's
exactly right so you do transformation
into a new space where you know how to
do filtering got it
excellent</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>