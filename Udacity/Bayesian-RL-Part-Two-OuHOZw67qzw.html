<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bayesian RL Part Two | Coder Coacher - Coaching Coders</title><meta content="Bayesian RL Part Two - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bayesian RL Part Two</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OuHOZw67qzw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so what I'm basically arguing here is
that we can think of reinforcement
learning as a kind of planning it's not
learning at all it's actually planning
in a kind of continuous space and palm
DP where the hidden state is the set of
parameters of the MDP that we're trying
to learn now there's an infinite number
of those which makes things a little bit
awkward and so we don't really get the
piecewise linear and convex property
anymore but be shown that you can
actually get a result that shows that
the value function in this continuous
based palm DP is actually piecewise
polynomial and convex ok you know it's
not as nice as linear but it's still
representable how big is the degree of
the polynomial it grows with iterations
of value duration I see yeah so it's not
like piecewise cubic and convex is like
piecewise it could be a lot of degree
depending on how many iterations you do
so it this is still pretty awkward to
work with there's there's an algorithm
that actually works fairly nicely called
beetle that tries to approximate this
piecewise polynomial and convex function
and can sometimes actually end up
learning very good approximations of the
optimal way to do reinforcement learning
for those for those spaces of problems
that's cuz beetle stands for something
bayesian exploration exploitation
trade-off in learning which is cheating
I think we agree I think we agree yeah
but otherwise it would be like butyl
which would be just way too hard to say
there's an exploration exploitation
trade-off in learning everything to eat
for everything and then that would be
that would be better and I think I could
forgive that learning excellently nods
to me that's that's just absurd the
point that i'd like to make though there
or the point of this slide is that there
are there are there are bayesian RL
algorithms beetle is one of them it is
not by any means the only one there's
lots of different ways that people have
looked at actually keeping bayesian
posteriors over the mvps that are being
learned and then trying to use that
bayesian posterior to make better
decisions than you would in just you
know reinforcement learning where you
don't have any kind of prior any kind of
structure on what the possible
underlying MDP is so a lot of people
really like this stuff it seems to at
the moment be on the side of just kind
of too expensive to be practically
useful q-learning seems to tend to win
but but it's very elegant and it's it's
a useful way of sort of realizing that
and learning actually are two sides of
the same coin we've used it before in
some work that I've done oh ok but we
were taking advantage of his the fact
that it gives you probability
distributions over value functions or
rather over Q functions how does it do
that you're in a state and it just tells
you basically the probability of an
action being optimal and that turns out
to be very nice because you can then
compare various sources of information
together because they're all if you can
make them all look like probabilities oh
that sounds like that that really cool
policy submerging paper that I saw yeah
yeah that one thank you yes buddy is
building his colleagues whoo</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>