<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What Have We Learned - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="What Have We Learned - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What Have We Learned - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ws5BOy6L_V0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so that's at least in a nutshell the
reinforcement learning story there's um
there's a lot of other topics and I
think we're planning to get to some of
them in a topics lesson a little bit
later but there's you know courses and
books worth of stuff to study in this
area just like supervised learning as a
whole
but we're gonna just kind of end it
there with the idea that we now have a
handle on how we can use learning to do
decisions with delayed rewards so can
you help us summarize what we what we
learned in this lesson
okay sure I think what did I learn here
today I think I learned a lot of things
some of which had to do with
reinforcement learning mainly that you
can actually learn how to solve an MDP I
think that's actually a pretty big deal
right meaning that we don't know T&amp;amp;R
mm-hmm
but we just have access to the ability
to interact with the environment and
receive transitions and that's actually
that's actually pretty impressive and a
very powerful thing because we're often
though if we assume the world is an MDP
but we don't know TNR and we don't have
some way of learning in that we really
don't have much we can do and you've
shown me that there's something that we
can do cool well then it's the biggest
thing we learned some specific things in
particular we learned about cue learning
several kinds of cue learning but one is
actually a real word yes Andy they're
all real Michael we learned about cue
learning and I think the other most
important thing that we learned about is
the exploration versus exploitation
trade-off and with Q learning we learned
a little bit about when it converges
mm-hmm and that it's actually a family
of algorithms and different members of
that family have different behaviors
associated with them oh there's one
other thing I wanted to say on that
topic okay which is that one way to
achieve this exploration exploitation
balance was to randomly choose actions
so to change the way we're doing action
selection but there's another one too
which is that we can actually by
manipulating the initialization of the
cue function we can actually get another
kind of exploration can you see how that
might work oh I know you can you set say
the Q values to all be the highest
possible value they could be right so if
we initialize the Q hat - awesome values
then what the Q learning algorithm will
do even with greedy exploration what it
will do is it'll try things that it
hasn't tried very much and it still
thinks are awesome and little by little
it gets a more realistic sense of how
the environment works and so that's very
optimistic that's right exactly and it's
referred to often as optimism in the
face of uncertainty and it's a similar
kind of idea that's used in algorithms
like a star if you're familiar with the
search algorithms in AI remember those
but this is this is a really powerful
idea and it's used in reinforcement
learning and banded algorithms and
planning and and search okay and that
makes sense because if everything's
awesome then your true key value can
only go down if awesome is bigger than
the biggest key value could ever have
and so that means you're gonna look at
every single action and as you learn
more about them you will just get more
depressed about them and that's good yes
the world slowly beat you down so is
that is that how we really talked about
I guess I guess that's about right we
talked about what a queue function was
right and and how that kind of binds
everything together we talked about
different approaches including policy
search and model-based reinforcement
learning yeah that was very nice we tied
it all back in the planning so one thing
we didn't talk about is connecting to
function approximation and the issues in
machine learning that are really
important things like overfitting they
come up in the reinforcement learning
setting but not in this simplified
setting that we're looking at here where
we learn a separate value for each state
action pair we're gonna have to start
generalizing you see the importance of
that and that's we're gonna do in a
later lesson okay I like it and we also
learned a bunch of things about letters
like exploration versus exploitation in
fact we know enough that we can now get
an A in letters I like it okay well I
think we learned a lot okay well good
well thanks it's very nice to get a
chance to talk to you about this stuff
cool and so uh I guess we're gonna talk
about next well um whatever it says on
the syllabus I think it's game theory
that's pretty cool oh when I see why
we're gonna do that because right now
all we've been doing is talking about
the world as if there were just one
agent and now
body else and now we're going to see
what happens when there are other people
right other people show up at the party
next time on machine learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>