<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Still More Support Vector Machines - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Still More Support Vector Machines - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Still More Support Vector Machines - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ND288J1BsSk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so we've done a little bit of
moving and moving stuff around and kept
the same equation up before remember our
goal is to use quadratic programming to
maximize this equation so let me talk a
little bit about the properties of the
solution for this equation so here's the
first one it turns out that once you
find the alphas that maximize this
equation you can actually recover the W
which was the whole point of this
exercise in the first place
that's the little W not the Big W that's
a little W no not the Big W that's right
okay II yeah then it's kind of neat so
it's really easy to do and of course
once you know W it's easy to recover B
you just find a value of x you stick it
in the W you that you know is equal to
plus one and then poof you you can find
out B so you can recover W directly from
this and you can recover be from it and
sort of the obvious way but here's some
other properties that are a little bit a
little bit more interesting for you so I
want you to pay attention to two things
one I'm just going to have to tell you
and the other I want you to think about
so here's the one that I'm going to tell
you it turns out okay that alpha each of
those alphas are mostly zero usually so
if I told you that in the solution to
this most of the alphas that you come
back with are going to be zero what does
that tell you about W so W is the sum of
the data points times their labels times
alpha and if the alpha is zero that the
corresponding data point isn't really
going to come into play in the
definition of W at all so so a bunch of
the data just don't really factor in to
W that's exactly right so basically some
of the vectors matter for finding the
solution to this and some do not
so it turns out each of those data
points are vectors but you can find all
of the support that you need for finding
the optimal W in just using a few of
those vectors than nonzero alphas
yeah well the ones with nonzero alphas
so you've basically built a machine that
only needs a few support vectors
so the the data points for which the
corresponding alpha is nonzero those the
support vectors yes those are the ones
that provide all the support for W so
knowing that W is the sum over a lot of
these different data points and their
labels and the corresponding alphas and
that most of those are zeros that
implies that only a few of the X's
matter now Michael let me let me do a
quick quiz yay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>