<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Implementing LRU - Georgia Tech HPCA Part 3 | Coder Coacher - Coaching Coders</title><meta content="Implementing LRU - Georgia Tech HPCA Part 3 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Implementing LRU - Georgia Tech HPCA Part 3</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bq6N7Ym81iI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so how do we implement the least
recently used policy that works really
well and it works really well because it
exploits locality the most recently used
thing is likely to be used very soon
afterwards the thing that we haven't
used for a while
means that there is less reason for
locality to kind of cause it to be
accessed again let's say we have a 4-way
set associative cache and let's just
track what's happening in this one set
so there is four blocks in it for each
of these blocks we keep the tag and the
valid bit and in order to track which
block was accessed when we are going to
also have an LRU counter for each block
these counters take values that
correspond to the size of the set in
this case zero through three because
there are four things in the set so
let's initialize them to 0 1 2 &amp;amp; 3 they
have to all be different all the time
and we will see how that is maintained
so we start like this when we want to
replace something we will replace the
block whose counted is currently 0
because that's going to be the least
recently used block so let's say that we
access some block a and we need to put
it somewhere we will choose the place
with a zero counter and put it there now
that the block is placed there the
processor accesses it and suddenly this
becomes the most recently used block so
it's counter now needs to be 3 in order
to make sure that all of the counter
still have different values that means
that all of the other counters no need
to be decremented so the one that was
one becomes 0 2 becomes 1 and 3 becomes
2 and now let's say that we access some
Block B we put it here because the
counter is 0 now here when B is accessed
after putting it here that counter
becomes 3 this one becomes 2 this one
becomes 0 and this one becomes 1 we will
now put some Block C here that becomes 3
this becomes 2 this becomes 1 this
becomes 0 D 3 2 1 0 now we're back to
this situation so I'm going to clean up
here and now
if we access e we're going to find what
was the least recently used block and a
at this point is because it's been
accessed the least recently so a will be
replaced by E and the LRU counter here
becomes three which brings these one
notch lower so this is the state we have
now let's say that B which is the least
recently used block now is react cest
what happens then is this counter needs
to become three all of the other ones no
need to be changed lower what happens if
the most recently block let's say B is
accessed again then this counter just
stays at three because that indicates
that this is already the most recently
accessed block what happens if block D
which is neither the most nor the least
recent is accessed well this counter
needs to become three and now we have an
interesting situation we don't just
lower all the other counties a notch
down we take the ones that were above
the original value of this counter so we
take the counter that is above one and
those are the ones that get a notch down
but the ones that are below one stay the
same that ensures that the counter still
have different values let's do another
one of those where let's say Block B
that is almost the most recently used
block is accessed in that case this
counter needs to become three all
counters that were above its original
value of two need to be decremented so
this becomes two but these two stay the
same because they are below two so again
we have this situation so now you know
how to maintain the LRU counters
properly after every step you can verify
that the counters have different values
if they don't then you probably made a
mistake somewhere so as you can see
maintaining LRU is relatively
complicated for an end way set
associative cache we need n counters
whose size is log to a fan in this case
the size of the counters is two bit
because
they need to have a number between zero
and three to tell us where are we in the
order of axes in this set so it's a
4-way set associative cache that means
we need two bits per counter and four
counters so for highly associative cache
is something like 32 way associative you
would need to have five bit counters and
32 of them per set so this was the cost
it's pretty high worse there is an
energy problem we need to bada Phi up to
n counters on each access even cache
hits that can happen very frequently so
the LRU approximations try to keep fewer
counters and do fewer updates on cache
hits</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>