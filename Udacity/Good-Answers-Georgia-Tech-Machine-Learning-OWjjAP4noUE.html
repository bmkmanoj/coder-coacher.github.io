<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Good Answers - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Good Answers - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Good Answers - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OWjjAP4noUE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Michael so we've done our little
example I want to ask you a quick
question and try to talk something
through with you and then we can start
to wrap up okay awesome all right so
here's my quick question now in the
reading which I know you've read there's
a proof that shows that boosting not
only you know does pretty things with
axis aligned semi planes but also that
it will converge to good answers and
that it will find good combined
hypotheses you know we could go look at
the reading and write down a proof that
shows that boosting does well and
there's one in the reading or we could
talk about an intuition so if someone
were to come up to you if a student were
to find you somewhere and said I read
the proof I kind of getting it but do
you have a good sort of intuition about
why boosting tends to do well what do
you think you would tell them can you
can you think of something simple I've
been struggling with this for a while no
okay well then let me try something on
you and you can tell me if it sort of
makes sense so this is just an intuition
for why boosting tends to do well okay
so what does boosting do okay
boosting basically says if I have some
examples that I haven't been able to
classify well I'm going to rewrite all
my examples so that the ones I don't do
well become increasingly important
right that's what boosting does yes yes
right that's what this whole whole bit
of D is all about it's all about
re-weighting based on difficulty and
hardness and we know that we have the
notion of a weak learner that no matter
what happens for whatever distribution
we're always going to be able to find
some hypothesis that does well so if I'm
trying to understand why boosting in the
end why the final hypothesis that I get
at the end is going to do well I can try
to get a feeling for that by asking well
under what circumstances would it not do
well so if it doesn't do well then that
means there has to be a bunch of
examples that it's getting wrong right
that's what it would mean not to do well
agreed yeah okay so
many things could it not get right how
many things could it misclassify how
many things could it get incorrect well
I'm gonna argue Michael that that number
has to be small there cannot be a lot of
examples that it gets wrong so you want
to know why you want to my reasoning for
why yeah so here's my reasoning let's
imagine I had a number of examples at
the end of this whole process I've got a
tee times I've gone through this many
times and I have some number of examples
that I'm getting wrong if I were getting
those examples wrong then I was getting
them wrong in the last time step right
and since I have a distribution and I
renormalized and it has to be the case
that at least half of the time more than
half of the time I am correct that
number of things I'm getting wrong has
to be getting smaller over time because
let's imagine that I was at a stage
where I had a whole bunch of them wrong
well then I would naturally renormalize
them with the distribution so that all
of those things are important but if
they were all important the ones that I
was getting wrong then next time I run a
learner I'm gonna have to get at least
half of them right more than half of
them right does that make sense it does
but it but what scares me is okay but
why can't it just be the case that the
previous ones that were getting right
now start to get more wrong as we shift
our energy towards than the errors yeah
what is it
I don't know but did you wanted are we
working up to some kind of a you know
login kind of thing where each time
you're knocking out half of them and
therefore I don't know do you remember
the proof what goes on is that you get
sort of this exponentially aggressive
waiting over examples right yeah and
you're driving down the number of things
you get wrong sort of exponentially
quickly over time that's why boosting
works so well and work so fast I get
that we're that we're quickly ramping up
the weights on the hard ones I don't get
why that's causing us to get fewer
things wrong over time so like when you
shoot in your in your example that you
work through that had the error and the
alphas and the errors kept going down
and the alphas kept going up right like
is that necessarily the case well what
would be the circumstances under which
it isn't the case how would you ever go
back and forth between examples well
certainly it's the case that if you
getting something right you will get
them well so here's what happens over
time right is it overtime every new
hypothesis it gets to get a vote based
upon how well it does on the last
difficult that's a distribution so the
ones that are even if the ones that you
were getting right you start to get
wrong they are going to if you get them
increasingly wrong that error is going
to go down and you're going to get less
of a vote right because each of T is
over the current distribution it's not
over the sum of the vote it over all the
for all the examples you've ever seen I
understand so does that make sense is
that right I don't know I don't have the
intuition it seems like it could be you
know because we keep shifting the
distribution it could be that the error
is going up like if the error could be
low why didn't we just make it low from
the beginning right like I feel like the
error should be going up because we're
asking it harder and harder questions as
we go no no because we're asking harder
and harder questions but even though
we're asking harder and harder questions
it's forced to be able to do well on
those hard questions it's forced to
because it's a weak learner I mean
that's why I having being able to always
do that's why having a weak learner is
such a powerful thing but why couldn't
we like on on iteration 17 have
something where the weak learner works
right at the edge of its abilities and
it just comes back with something that's
1/2 minus Epsilon that's fine but it has
to always be able to do that if it's 1/2
minus epsilon the things that's getting
wrong will have to go back down again no
no I understand that what I'm saying is
that why would the error go down each
iteration well it doesn't have to but it
shouldn't be getting bigger why should
imagine imagine the case that you're
getting right you you are working at the
edge of your abilities you get half of
them right roughly and half of them
wrong the ones you got wrong will become
more important so the next time around
you're going to get those right versus
the other ones so you could cycle back
and forth I suppose in the worst case
but then you're just going to be sitting
around always having a little bit more
information so your error will not get
worse you'll just have different ones
that are able to vote on that do well on
different parts of the space right
because you're always forced to do
better than
chance yeah but that's no that's not the
same as saying that we're forced to get
better and better each iteration that's
right it's not so it's again I don't see
that that property just falling out well
I don't see falling on either but then I
haven't read the proof in like 7 8 9
years I mean I feel like it should be it
should be something like look we had
look at what the so okay so we generate
a new distribution what was the previous
what's the previous classifiers error on
this distribution it better be the case
I mean if it were the case that we
always returned the best classifier that
I could imagine trying to use that but
well we we don't we don't require that
yeah I mean it's just finding one that's
that's epsilon minus 1/2 minus epsilon
right so look let's let's see if we can
take this implication we got three
examples right and you're bouncing back
and forth and you want to construct
something so that you always do well on
two of them and poorly on one kind of a
thing and that you keep bouncing back
and forth so let's imagine that you have
one-third one-third one-third and your
first thing gets the first two right and
the last one wrong so you have an error
of a third and you make that last one
more likely and the other two less
likely suitably normalized right yep so
now your next one you want to somehow
bounce back and have it decide that it
can miss so let's say it missed the
third one so you you get the third one
right you get the second one right but
you get the first one wrong okay what's
gonna happen well three is gonna go down
you're still gonna well you won't have a
third error actually you'll have less
than a third error because you had to
get one of the ones that you were
getting right wrong you had to get the
one you were getting wrong right so your
error is going to be at least an example
I just gave less than a third so if your
error is less than a third then the
weighting goes up more and so the one
that you just got wrong goes doesn't go
back to where it was before it becomes
even more important than it was when you
had a uniform distribution so the next
time around you have to get that one
right but it's not enough to break a
half so you're gonna have to get
something else right as well and the one
in the middle that you were getting
right isn't enough so you'll have to get
number three right as well interesting
right and so it's really
hard to cycle back and forth between
different examples because you're
exponentially waiting how important they
are which means you're gonna always have
to pick up something along the way
because the ones that you coincidentally
got right two times in a row becomes so
unimportant it doesn't help you to get
those right whereas the ones that you've
gotten wrong in the past you've got to
on these cycles pick up some of them in
order to get you over 1/2 and so it is
very difficult for you to cycle back and
forth interesting and that kind of makes
sense right if you think about it in
kind of an information gain sense
because what's going on there is you're
you're basically saying you must pick up
information all the time and then you're
non-uniform is the wrong word but you're
kind of you know nonlinearly using that
information in some way so that kind of
works and makes some sense to me but I
think in the end what kind of has to
happen is you you there must be just a
few examples in a kind of weighted sense
that you're getting wrong and so if I'm
right that as you as you move through
each of these cycles you're waiting in
such a way that you have to be picking
up things you've gotten wrong in the
past so in other words it's not enough
to say only the things that are hard in
the last set are the ones that I have to
do better you must also be picking up
some of the things that you had gotten
wrong earlier more than you were getting
them right because there's just not
enough information in the ones that
you're getting laid all the time because
by the time you get that far along the
weight on them is near zero and they
don't matter
interesting and then if you say well
Charles I could cycle back by always
getting those wrong yes but then as you
get those wrong they're gonna pull up
and you're gonna have to start getting
those right too and so over time you've
got to not just pick up things that do
better than half but things that do well
on a lot of the data but because there's
no way for all of the possible
distributions for you to do better than
chance otherwise it's cool</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>