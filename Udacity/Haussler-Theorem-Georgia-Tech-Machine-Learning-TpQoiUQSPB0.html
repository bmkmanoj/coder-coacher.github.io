<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Haussler Theorem - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Haussler Theorem - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Haussler Theorem - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TpQoiUQSPB0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so now we're ready to dive in
and actually work out some math that
turned out not to be so bad but it was a
it ends up being okay specifically
because we were very careful about
setting up all the definitions to lead
us to this moment in time it is our
destiny Charles oh good I love destiny
this is Destiny's Theorem no actually
turns out it's houseless near his house
were like German for destiny it might be
but I don't think it is so how sir is
the name of a person in this particular
case and what he worked out is a way of
bounding the true error as a function of
the number of training examples that are
drawn so let's consider from the
hypothesis set that we have all the
hypotheses can be categorized as to
whether or not they have high true error
or low true error sure all right so
let's let each one through H K be the
ones that have a priori they have high
true error what we'd like to do is make
sure that as we're drawing data sets
that we have knocked all these guys out
we've gotten enough examples that
actually allow us to verify that they
have high error so they have high error
on the on the training set so they have
height rainier alright so how many - how
much data do we need to establish that
these guys are actually bad all right so
let's take a look at the probability
that if we draw an X an input from this
distribution D that for any of these
hypotheses H I in this set of bad
hypotheses that it will match the true
concept right so that H sub I of X is
equal to C of X and we know that that's
less than or equal to 1 minus Epsilon
it's unlikely that they match because
it's likely relatively likely that they
mismatch right that's what this exactly
means is this error being greater than
Epsilon oh I see so if I have an error
of greater than Epsilon
that means that the probability that I'm
wrong is greater than Epsilon which
means the probability that I'm right is
1 minus Epsilon
less than 1 minus Epsilon ok the metrans
yeah so it's sort of a relatively low
probability of match well if epsilon is
high low relative to 1 minus Epsilon
right okay so this is this is a fact
about the hypothesis in in the abstract
for any given sample set we've got
M examples and what we'd like to know is
since we're trying to knock it out
what's the probability that even after
we've drawn M examples that this
hypothesis H sub I it remains consistent
with C right even though it data it
doesn't really match all that well we've
drawn M examples and it still looks like
it matches it's still in the version
space so the probability that that
happens if it were the case that
everything was independent is going to
be 1 minus epsilon raised to the M power
right because it's less than 1 minus
epsilon to be wrong once well which is
to say that you're consistent wants to
continue to be consistent we keep having
to have this probability come true so
it's going to be we're gonna just keep
multiplying it in again and again and
again so 1 minus epsilon raised to the M
power right so that makes sense because
you're basically saying it's consistent
with the first example and the second
example and the third example and
dot-dot-dot the IMP example so and with
independent variables it's just
multiplication so it's 1 minus epsilon
times 1 minus epsilon times 1 minus
epsilon M times which is just 1 minus
epsilon to the M ok I see that great all
right so can we use that to figure out
what's the probability that at least one
of these h1 through a cheese is
consistent with C on M examples we have
to knock them all out right that's
that's really what the goal is to knock
knock out all the ones that have high
true error so we failed at that if one
of them still slips through one of those
still looks consistent and remains in
the version space so what's the
probability that at least one of these
is remains consistent that this still
has happened yeah I think I know that so
just like you did and before and did
multiplication another way of writing at
least one of is to say or so h1 or h2 or
h3 or h4 dot dot or HK is consistent and
just like and is multiplication or is
addition that's true and there are K
different ones of these so I have to say
this 1 minus epsilon to the M plus 1
minus epsilon to the M plus 1 minus
epsilon to the M K times so that would
be 1 minus epsilon to the M
okay great so that is a bound on the
probability that at least one of these
bad hypotheses is going to remain in the
version space even after M examples but
how many bad examples how many bad
hypotheses are there what's an upper
bound on that well there might be one or
there might be two there's actually K of
them but we know that K itself is bound
by the total number of hypotheses yeah
that's right so it has to you know that
the number of bad hypotheses we we
assumed if C is equal to H any way that
there's at least one that is not bad but
you know almost H of them can be bad so
that that should give us a bound all
right so now we're going to to work on
this expression a little bit more to put
it in a more convenient form okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>