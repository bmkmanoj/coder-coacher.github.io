<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Reinforcement Learning Outro Part 4 - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Reinforcement Learning Outro Part 4 - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Reinforcement Learning Outro Part 4 - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/G6ZXV9b9gUs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright so that's supervised learning
I'm going a whole bunch of stuff that we
covered what about unsupervised sorry is
there anything there in that section
that you wish we had been able to talk
about they do it certainly seems like
there ought to be in the sense that it
was a lot shorter than the other
sections a lot shorter yeah so yeah so
one thing that I thought would be worth
bringing up in this context is tf-idf so
tf-idf is something that I think is in
your one version of this class that you
teach yeah but we didn't get a chance to
talk about it in here and so what does
it have to do with unsupervised learning
or randomized optimization I don't know
Michael what does it have to be right so
tf-idf is it stands for term frequency
inverse document frequency and it is a
way of applying weight when you're doing
a kind of a nearest-neighbor operation
in textual data so if you have a big
collection of text like web pages and
queries and you're making queries
against the web pages you might want to
know how close is this query to all the
various documents that are in your
collection all the various web pages you
might want to return and it turns out
that there's lots of different
similarity measures that you can use you
can use euclidean distance you could use
some kind of dot product key thing
usually is like cosine of the angle
between dr. yeah there we go all right
as a way of measuring how similar this
query is to a document so we can return
the most similar ones but it turns out
that there's better and worse ways of
doing this waiting and so it is turn it
has it has become apparent through the
years i think back into the 60s actually
Oh Jerry Salton that a really good way
of doing this that's that's quite simple
but incredibly powerful is to say that
the amount of weight that you put on the
appearance of a term in a document
should be proportional or positively
related to the term frequency the number
of times that term appears searching for
things about snow and we have a web page
that just mention snow and passing
that's not as important as if we have a
document that you've mentioned snow all
over the place that the importance of
that word grows with the number of times
it appears that makes sense but then
that would imply that probably the most
important work is be yes well first of
all no it's not really that important at
least as far as determining whether some
document is relevant so exactly so so
that's the tee eff
is the term frequency the IDF part
inverse document frequency says well how
many documents in your entire collection
have that word in them if it appears
indiscriminately across a large number
of documents then we want to down weed
it but be like thee so the appears
almost everywhere and so the document
frequency the number of documents at
Pearson is huge the inverse document
frequency is therefore very small so
though gets very little weight when
you're doing this kind of comparison
that makes the whole thesis about this
oh that's so maybe you've heard of this
before I have heard this before and it's
actually one of these things that are
just completely accepted and it makes
sense in the unsupervised learning case
mmm because you're effectively in the ad
hoc retrieval task as they call it mmm
we would probably call it the Google
task if it had been admitted no it was
funny i worked in information retrieval
before Google and what was funny is a
seller I do no no no no no yes no so so
they know it was it was hard sometimes
writing papers to make the case that it
was important to be able to do
information retrieval like what are we
retrieving on there's only ten documents
in the world like it was it was a pretty
funny thing and now it's like so obvious
to everyone that this is important in
fact they don't even think about it it
was it changed the world it did okay so
that was something that we could have
talked about but mainly me you didn't
have time of you retire let's see we've
talked about semi-supervised learning
already yep and that is something that
could have makes it in this section and
I guess but that's all sort of the
unsupervised learning space spectral
clustering grace different kinds of
classroom could have talked about but
like spectral clustering yeah it's kind
of a neat thing and pretty pretty good
right it does pretty well in the world
all right yeah I think I think so okay
so you should look that up what's neat
about is it is it actually doesn't get
stuck in local Optima so it's a whole
series of methods that are based more on
linear algebra where you can kind of
invert matrices and get one answer than
it is on you know search and a.m. and
gradient descent things like that
there's a claim you could make that a
lot of machine learning particularly
unsupervised learning space is
effectively linear out at bottom well in
spectral clustering they just they fess
up to that right okay anything else we
were didn't say anything new there
anything in randomized optimization that
we didn't
i think it's worth at least mentioning
cross entropy which is a method that's
really simple to implement it's
incredibly effective we keep the
students that I work with have
implemented in a number of settings and
it just always does it does well it does
better than you know our friends
simulated annealing and hill climbing
and things like that it does it does
really well well so here's the thing i
think it's an awful lot like mimic so i
don't know what it's a mimic mimic yeah
very nice that's that's good but it
seems to me that the two communities
didn't really weren't really aware of
each other so maybe we're maybe we need
to put these two things to you can be
mimicked and I can be cross entropy and
we can figure out how they relate to
each other we're gonna cross fertilize
each other I don't like the sound of
that at all okay so on that note we'll
leave unsupervised on your randomized
optimism yeah see this is what happens
when we're not supervised well duh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>