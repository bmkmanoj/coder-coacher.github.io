<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Wrapping Up - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Wrapping Up - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Wrapping Up - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/B4o2tTG4V4I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I was thinking of talking to you more
about sampling but it seems like it
might work out best to just have some
hands-on experience with it so we're
gonna put those things on the homework
so given that we're actually in a
position now to kind of wrap up the
whole Bayes net inference piece that we
were talking about so you want to help
remind me
Charles what were the things that we
covered sure I can help you with that
we covered Bayesian inference I'm sorry
I'm punch-drunk I'm gonna choose not to
pay attention to that instead write
Bayesian networks we talked about the
Bayesian network representation of joint
probability distributions right and we
did a lot of examples of how to actually
do inference with networks exactly how
do we do we compute probabilities of
particular values we mentioned sampling
that's right and then we did a naive
Bayes well first we did say that that in
general it's hard to do exact inference
it's actually hard to do even
approximate inference mm-hmm but we
talked about a special case of Bayesian
networks that was called naive Bayes
with the naive part being that we're
assuming that attributes are independent
of one another condition on the label
right and this was actually helping us
make a link between all this Bayesian
stuff that Bayesian rabbit-hole that we
went down and classification which is
the core machine learning topic that
we've been spending a lot of time on so
the other thing that I really liked
about this with this notion this link to
classification Michael is that when I
was talking about Bayesian learning
right well we ended up with at the end
is this nice idea that we had a gold
standard right we had a sort of way of
talking about what the right hypothesis
was and ultimately what the right
classification was by computing these
probabilities and sometimes we couldn't
do it because typically you can't
actually do the for loop that requires
you compute conditional probabilities of
hypotheses given data over say an
infinite number of hypotheses but at
least we kind of knew what the right
thing was and we made right assumptions
we could do things like derive oh I
don't know a sum of squared errors or or
various other things that you might do
and that was all very cool but what
you've done here when you do inference
is at least with the naive Bayes case
you've shown as a way that we can do
classification use
these things that actually is tractable
and is the right thing to do under
certain assumption so I really like that
and the other thing that I think that's
worth mentioning is that not only does
it link this Bayesian learning to
classification but it connects
classification back to this general
notion of Bayesian learning and Bayesian
inference where you don't have to worry
about just figuring out the most likely
label given a bunch of attributes but
because it's a Bayes network and you can
compute anything from it
you could try to ask well what's the
likelihood that I see some particular
attribute or set of attributes given a
label or given a subset of attributes
and on all those kinds of things that
you could do with Bayesian learning and
so inference gives us this power did not
just do classification but to do a
larger set of things beyond
classification I think that's kind of
cool cool yeah well said the another
thing is kind of in that same space is
that it handles missing attributes
really well so whereas things like oh
you know decision trees and so forth if
you give me an example that doesn't have
one of the attribute values and you hid
that part of the decision tree where you
need to know that attribute value you're
stuck whereas in this naive Bayes
setting you can still do the
probabilistic inference over the missing
attributes because all the things are
linked by probabilities nice all right
so I think you know you'll get a much
stronger handle of this when you go
through the the homework problems but I
think that's enough for Bayesian
inference and I think that actually
wraps up classification and regression
more generally right so we're done with
the supervised learning well one's never
done with supervised learning but we're
at least done with this part of the
course because there's always more to
supervise learn that's right and in
particular you'll get a nice example of
this because you'll be taking an exam
and your input will be the exam and then
we'll give you a label back I guess
that's one way to think about it well
and then they'll get to generalize
beyond that for the next time they take
the exam very good all right well well
thanks very much this has been fun
thanks Charles this has been fun I will
see you on the second mini course
alright bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>