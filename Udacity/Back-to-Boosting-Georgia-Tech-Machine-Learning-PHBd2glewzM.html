<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Back to Boosting - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Back to Boosting - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Back to Boosting - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PHBd2glewzM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so back to boosting Michael so
as you recall the little teaser I left
you with last time is that it appears
that boosting does not always over fit
and the little graph that's true but it
doesn't seem to over fit in the ways
that we would normally expect it to over
fit and in particular we see a you know
an error line on training and what we
would expect to see is a testing line
that would you know you pretty closely
and then start to get bad but what
actually happens is that instead this
little bit at the end where you get
overfitting seems to instead just keep
doing well in fact getting better and
better and better and I promised you an
explanation for why that was so given
what we talked about with support vector
machines and what we spent most of our
time thinking about what do you think
the answer is well I I don't think I
would have asked again if I had a
thought about it but you mean you want
me to connect it to support vector
machine somehow well the thing that was
fighting overfitting in support vector
machines was trying to focus on maximum
margin classifiers here let me try to
explain to you why it is that you don't
have this problem with with overfitting
at least not in the typical ways you
keep applying it over and over again
like you do with something like neural
networks and it really boils down to
noticing that we've been ignoring some
information so what we normally keep
track of is error so error on say a
training set is just you know the the
probability that you're going to come up
with an incorrect answer or come up with
an answer that disagrees with your
training set and that's a very natural
thing to think about it makes a lot of
sense but there's also something else
that is actually captured inside of
boosting and captured by a lot of
learning algorithms we haven't been
taking advantage of and that's the
notion of confidence so confidence is
not just whether you got it right or
wrong it's how strongly you believe in a
particular answer that you've given it
makes sense yeah I guess a lot of the
algorithms we talked about indirectly
have something like that so like in a
nearest neighbor method if you're doing
five nearest neighbor and all five of
the neighbors agree that seems different
than the case where three vote one way
into vote the other right in fact that's
a really good example if you think of
that in terms of regression then you
could say something like the
variance between them is a sort of a
stand-in for confidence low variance
means everyone agrees high variance
means there's some major disagreement
okay so what does that mean in the
boosting case well as you recall the
final output of the boosted classifier
is given by a very simple formula and
here's the equation here that H of X is
equal to the sine of the sum over all of
the weak hypotheses that you've gotten
of alpha times H so the weighted average
of all of the the hypotheses right and
you just simply if it's positive you you
produce a plus one if it's negative you
produce a minus and if it's exactly zero
you don't know what to do and so you
just produce a zero just throw up your
hands so I'm not I'm gonna make a tiny
change to this formula Michael just just
for the purpose of sort of explanation
that doesn't change the the fundamental
answer and I'm just gonna take exactly
this equation as it is and I'm gonna
divide it by the weights that we used
now what does that end up doing okay so
the weights I'm getting there's alphas
in the SVM two so I'm getting a little
confused so that I think these alphas
all have to be non-negative right are
they kind of like this support vector
values in that they could be zero if
that hypothesis doesn't come into play
well but they won't in this case because
the hypothenuse umping ooo and also
these alphas are applied to hypotheses
whereas the alphas in the in the SVM
settings were being applied to data
points that's right so unfortunately in
machine learning people invent things
separately and reuse notation and alpha
is an easy Greek character draw so
people use it all the time but here
remember alpha is a measure of how good
a particular weak hypothesis was and
since it has to do better than chance it
works out that it'll always be greater
than zero gotcha okay so this this
normalization factor this denominator
doesn't it's just a constant with
respect to X the input so it won't
actually change the answer so it really
is the same answer as we had before just
differently writing it right and what it
ends up doing like often is the case in
these situation
is it normalizes the output so it turns
out that this value inside here is
always going to be between minus 1 and
plus 1 okay but otherwise doesn't change
anything about what we've been doing for
boosting so you might ask why did I go
through the trouble of normalizing it
between minus 1 and plus 1 by indeed
well it makes it easier for me to draw
what I want to draw next so we know that
the output of this little bit inside the
sine function is always going to be
between minus 1 and plus 1 and let's
imagine that I take some particular data
point X and I pass it through this
function I'm getting it with some value
between minus 1 and plus 1 and let's
just say for the sake of argument it
ends up here ok is that an extra plus
that's a plus ok so it's a positive
example and it's near plus 1 right so
this would be something that the
algorithm is getting correct yes it's
not just getting it correct but it is
very confident in its correctness
because it gave it a very high value by
contrast there could have been another
positive that ends up around here so it
gets it correct but it doesn't have a
lot of confidence so to speak and it's
correct answer because it's very near to
zero so that's the difference between
error and confidence because for example
I could also have a plus value way over
here so I am very very confident in my
very very incorrect answer so this is my
daughter for example very confident way
she's right or wrong okay and so now
imagine there's lots of little points
like this and if you're doing well you
would expect that you know very very
often you're going to be correct and so
you ended up shoving all the positives
over here to the right and all the
negatives over here to the left and it
would be really nice if you were sort of
confident all of them ok so this makes
sense Michael as a picture and what
might be going on absolutely ok good so
now I want you to imagine that we've
been going through these training
examples and we've gotten very very good
training here in fact let's imagine that
we have negative training here I'm just
in fact let's imagine that we have no
training error at all so we we label
everything correctly so then the picture
would look just a little bit different
we'd have all the pluses
one side and all the minuses on the
other but we keep on training we keep
adding more and more weak learners into
the mix so here's what ends up happening
in practice right so what ends up
happening in practice is you have to do
some kind of distribution on the hard
examples and the hard examples are going
to be the one that are very near the
boundary so as you add more and more of
these weak learners what seems to happen
in practice is that these pluses that
are near the boundary and these minuses
that are near the boundary just start
moving farther and farther away from the
boundary so this - starts drifting and
drifting and drifting until it's all the
way over here this - starts drifting and
drifting and drifting until it's all the
way over here and the same happens for
the pluses and as you keep going and you
keep going what ends up happening is
that your error stays the same it
doesn't change at all however your
confidence keeps going up and up and up
and up and up which has the effect if
you look at this little drawing over
here of moving the pluses all over here
so they're all in a bunch and the
minuses on the other side so what does
that look like to you Michael what this
picture yeah I mean that there's a
there's a big gap between the leftmost
plus and the rightmost minus which you
know in the context of this lecture
reminds me of the margin that's exactly
right that basically what's ended up
happening is that as you add more and
more weak learners here the boosting
algorithm ends up becoming more and more
confident in its answers which there
it's getting correct and therefore
effectively ends up creating a bigger
and bigger margin and what do we know
about large margins large margins tend
to minimize overfitting that's exactly
right so it counter-intuitively as we
create more and more of these hypotheses
which you would think would make
something more and more complicated it
turns out that you end up with something
smoother less likely to overfit and
ultimately less complicated so the
reason boosting tends to do well and
tends to avoid overfitting even as you
add more and more learners is that
you're increasing the margin and there
you go and if you look in the reading
that we gave the students there's
actually a detailed description about
this and a proof cool ok so there you go
Michael do you think then that boosting
overfits
never seems like such a strong word I
mean the story that you told says that
it's gonna try to separate those things
out but I guess I guess it doesn't have
to be able to do that I mean it could be
that for example all the weak learners
are I don't know very unconfident very
inconsistent hmm okay well you know
maybe maybe it's worthwhile in our
little diversion here to take a five
second quiz what do you think I think
it's worth the time
done</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>