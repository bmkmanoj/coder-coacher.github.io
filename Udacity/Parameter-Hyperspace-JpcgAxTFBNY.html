<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Parameter Hyperspace! | Coder Coacher - Coaching Coders</title><meta content="Parameter Hyperspace! - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Parameter Hyperspace!</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JpcgAxTFBNY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">learning rate tuning can be very strange
for example you might think that using a
higher learning rate means that you
learn more or that you learn faster
that's just not true in fact you can
often take a model lower the learning
rate and get to a better model faster it
gets even worse you might be tempted to
look at the curve that shows the loss
over time to see how quickly you learn
here the higher learning rate starts
faster but then it plateaus when the
lower learning rate keeps on going and
gets better it is a very familiar
picture for anyone who's trained neural
networks never trust how quickly you
learn it has often little to do with how
well you train this is where SGD gets
its reputation for being black magic you
have many many hyper parameters that you
could play with initialization
parameters learning rate parameters
decay momentum and you have to get them
right in practice it's not that bad but
if you have to remember just one thing
is that when things don't work always
try to lower your learning rate first
there are lots of good solutions for
small models but sadly none that's
completely satisfactory so far for the
very large models that we really care
about I'll mention one approach called a
de grad that make things a little bit
easier I regard as a modification of SGD
which implicitly does momentum and
learning rate DK for you using a de Brad
often makes learning less sensitive to
hyper parameters but it often tends to
be a little worse than precisely tuned
SGD with momentum it's still a very good
option though if you're just trying to
get things to work so let's recap we
have this very simple linear model which
emits probabilities which we can use to
classifies things we now know how to
optimize its parameters on lots and lots
of data using SVD and its variants it's
still a linear shallow model though but
now we have all the tools that we need
it's time to go deeper</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>