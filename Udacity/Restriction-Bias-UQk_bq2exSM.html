<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Restriction Bias | Coder Coacher - Coaching Coders</title><meta content="Restriction Bias - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Restriction Bias</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UQk_bq2exSM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this brings up the issue of what
neural nets are more or less appropriate
for what is the the restriction bias and
the inductive bias of this class of
classifiers and regression algorithms so
Charleston you remind us what
restriction bias is well restriction
bias tells you something about the
representational power of whatever data
structure it is that you're using so in
this case the network of neurons and it
tells you the set of hypotheses that
you're willing to consider right so if
the if there's a great deal of
restriction then there's lots and lots
of different kinds of models that we're
just not even considering we're
restricting our view to just a subset of
those so in the case of neural nets what
what restrictions are we putting well
and we started out with a simple
perceptron unit and that we decided was
linear so we were only considering
planes then we moved to network so that
we could do things like XOR and that
allowed us to do more and then we
started sticking Sigma lights and other
arbitrary functions and two nodes so
that we could represent more and more
and you mentioned that if we let the
weights get big and we have lots of
layers and lots of nodes we can be
really really complex so it seems to me
that we're actually not doing much of a
restriction at all so let me ask you
this then Michael what kind of functions
can we represent clearly gonna represent
boolean functions because we did that
can we represent continuous functions
that's that's a great question to ask
that's we should try to figure that out
so in the case as you said in boolean
functions we can get if we give
ourselves a complex enough network with
enough units we can basically map all
the different subcomponents of any
boolean expression to threshold like
units and basically build a circuit that
can compute whatever boolean function we
want so that one definitely can happen
so what about continuous functions so
what is a what is a continuous function
a continuous function is one where as
the input changes the output changes
somewhat smoothly right there's no jumps
in the function like that well there's
discont there's no discontinuities
that's for sure alright now if we've got
a continuous function that we're trying
to to model with a neural network as
long as it's connected it has no no
discontinuous jumps and any place in the
space we can do this with just a single
hidden layer as long as we have enough
hidden units as long as there's enough
units in that layer and essentially one
way to think about that is if we have
enough hidden units each hidden unit can
worry about one little patch of the
function that it needs to model and they
the patches get set at the hidden layer
and at the output layer they get
stitched together and if you just have
that one layer you can make any function
as long as it's continuous if it's
arbitrary we can still represent that
with a neural network any mapping from
inputs to outputs we can represent even
if it's discontinuous by just adding one
more hidden layer so two total hidden
layers and that gives us the ability to
not just stitch together these patches
at their seams but also to have big
jumps between the patches so in fact
neural networks are not very restrictive
in terms of their bias as long as you
have a sufficiently complex network
structure right so maybe multiple hidden
layers and multiple units so that
worries me a little bit Michael because
it means that we're almost certainly
going to over fit right we're if we can
have arbitrarily complicated neural
networks and we can represent anything
we want to including all of the noise
that's represented in our training set
so how are we going to avoid doing that
excellent question so it seems like
there's there isn't exactly that worry
but it is the case though that when we
train neural networks we typically give
them some bounded number of hidden units
and we give them some bounded number of
layers and so it's not like any fixed
network can actually capture any
arbitrary function so any fixed network
can only capture whatever it can capture
which is a smaller set so going to
neural nets in general doesn't have much
restriction but any given network
architecture actually does have a bit
more restriction so that's one thing the
other is hey well we can do with
overfitting what we've done the other
times when we would to deal with
overfitting and that's to use ideas like
cross-validation and we can use
cross-validation to decide how many
hidden layers to use we can use it to
decide how many nodes to put in each
lair and we can also use it to decide
when to stop training because the
weights have gotten too large so and
this is it's probably worth pointing
this out that this is kind of a
different different property from the
other classes of supervised learning
algorithms we've looked at so far so in
a decision tree you build up the
decision tree and you may have over fit
but it is what it is in regression you
know you solve the regression problem
and again that may have overfit what's
interesting about neural network
training is is this iterative process
that you started out running and as it's
running it's actually errors going down
and down and down so in this standard
kind of graph we get the error on the
training set dropping as we increase
iterations it's doing a better and
better job of modeling the training data
but in classic style if you look at the
error in the in some kind of held out
test set or maybe in a cross-validation
set you see the errors starting out kind
of high and maybe dropping along with
this and then at some point it actually
turns around and goes the other way so
here even though we're not changing the
network structure itself we're just
continuing to improve our fit we
actually get this this pattern that
we've seen before that the
cross-validation error can turn around
and and at this you know at this low
point you might have you may want to
just stop training your network there
the more you train it possibly the worse
you'll do and again that's it's
reflecting this idea that the complexity
of the network is not just in the nodes
and the layers but also in the magnitude
of the weights typically what happens at
this turnaround point is some weights
are actually getting larger and larger
and larger so I just wanted to highlight
that difference between neural net
function approximation and what we see
with some of the other algorithms</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>