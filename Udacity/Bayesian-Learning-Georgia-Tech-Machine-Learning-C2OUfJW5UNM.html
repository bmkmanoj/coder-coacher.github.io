<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bayesian Learning - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Bayesian Learning - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bayesian Learning - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/C2OUfJW5UNM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Michael so we've gotten to that
quiz and you see that Bayes rule
actually gives you some information it
actually helps you make a decision so
I'm going to suggest that that whole
exercise we went through was actually
our way of walking through an algorithm
so here's a particular algorithm that
follows from what we just did let me
just write that down for you all right
so here's the algorithm Michael so it's
very simple for each H in H that is each
of each candidate hypotheses in our
hypothesis space simply calculate the
probability of that hypothesis given the
data which we know is equal to the
probability of the data given that
hypothesis times the prior probability
of the hypothesis divided by the
probability of the data and then simply
output
whichever hypothesis has a maximum
probability does that make sense yeah
okay so I do want to point out that
since all we care about is computing the
Arg max as before we don't ever have to
actually compute that little bit so
that's a good thing because we don't
always know what the prior probability
on the data is so we can ignore it for
the purposes of finding the maximal
hypothesis so the place you removed it
from it seems like that's not actually
valid because it's not the case that the
probability of H given D equals that
probably if D given H times the
probability of H it just means that we
don't care what the value of it is when
we go to compute the Arg max that's
right so in fact it's probably better to
say that I'm going to approximate the
probability lipolysis given the data by
just calculating the probability of the
data given the hypothesis times the
probability the hypothesis and just go
ahead and ignore the denominator
precisely because it doesn't change the
maximal age yeah so it's nice that that
goes away right because it's hard to
know often what the prior what the
product probability over the data is
it'd be nice if we didn't have to worry
about the other one either which other
one the probability of eight where is
that coming from all right so where does
that come from so that's a deep
philosophical question sometimes it's
just simply something you believe and
you can write down and sometimes it's a
little harder and that's actually good
that you bring that up when we compute
our probabilities this way so it's
actually got a name it's the map or the
maximum a posteriori hypothesis and that
makes sense it's the biggest posterior
given all of your priors but you're
right Michael that often it's just as
hard to say anything particular about
your prior over
the hypotheses as it is to say something
about your prior over the data and so it
is very common to drop that and in
dropping that we're actually computing
the Arg max over the probability of the
data given the hypothesis and that is
known as the maximum likelihood
hypothesis I guess you can't call it the
maximum a priori hypothesis because then
it would also be mapped exactly although
I've never thought about that before by
the way just to be clear we're not
really dropping this in this case what
we really said is that our prior belief
is that all hypotheses are equally
likely so we have a uniform prior that
is the probability of any given
hypothesis is exactly the same as the
probability is any other given
hypothesis I see so you're saying if we
assume that they all are equally likely
then the choice of hypothesis doesn't
change that term at all the P of H term
so it really is equivalent to just
ignoring it exactly it's some constant
we don't even have to know what the
constant is but whatever it is it's the
same everywhere and therefore it doesn't
affect the other terms or in particular
affect the Arg max computation so that's
actually pretty cool right but I want
you to think about what we just did we
just took a something that was very hard
computing the probability of a
hypothesis given the data and turned it
into something that's much easier that
is computing the probability of you
seeing the data labels given a
particular hypothesis and it turns out
that those are effectively the same
thing if you don't have a strong prior
so that's really cool so we're done
right we now know how to find the best
hypothesis you're just finding the most
likely hypothesis or the most probable
one and that turns out to be the same
thing as just simply finding the
hypothesis that best matches the data
we're done it's all it's easy everything
that the math seems very nice and pretty
and easy but isn't it hiding like a lot
of work to actually do these
computations well sure well well look
you know how to do multiplication that's
pretty easy right so I guess the only
hard part is we have to look at every
single hypothesis yeah that's just a
slight little you know issue so
mathematically meaningful but
computationally questionable so the big
point there is that it's not practical
well unless the
of hypotheses is really really small but
as we know a lot of the hypothesis
spaces that we care about like four
examples linear separators are actually
infinite and so it's going to be very
difficult to use this algorithm directly
but despite all that I think that
there's still something important that
we get out of thinking about it this way
in just the same way that we get
something important out of thinking
about VC dimension even if we're not
entirely sure how to compute it in some
particular case this really gives us a
gold standard right we have an algorithm
at least a conceptual algorithm that
tells us what the right thing to do
would be if we are capable of computing
it directly so that's good because we
can maybe prove things about this and
compare results that we get from some
real live algorithm to what we might
expect to get but also it turns out it's
pretty cute because it helps us to say
other things about what it is we
actually expect to learn and I'm going
to give you a couple examples of those
just to sort of prove my point
sound good yeah okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>