<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Window Based Models | Coder Coacher - Coaching Coders</title><meta content="Window Based Models - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Window Based Models</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LeCWLX3yFIA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">for computer vision object recognition
we're inherently interested in finding
objects in images okay and that finding
means essentially two parts or there
sometimes mine there's this notion of
detection and labeling right so here on
this picture of you know person standing
next to a car I have to not only sort of
say what's in it I'd say oh okay over
here and I'll draw a box because a lot
of these things are window based saying
okay that's the car and by the way over
here that's the person so in general
that's what you have to do and the sort
of the most common approach to doing
that is referred to as windowing methods
alright so there are lots of different
ways of thinking about windows we can
start by thinking about sort of whole
images right so here I have a picture of
a couple of training examples of koalas
and a couple of training examples of
pandas this side borrowed from Kristen
gromek kristen has a thing for cuddly
folks with big ears so if any of you are
cuddly with big ears you can give
Kristen a call I hope Kristen's okay
with that so you know clearly here i
have my koalas we've talked about koalas
before how they're not bears right that
koalas are this thing and here we have
pandas which are bears butter just mean
anyway and what we would like to do is
we'd like to somehow learn a way of
discriminating between them and say you
know which ones are which so one way of
doing this would be to just build some
sort of a holistic description of the
entire picture with the assumption we'll
get to this part later that the picture
thing that I'm looking at is mostly the
Koala or mostly the panda so what would
be the simplest possible description
well here's one how would I just build a
histogram of the pixel colors now I'm
showing it here in a single image so
it's black and white you could do as an
r g and b and have you essentially have
three histograms and you could imagine
that basically you could try to
determine that you know these things
look kind of pretty much different than
this so that might be a good descriptor
to try to find where the boundary might
be you might try something even simpler
oh I don't know if it's simple yeah I
guess it's simpler it's not a very good
idea
right the same way we did in PCA you can
take all the little pixels put them in a
great big vector space and you could say
okay here a whole bunch of koalas here a
bunch of pandas find me the boundary you
know do you think that would work well
or you think that would not work well
what do you think Megan you think it
would work well no no okay one of the
problems is you know if I shift the
picture just a little bit all those
pixels move around in that feature
vector so a small change in the image is
going to give me a large change in my
feature vector which is not something
you want to have if you're going to have
a good ability to discriminate between
them so that won't work and what about
the color thing well you think if I just
did that color and intensity thing that
would work pretty well no no okay look
suppose I make it brighter today or
there's a shadow being cast upon the
thing that said or whatever I get you
know major variations in sort of the
distribution of the intensities okay do
you remember when we were talking about
trying to do tracking or trying to find
things in motion we were looking for
what we're called interest points I hope
you remember that okay you remember
interest points Harris features those
kind of things they were based on the
idea of places where you at high
gradients okay particular those where
you had variations in the direction of
the gradient as well and the idea was
that gradients corners were quite robust
with respect to changes in illumination
so if we take our koalas and our pandas
we can consider sort of edges contour
the idea of these intensity gradients
write those things tend to sort of stay
where they are in terms of you know if I
change the intensity all right but of
course we do have this problem that is I
move the image just a little bit i might
get shifts in the location of where
those pixels are so now remember sift
features so we did that whole thing with
sift features which took histograms of
the orientation and we computed these
little feature vectors right well here
imagine taking your picture and you just
cut it up into say four quadrants right
it's easier right so one two three four
and I suppose I just compute some
histograms of the orientations of the
gradients in each of those regions and I
do that for each of these pictures okay
its robust with respect to illumination
changes because the gradients there is
it robust with respect to small shifts
in the picture Megan yes or no yes yes
it is so the reason that it's robust
with respect to these small changes is
that you know if I move this guy's ear a
little bit around you know a bunch of
pixels the distribution of the gradients
is in that quadrant is going to remain
about the same that's why down here it
says locally order 'less what it means
is that i'm not worried about exactly
which pixel you all you're in I've got a
general distribution in one or two
different areas of the image okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>