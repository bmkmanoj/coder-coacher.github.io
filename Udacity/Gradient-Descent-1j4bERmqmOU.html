<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Gradient Descent | Coder Coacher - Coaching Coders</title><meta content="Gradient Descent - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Gradient Descent</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1j4bERmqmOU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so we're going to need a learning
algorithm that is more robust to non
linear separability or non-linear
non-separable 'ti that's on right uh
nonlinear so building non linear
separability non yeah I think left
parenthesis linear separability yeah yes
that's right negating the whole phrase
very good so and gradient descent is
going to give us an algorithm for doing
exactly that so what we're going to do
now is think of things this way so what
we did before was we had a summation
over all the different input features of
the activation on that input feature
times the weight W for that input
feature and we sum all those up and we
get an activation and then we have our
estimated output as whether or not that
activation is greater than equal to 0 so
let's imagine that the output is not
thresholded when we're doing the
training and what we're going to do
instead is try to figure out the weights
so that the not thresholded value is as
close to the target as we can so this
actually kind of brings us back to the
regression story we can define an error
metric on the weight vector W and the
form of that's going to be one half
times the sum over all the data in the
data set of what the target was supposed
to be for that particular example - what
the activation actually was right the
activation being the dot product between
the weights and the input and we're
going to square that we're going to
square that error and we want to try to
now minimize that hey Michael can I ask
you a question sure why one half of that
hmm yes it turns out that it turned in
terms of minimizing the error this is
just a constant and it doesn't matter so
why do we stick in 1/2 there let's get
back to that ok just like in the
regression case we're going to fall back
to calculus right calculus is going to
tell us how we can push around these
weights to try to push this error down
right so we would like to know how does
changing the weight change the error and
let's push the weights in the direction
that causes the error to go down so
we're going to take the partial
derivative of the this error metric with
respect to eat to the individual weights
so that we'll know for each weight which
way we should push it a little bit to
move in the direction of the gradient
so that's the partial difference so
that's the partial derivative with
respect to weight WI of exactly this
error measure so to take this partial
derivative we just use the chain rule as
we always do and what is to take the
derivative of something like this if you
have this quantity here we take the
power move it to the front keep this
thing and then take the derivative of
this thing but that's
so this now answers your question
Charles why do we put a half in there
because down the line is going to be
really convenient the two and a half
cancelled out so it's just going to mean
that our partial derivative is going to
look simpler even though our error
measure looked a little bit more
complicated so so what we're left with
then is exactly what I said the sum over
all these data points of what was inside
this quantity here times the derivative
of that and here I expanded the a to be
what the definition of the a now we need
to take the partial derivative with
respect to weight WI of this sum that
involves a bunch of the W's in it so
when we don't match the WI then that
derivative is going to be zero because
the you know changing the weight won't
have any impact on it the only place
where this changing this weight has any
impact is at X sub I so that's what we
end up carrying down that the this the
summation disappears and all that's left
is just the one term that matches the
weight that we care about so so this is
what we're left with now the derivative
of the error with respect to so any
weight W sub I is exactly this this sum
the sum of the difference between the
activation and the target output times
the activation on that input unit now
that looks exactly like or almost
exactly like the rule that we use with
perceptrons before it does indeed what
what's the difference well let's
actually let's um let's write this down
this is now just the derivative but
let's actually write down what our
weight update is going to be because
we're going to take a little step in the
direction of this derivative and it's
going to involve a learning rate</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>