<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Wrapping - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Wrapping - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Wrapping - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rA_WFUf2-YM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Michael so I just gave you a you
know one particular way you might do
filtering but let's see if we can expand
on that a little bit and come up with
some similar ways we might do wrapping
okay sure okay so I just gave you an
example of a kind of filtering you might
do through decision trees but really
what I'm doing there is I'm defining
some criterion and in this case it was
information gained right this is a way
that I will this is the function that I
will use to evaluate the usefulness of a
subset of features how much information
is that feature give me about a class
label can you think of any other kind of
filtering criteria we might use so I
don't know variants like so I want
features that have lots of different
values
that'll do something similar to
information gain I suspect okay yeah I
like that one so we might call that
variance or actually another version of
that would be entropy and there's tons
of ways to do this right so there's
there's something called the Gini index
which is a kind of version of entropy
there's variants as you're telling me
here that you're trying to pick features
that show up a lot anything else I could
imagine running a neural net and then
pruning away any input features that
have low weights okay why would we
describe that that's um that's actually
a kind of information game too right
because if the neural network doesn't
need it then somehow it doesn't seem to
be terribly useful in fact when we just
write that down oh I have another one
mm-hmm so if there's features that are
linearly dependent on other features
then maybe get rid of them so you want
independent features yeah or maybe we'll
say non redundant but but in this in
this kind of linear algebraic sense
right so if I have a feature let's say X
2 which turns out to be equal to you
know X 1 plus X 3 then I should probably
get rid of X 2 is that what you're
suggesting sure yeah that's that is what
I'm saying okay good and that makes
sense because I don't need X 2 well in
principle gives Y as you said we're done
in information but it also is the case
that for some learners like neural nets
shouldn't really even feel the
difference for things like decision
trees it might matter though because
having it having those bets um together
in one place could be more useful to
split on that's true but I do think you
said something important there but she
says it shouldn't feel the difference
you you're defining whether this is good
or bad in terms of whether it helps you
to learn but if we can get rid of a
feature then we save exponent as we get
rid of features we save exponentially on
the need for data yes so as long as it
doesn't make the learning problem harder
that's a good thing but as long as it
makes the learning problem easier in
terms of the amount of data you need
it's a win up until the point where it
makes the learning problem harder
because you don't have enough features
to do the actual learning and there's a
bunch of these and and I think all of
these are perfectly good answers you
could think of all kinds of statistical
tests for relevance I mean it's
basically built in here these are all
different ways of doing the same thing
which is figure out which features you
need in order to do learning by some
generic criterion I do want to point out
that something like entropy doesn't even
depend on the labels whereas something
like information game which is a kind of
conditional entropy does depend upon
knowing the labels what about wrapping
how would you go about doing wrapping so
we've got a bunch of different ways we
might do filtering what are sort of the
the way in which you could imagine doing
a wrapping you've got some learning you
don't know what it is you've got a bunch
of features you want to search over
those pass those features on to the
learner and see how well it does but you
don't want to pay an exponential cost by
looking at all possible subsets what
might you imagine doing well so I mean
we've already talked about a bunch of
algorithms that can search that aren't
necessarily exponential so various kinds
of local search and hill climbing could
be a way of doing the search part hmm I
like that let's write that down hill
climbing
now when you say hill climbing you
really mean like a gradient a regular
deterministic gradient search yeah I
mean I guess I was thinking of something
where what the search algorithm does is
it tells the learning algorithm here's
the features to try to use the learning
algorithm runs and it comes back with
some kind of error presumably on
held-out validation data otherwise we
run the risk of overfitting fair and it
gives a number back and then the the
search algorithm uses that to decide
what subset to try next
okay so you imagine that it's gonna jump
in some gradient directly or is it gonna
have to do something like randomized
optimization I think it would have yeah
I mean like randomized optimization okay
so in general we could take almost
everything we just did in the last
lesson on randomized optimization and
apply here so I think we might just say
just do random oh I see so it could be
could be mimic it could be genetic
algorithms it could be simulated
annealing right so you could think that
you could do randomized optimization and
that's going to be important precisely
because we want to solve this
exponential problem we don't want that
to be an issue okay that makes sense now
we've already thrown out exhaustive
search where we look at all possible
features because again that's
exponential is there anything else you
can think of I'm thinking of one more so
let's say we can read my mind and really
when I say I'm thinking one more I'm
thinking of two more that's that are not
randomized optimization algorithms but
they're gonna somehow search mm-hmm a
star yeah but that's not what I was
thinking oh I give up I'll give you a
hint
you could go forward or you could go
backward M no no no no no I'm thinking
of exactly a forward sequential
selection and backward elimination I
don't think I know what that means
oh so let's talk about that then</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>