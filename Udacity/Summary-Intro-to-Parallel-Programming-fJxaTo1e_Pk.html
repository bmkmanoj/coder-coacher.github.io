<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Summary - Intro to Parallel Programming | Coder Coacher - Coaching Coders</title><meta content="Summary - Intro to Parallel Programming - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Summary - Intro to Parallel Programming</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fJxaTo1e_Pk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right it's time to wrap up here's
what I hope you've taken away from this
unit remember a pod analyzed parallel
lies optimize and deploy the important
points here are two profile guided
optimisation at every step and deploy
early and often rather than optimizing
forever in a vacuum I can't emphasize
this enough optimization takes effort
and often complicates code so optimize
only where and when you need it and go
around this cycle multiple times and
most codes are limited by memory
bandwidth so compare your performance to
the theoretical peak bandwidth and if
it's lacking see what you can do about
it things that will help improve in
order from most the least important are
assure sufficient document C make sure
you have enough threads to keep the
Machine busy this doesn't mean having as
many threads as you can possibly fit on
the machine but you do need enough that
the machine is basically busy coalesce
global memory accesses really strive to
see if you can find a way to cast your
algorithms that you achieve perfect
coalescing and if you can't consider
whether you can do a transpose operation
or something that will get poor
coalescing once but then put the data
into a memory form where all your
subsequent axises will get good
coalescing remember littles law in order
to retrieve the maximum bandwidth you
may need to reduce the latency between
your memory accesses and so for example
we saw that in one case we spent too
much time waiting at barriers by
reducing the number of threads in a
block we were able to reduce the average
time spent waiting at a barrier and help
saturate that global memory bandwidth
we've talked about minimizing the branch
divergence experienced by threads
remember that this really applies to
threads that diverge within a warp if
the warps themselves diverge in other
words of all the threads within a warp
take the same branch going the same code
path then that comes for free there's no
additional penalty for threads in
different warps diverging it's only when
threads within a warp diverge that you
have to execute both sides of the branch
as a rule you should generally try to
avoid branching code ok code with lots
of if statements switch statements and
so on and you should generally be
thinking about avoiding thread workload
imbalance in other words if you have
loops in your kernels that might execute
a very different number of time
between threads then that one thread
that's taking much longer than the
average thread can end up holding the
rest of the threads hostage all that
said don't let a little bit of thread
divergence freak you out
remember we analyzed a real world
example of dealing with boundary
conditions at the edge of an image and
figured out that in fact the if
statements to guard the edge of the
images weren't really costing us very
much
only a few warps ended up being
divergent if you're limited by the
actual computational performance of your
kernel rather than the time it takes to
get the data to and from your kernel
then consider using fast math operations
this includes things like the intrinsics
for sine and cosine and so forth that go
quite a bit faster than their math dot h
counterparts at the cost of a few bits
of precision and remember that when you
use double precision it should be on
purpose so just typing the literal 3.14
well that's a 64-bit double-precision
number and the compiler will treat it as
such whereas typing 3.14 f tells the
compiler hey this is a single precision
operation you don't have to promote
everything I multiply this by or add
this to to be a double precision number
finally if you're limited by host device
memory transfer time consider using
streams and asynchronous mem copies to
overlap computation and memory transfers
and that's it now go forth and optimize
your codes</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>