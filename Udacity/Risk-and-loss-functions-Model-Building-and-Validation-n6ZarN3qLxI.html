<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Risk and loss functions - Model Building and Validation | Coder Coacher - Coaching Coders</title><meta content="Risk and loss functions - Model Building and Validation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Risk and loss functions - Model Building and Validation</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/n6ZarN3qLxI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">We are ready to try to build various models.
But before we get into the next steps,
let's learn what it means to evaluate the model.
As you must have noticed, we are constantly iterating to and
fro from the validation phase and the questioning phase and the modeling phase.
So what it is that we use to validate these models?
We'll take a short digression from our example that we have been following
to introduce some key concepts involved with the model validation techniques.
We won't end up using all these techniques in our specific example, but
this material is very important to clearly understand the concepts behind
model validation.
When we build models and train them,
we need some way to measure how far out model is from the actual values.
The loss function is just such a concept.
It measures how far an estimated value of a quantity is from the true value.
Let's take an example to see what the loss function is.
Let's take the special case of parametric model.
Let's say, the parameter is theta, which is the true value of a quantity.
And then theta hat is an estimated value for the same quantity.
Remember, theta hat is a function of the data that we collected.
The idea is that we are fitting a parametric model to the data.
And we always need a measure of how well we perform.
We accomplish this by defining a loss function.
It is up to you to choose how you define the loss function.
The choice depends on the specific problem you're trying to solve.
Here are some examples of loss functions you may encounter.
I'm using the notation of theta and theta hat for a parametric model.
One example is the squared error loss.
In this case, you take the difference between theta and
theta hat and square them.
Another is the absolute error loss.
In this case, you take the difference between theta and theta hat and
the absolute value of that difference.
An Lp Loss is essentially the Absolute Error Loss raised to the power p.
Another very interesting quantity is the Kullback-Leibler Loss.
It's a complicated formula and
it's an information theoretic loss calculated for two different distributions.
You will often encounter squared error loss in statistical learning.
The square error loss is sensitive to outliers.
For values of theta hat that are far from the values of theta,
it contributes a large quantity to L.
Also, notice this is always a positive quantity given it's
a square of difference.
The Absolute Error Loss is not so sensitive to outliers.
However, it is not smooth at the value where theta hat is exactly
equal to theta and thus, it's difficult to differentiate it at that point.
We mentioned that the loss function depends on the selected data.
Remember, theta-hat is a function of the data.
We want a measurement that is averaged over many possible datasets.
The risk is such a function.
It is an average measure of the loss.
And we calculate this by taking the expected value of the loss function.
So as you see here, the risk is denoted by R which is
the expected value of the loss function and it's calculated under the integral.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>