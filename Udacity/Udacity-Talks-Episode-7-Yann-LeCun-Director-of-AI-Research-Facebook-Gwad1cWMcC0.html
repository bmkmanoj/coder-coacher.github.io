<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Udacity Talks Episode 7: Yann LeCun | Director of AI Research, Facebook | Coder Coacher - Coaching Coders</title><meta content="Udacity Talks Episode 7: Yann LeCun | Director of AI Research, Facebook - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Udacity Talks Episode 7: Yann LeCun | Director of AI Research, Facebook</b></h2><h5 class="post__date">2016-11-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Gwad1cWMcC0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay I'm super excited to be here with
my really good old and longtime friend
Yan Lacan who works at Facebook and NYU
as the father of deep learning so to
speak
this is audacity talks number seven we
hear not miss to do and young welcome we
met I think in 1990 which dates us
unfortunately and I I you kept you here
I've lost my hair and all the way yeah
but you you did a thesis on machine
learning in 1987 in a small European
country called France it's more any
country than then no one no one was
thinking about machine learning like how
on earth did you do this you know I got
into machine learning through philosophy
it's a weird story but but I got I was
interested I was an engineering student
here Kiko engineering student and I was
interested in intelligence running etc
and I stumbled on a philosophy book that
was sort of a debate between Noam
Chomsky and Jean Piaget Jean Piaget is a
developmental psychologist that you know
wrote a lot about the child children
develop and there was this argument
basically between innate and and and you
know nature and nurture he want right
and on the side of Piaget was an article
written by Seymour Papert
the co-author of the book perceptron
that basically killed the field of
neural nets in the 60s because I
couldn't know an EXOR well because of
you know various reasons the limitations
were were were clear in that in that
book but this this debate I was reading
about took place ten years later in the
late 70s and and paper was here arguing
for the fact that you could learn things
because there was this very simple model
the perception that could run you know
surprisingly complex concepts and it's
the first time I read about a running
machine and so I was hooked I kind of
dig the literature everything I could
find on on the perceptron and figure out
that the all the papers basically ended
in the late 60s when I backed by the
machine learning at that time
a very active field I get Tom Mitchell
and many others are working on it and
they all use symbolic methods you select
decision trees ya nearest neighbor why
on earth did you look at the neural
networks well you seem to me that if you
want to do perception that would be kind
of a better approach because of the kind
of massive amount of data you have to
process which at the time was you know
8x8 binary matrices right that was the
nice event we had at the time you know
we couldn't afford to run to run neural
net some bigger images than that I just
thought the concept was very appealing
of sort of trying to reproduce the idea
of learning in the brain with you know
synaptic changes and everything and it
was it was a whole school that that had
died out but I thought I thought the
idea was very intriguing it's true at
the time the what was fashionable were
expert systems on the side of AI as well
as there was there was a very small sub
community Tommy sure was one member of
this of symbolic learning and then a
completely separate community in pattern
recognition which also had gone to was
works called structural pattern
recognition which was basically also
symbolic and grammatical in France and
then you you basically invented not just
back propagation that's probably two or
three people who'd clean that and you're
one of them which is amazing but you
also invented convolutional neural
networks at a time when their compute
power was so limited like what was the
reception of the feel to this crazy idea
of like train something with like a
million data points many parameters well
so we didn't have any on data points or
me on parameters we didn't have the
computers to run this either
you have to realize also that one reason
why it took so long for backprop to
emerge is that for back back to work you
need continuous nonlinearities you know
you need things that are relatively
smooth you know value is not smooth but
you know things with Sigma rates and
stuff like that and back in the old days
were computing a multiplication or
performing a multiplication on a
computer for point multiplication was
very expensive and so people would stick
with you know binary units because it
was it was it was easier you didn't have
to do multiplies you just had to do adds
and subtract right and so I guess it's
one of the reasons that back prop didn't
come up earlier is the fact that
hardware was so limited and and people
so even think are using continued
did you or anybody else understand the
significance of your invention at the
time the potential perhaps but the the
fact that it was going to work so well
eventually not not clear at the time so
what happened there so during my PhD to
this I actually implemented so locally
connected multi-layer nets trained with
some form of back populist actually
closer to what we now call target pop so
instead of propagating gradient to
propagate virtual targets for each of
the units in the in the system but there
is a regime in which those those are
equivalent in fact that to to back pop
and you know a number of people sort of
came up with similar ideas in fact you
can trace the idea of sort of back
propagating gradient back to the sort of
static methods in control theory from
the 60s for like computing rocket
trajectories so the idea had been around
is just that people didn't realize they
could use them for running and and so
commercial Nets came later so I finished
my PhD in 87 and I did a postdoc at neo
tomorrow with Jeff Fenton that's where I
implemented the first commercial net but
it was kind of a small version of it you
know trade on a few hundred samples and
then I could scale them up when I went
to Bell Labs and you know I had a whole
you know Sun for just for me yeah you
spend a lot of time and postal data sets
remember there's all these yeah that's
right handwritten digits on unless it's
a little Dicky I guess rather get better
and better
yeah fast forward right so now you're
probably arguably one of the most famous
people machine learning you find a
phenomenal group at Facebook you teach
at a collagen in New York and called NYU
small sample so
40,000 students 4,000 students tell me
like what what happened like why on
earth is it so popular today why wasn't
it as popular 20 years ago they invented
it and what hadn't but happen to you
okay so you know I kept working on their
own ads in the early 90s and you know
we're pretty successful we had systems
that we built at AT&amp;amp;T that were reading
checks and they got commercialized and
at some point in the late 90s that were
reading something like 10 to 20 percent
of all checks in the US so you know very
successful but the the community turned
away from those methods
partly because
this was before it was very common to
distribute your code in open-source it
was before the internet really early 90s
and so it was before people had common
platforms where you could just you know
compile you thing with GCC and just you
know run on your another platform so it
was not easy to distribute your not your
knowledge essentially like we can do now
and and those neural nets are very
finicky you know at the time you needed
the fairly complex piece of software to
make them work and you know a lot of
investigation etc and and just building
that tools those tools were sort of big
investments and so very few people
figured out really how to do this and so
it kind of died because it was very
difficult to to use and it wasn't clear
to people that it was was worth the
effort so I kind of stopped working on
on their own heads really for a bit for
about five years between 1996 or 97 to
about 2000 to 2001 when I started again
worked an image compression and I was
also running a group at AT&amp;amp;T so I had
fewer you know less time for now in
research when I started again when I
left AT&amp;amp;T in a big way that's when
Jeffrey Antonio's revenge oh and I
started the deep running conspiracy to
basically rekindle the interest of the
research community in all nights because
we knew that those methods eventually
would work the whole idea of running
features was obviously good idea yeah so
I mean you have the dream job obviously
you work in this wonderful lamp that you
run at Facebook you get to play with
machine learning all day and and many of
our students in the world did take
another degree programs and listening in
right now would love your drop and drop
like your chops a revolver size how on
earth did Zach discover you you have to
ask him but I mean he was interested in
starting an effort in AI essentially the
word there was a few group there were
few people at Facebook who had been
experimenting with with commercial meds
for image recognition face recognition
various other things and they were
getting really good results and they
talked to to mark and Mark said that was
really interesting and then you realize
that this was just about the time that
we're talking early 2003
scene just by the time that Facebook was
about to turn 10 years old you know
well-established on this market and it
was thinking about the next 10 years or
the next 20 years what is it going to be
into what is Facebook going to be in 10
years how how are we going to interact
with the digital world with each other
etc and you realize quickly that AI was
going to be an essential piece of this
and with the progress the quick progress
brought about by by the Browning that
there was kind of an opportunity there
so he wanted to start an AI group and he
went around and you know found y'all
just people didn't ask me the usual
suspects you were trapped at Google or
it was running Google X yes
oops no I mean you know I'm gonna run it
you know talk to quite a few people but
you know eventually recruited by my
students out of Google X Marwan Zito
okay yeah and and then you know contact
with me and and and I said that sounds
great
creating a research lab on the iPhone
scratch it's probably one of the few
jobs I might know how to do because I've
been in industry before and I can have
some idea how to organizes but as I said
I only have two conditions are not
moving from New York and I'm not
quitting my job in active as an academic
and he said yes I say where do I sign
it's great it's great so where where
does Facebook use deep learning today so
Facebook uses deep longing for a lot of
things which you don't necessarily see
it's not very visible so there are
things that are very visible like there
is this system for the visually impaired
that describes an image mm-hmm when so
when visually impaired you know connect
on Facebook when an image is shown it's
it's just localized and that's the
visible part but there is a lot of
differently that's used in the
background to rank content so every
every time you connect your Facebook
Facebook can show you two or three
thousand items per day and we you don't
have time to look at all of this I don't
sure you don't
even though I spent a lot of time in
Facebook because we use it internally as
our discussion basically but but you
know Facebook needs to show you maybe
100 150 items every day and it needs to
select the ones among the two or three
thousand that are most likely to
interest you so it needs to understand
content and understand people's interest
and right to the two right that's
basically all deep running so we often
take questions from the audience we have
an online audience that asked questions
we have an impersonal unite but Calvin
online for asked question where you
think the future of AI will be in five
to ten years okay well if the things
that we're working on actually pan out
you know like I can tell you what I hope
will happen if those things work out so
there are essentially two or three areas
that we're working on general areas so
one of them is I guess something you
could call differentiable computing
which is the idea that you can implement
pretty much any operation that a
computer would do or a person would do
by reasoning like a expert system
applying wheels would do in terms of
differentiable operations where you
would replace logic by algebra you would
make everything differentiable so that
would allow a machine to essentially
learn complicated sequences of actions
by being shown examples so that would be
very useful for example for training
dialogue systems they can you know talk
with a person in serve a natural way and
you know perhaps be considerably more
useful or smarter than the current
veteran systems that's one area and the
other areas future lawyer will be in a
facebooking I system possibly yeah
possibly looking forward to that day
yeah yeah my future would be my other
son but so the other area which i think
is is also very important a very key to
making significant progress in AI is to
get machines to require common sense so
it's an old question that AI people have
asked themselves you know AI systems
don't have common sense and so
the ability to machine to have common
sense essentially relies on those
machines being able to learn how the
world works and the way we learn how the
world works is by living in it it's by
observing how things work and we don't
what's possible it's not possible by
just observing how how the world evolves
what what can happen the physical
constraints of the world and things like
this and how do we get machines to do
this and so I guess in the field we
would call this unsupervised learning or
maybe predictive learning basically
learning with data that is not has not
been annotated by humans in such a way
that the machine can run sort of Ford
models of the world right so the essence
of intelligence in my opinion this is a
very personal opinion not everybody
needs to agree but the essence of
intelligence I think is the ability to
predict if you can predict how the world
is going to evolve because the world is
the world or how it's going to change
because of your actions that means you
can plan ahead and you know act in an
intelligent way and so how do we get
machines to to learn to predict that
means you also get lots of data for free
because all you do is wait a few seconds
and then you have a train in there
exactly
all right so so one of things we're
working on is video prediction so here
is a snippet of video that we show to a
machine and we ask it what is the world
and it look like a quarter second I have
a second from now maybe a second maybe
an hour from now what is what is
Facebook for dictum for the US
presidential election at this point this
book is not pretty thing no ways you
could love to see bid went and gentlemen
general intelligence so you think this
is campaign artificial intelligence is
key it has to be specialized like you
can play go but you go playing computer
won't be able to drive your car and then
there's people who say you're going to
get this good general AI which kind of I
would smart all of us never dimension is
that in reach AGI right artificial
general intelligence right so I think I
don't know if it's in reach my picture
of this my analogy would be we are on
the landscape on the way to AI and we
see a mountain and the mountain I'm
seeing right now
is this problem of learning forward
models that's
so you know basically having a machine
learn by observation and maybe by action
as well and that's the first mountain we
don't know how many mountains are behind
it and so it's easy to be optimistic
like generations of AI researchers
before I also predicted that you know
the you know machines would be more
intelligent than than people by you know
10 years from now 15 years from now 20
years from now and they were all overly
optimistic and so the reason they were
overly optimistic is that you have a set
of technique you don't see the problems
yet until you you hit a wall or a
mountain so we see one mountain we know
we need to get behind it we don't have
any mountains that are behind
interesting interesting look we have
literally hundreds of students that send
us questions and I have a big stack of
questions here which I can't all read
but several of them asked basically what
does it take to become a member of your
group like what qualifications do people
need do they really need a master's
degree a PhD maybe a nano degree from
audacity sufficient perhaps another very
small they're like the small deliveries
okay so there are several roles at
Facebook a research so filled with our
research is roughly 2/3 research
scientists and 1/3 engineers ok and the
research scientists are all people with
PhDs and we generally only hire on a
like a permanent research scientist
after pisaq so people are PG we hire s
postdocs and then we can make them
permanent eventually the engineer
population is much more diverse in the
sense that there are people with PhDs
are people with masters there's people
with just a bachelor and their final
degrees and there could be people with
nano degrees if they have done something
that we think is interesting so you know
you've contributed to a like something
really interesting to some open source
deep rounding stuff or you made a
project that looks really impressive
your degree doesn't matter at that point
it's really your your your contribution
your expertise so you look at a project
portfolios occasionally and look at what
people have done and that there's a
factor in hiring I mean we look we look
at this for for anybody we hire you know
it's not always the case that people
have things that are visible like this
but it's a very important track record
for us if someone has contributed to an
important of source project in a good
way
good committed asking online in your
opinion areas in the workspace or
corporate world that we most affected by
AI power technologies next decade or so
well so you know obviously all the web
industry is you know very very active in
the in the domain but in every you know
everybody's life I'm not going to have
to convince you that you know is going
to have a huge effect on Transportation
on medicine
you know you'll be working on both of
those things so and those are probably
the most visible effects I think that
people will see over the next 5-10 years
of application of deep running in AI
chain is asking a question which I also
have by the way I'd love to hear you
answer this one what is the main bottom
lack of deep learning and what may be
the next big thing in this field so
unsupervised running as I said before I
think that's really the big bottleneck
right now we have to you know 95 percent
I went to 75 percent it's you know I'm
sort of pulling this number out of thin
air it's probably higher than this most
applications are deep running today
most applications of machine learning
today or use supervised running all
right so you train an image recognizer
you should be thousands of images and
you those images have been labeled and
tell the Machine what the label is and
you know if it makes a mistake you
correct the parameters but that limits
the amount of information that the
machine has access to per sample so
training a machine to do things like
video prediction for example would allow
it to learn a lot more about the world
and and that's that's really where the
bottleneck is I think that is kind of
supervised right because you could say
you look at the last 20 seconds of video
yeah take a little break
bathroom break and come back ten seconds
later and put it to fine then right here
is a difference though it's its
prediction with uncertainty so if I if I
put that that pen if I hold this pen in
my hand and I let it go you know it's
going to fall but you can't predict
which side is going to fall every time I
do it it falls in a different direction
right and so if you use a supervised
learning algorithm to do this prediction
and your supervised learning algorithm
you ComNet the ComNet whatever it is
predicts this and the world actually
does that and they mean by Peter says
this stays up there right well there is
a small
for that to happen outcome possibly no
but the mean outcome from all the false
is going to stay up no the mean
alcoholics the mean outcome is an
average of all of the positions of the
pair you know you think so what you get
is a transparent pen you know in all
position which is not a good prediction
that's so so if you use a normal you
know supervisor on you will be square to
do this prediction that's exactly what's
going to happen is going to predict the
average of all the possible futures
which is not a good future so what you
want to tell the Machine is you want to
tell it okay the you know the the data
tells you this in this particular
example you predicted that it's
conceptually correct so how do you try
to machine this way to to tell it you
know even if the answer you produce is
not the exact correct one it belongs to
the family of plausible futures and one
of the province approaches that we're
working on to solve this problem is
something called generative adversarial
networks so this is a an idea that came
up generative adversarial and their
social networks get what's your Facebook
feed because you're gonna have a
generative advisory network popping up
on you if you don't look out for you
right exactly loses those names this so
this idea came out of yoshua bengio slab
this is paper by Ian good fellow who is
now open AI and this idea that
essentially I mean you can interpret it
in several different ways
he's interpretation is probabilistic but
you could you can think of it as you
train a predictor or generator and you
you have another network which is
basically a trainable objective function
trainable loss function and what this
what this other network learns is the
the set of possible predictions that are
correct even though the you know the
actual prediction by the generator
network may not be exactly correct and
those two networks train against each
other which is why it's called
adversarial training I hope you guys
change the name before we see the public
little entice not what is this mind you
he's not mighty change channel it was
real it works and Facebook but it's a
great idea it's a good so I think I
think frankly I think it's the best idea
in machine learning last ten years
interesting you should teachers are we
teaching it not yet yes you should teach
it
okay we come back you teach I teach
analyzing running course at NYU yes so
we teach more students in other ways
maybe more up-to-date
I would I have an audience question here
anybody has a question here is please
proof so outside of deep learning is
there anything else that Facebook AI
works on and are there other
technologies that are in resurgence once
we find out about new capability the
computers and GPUs that we ignore just
like new clarity was ignored for so long
well I mean I I hope I hope people will
come up with you know the successor to
to keep running I think the whole idea
of assembling learning machines through
by assembly modules and and using sort
of gradient based learning that's not
going away so in that sense deep
learning is not going to disappear it's
just going to become you know just
another tool but I think what people are
working on now is you know different
types of architectures different
training procedures this adversarial
idea is is very new in the sense that it
breaks the the model that we're used to
in machine learning where we minimize
some loss function here we're actually
finding a set of points of a function
and it's very complicated there's a lot
of things we don't understand things
don't conversion we don't know why so
but when it works it's amazing so so
there are you know I love a lot of
things like this so I think deep
learning is part of the toolbox almost
every system we put out have some deep
running in them but a lot of people are
not working specifically on deep
learning they are using deep learning
for applications like no natural
language understanding keep your vision
you know speech recognition translation
you know things of that type there's
people working on causal inference the
people working on sort of the applied
math type side of things optimization
large-scale SGD fast nearest-neighbor
I mean there's a lot of techniques like
that addy asked a question online asking
how do you apply deep learning to very
sparse data like like small sample sizes
and sparse matrices okay so you could
you know then I can be sparse in two
ways right it can be sparse in the sense
of not having many samples okay just
parts in the sense of having a high
dimensional input where most of the
values are zero mm-hmm right and that's
a situation that's very common about
people I mean give me one data point of
your favorite pen and I'm able to
recognize the span from novi points even
after one team data right well but
but that's going to ignoring you all the
learning that you've done before right
yeah how do you do get deep learning to
become such a versatile learning machine
that they can just give it one example
and it's really useful so there's such a
thing called transfer learning right
where and people have done this with
imagenet is actually a product inside
Facebook that does is where you train a
commissioner to recognize images on
whatever categories that you have in
your in your training set the largest
number of categories possible if you
want and then what you have is a network
that can basically represent any image
so at the top layer or the the second
last layer it has a feature vector that
you can use for just about any
recognition task and so take this vector
plug it into your favorite classifier
which can be very simple well
regularized and with just a few examples
now you can recognize a new category in
fact that's how face recognition works
at Facebook very very cool I have to ask
you so other people like Elon Musk have
talked about how artificial intelligence
the most dangerous thing they were
developed and they he wants to go to
Mars to escape it and Stephen Hawking
has made some interesting predictions
about it so as Bill Gates yeah we are on
the spectrum of like complete paranoid -
absolutely optimistic various stand so
cautious optimism washes out the visible
yes okay what that means is that I don't
think the mizzen I don't think we're
going to build dangerous you know
humanity threatening or sort of you know
existence threatening AI systems by
accident I mean it's hard enough to get
them to just you know drive on a highway
without like that could be adamantly
threatening device say improperly
trained highway car yeah but but but it
won't destroy the you know all of
humanity I think some
a limited number of people hopefully but
so so I you know there there are the the
paper clips in are you right where you
build a machine whose only purpose is to
build paper clips and and you know all
of a sudden it's so smart that you
convince yourself not to turn it off and
it turns the entire galaxy to paperclip
so this is the this is the NIC box from
scenario in this book super intelligence
and this is work that you know all of
these people you mentioned have read and
have told themselves oh my god we need
to be careful I don't believe in this
scenario I mean there is this would be
kind of like you know building a sir
driving car with a thousand horsepower
engine the throttle is all the way
through and you can't remove it there is
no brake and no safety belt nobody would
buy it first of all that's true so let's
see so I so cautious optimism in a sense
that I think there are relatively simple
way to build machines in such a way that
their intrinsic motivations are aligned
with human values so how to do this in
detail is really not entirely worked out
but in my opinion we have at least 20 30
maybe 50 years to think about it because
you know we just don't have the
technology to build super intelligent
machines so GI as you were mentioning
before that's going to take decades I
think is decades obviously yeah so do
you think we are in a singularity right
now when it comes to you is it too
premature I well I think we are in a
fast evolution which is overdue so my
expectation you know when I started
getting this field is that we're going
to have some sort of continuous
evolution turns out that there was some
evolution and then a plateau for 10 15
years and now we stop work on new
networks right it's not because I
because the entire works you broke know
neighs sorry I find said that if I had
gone to academia and like you know had a
photo wing with students and stuff like
that maybe you would have been different
I don't know you know if I go to
academia earlier I don't know if that's
true but um so I don't I don't I don't
believe that we're any singularity you
know there is nothing that looks more
like an exponential at the beginning of
a Sigma
and and in every physical process or
social process there is friction which
initially those terms with friction
terms don't matter you don't see them
but eventually when things start picking
up the friction terms start start
getting non-negligible and even
eventually take over and that's why
create saturation so I don't believe in
the whole concept of singularity really
right I'm going to ask one last question
before close down and that pertains
earth students we have many many
students online and some in our room
here and quite a few come to us audacity
because they want to basically get a bit
actually I'm going to get into machine
learning and our our goal of course has
been to empower this these are students
in all countries in all genders and all
ages so my question would be what piece
of advice would you give the student say
who grows up in India or Bangladesh or
South America well I think it's a it's
it's a great time to to be a young young
student who is really fascinated or or
interested by by this kind of technology
now you have access to data computers
are relatively cheap you know access to
information is ubiquitous when I was
growing up it wasn't like that I was you
know kind of a nerdy you know kid and I
was fascinated by technology already at
a young age science and technology in
general so so were you and and you know
I was building stuff you know building
electronic things and airplanes and
stuff like that and you know about my
first computer I was seventeen and this
was 1977
so taught myself assembly programming
because there was no alternative to it
me too that's great 'full yes so it's
very proud I was able to do this that's
right and it was you knowing later
complicated expensive etc but you know
you could buy magazines and learn about
this there that there was the equivalent
of destiny now of course things like
like like Udacity open source menu put
your assembly code into a magazine right
yeah that's right that was the other
open source then you had to like type it
in your copy go and save it to a
cassette player which I could reward so
I had to do it every single time for
French right yeah yeah
so I think it's a it's a great time to
be a you know young motivated student
you can you can play with the you know
with the grades and you can make
contributions and so you don't need to
be shy
and you'll be you'll be recognized if
you make contributions to the pan sauce
well that's a wonderful vehicle for
showing what you can do and I'm so
delighted you're here and I can't
imagine that resume met but 25 years ago
36 years ago and it's great to see you
so successful Facebook and changing the
world thanks for coming to our students
is our seventh version of Udacity talks
again Yann LeCun
director of AI research at Facebook
professor at NYU and all-around a
wonderful human being and I see you next
time hopefully with our technology
working thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>