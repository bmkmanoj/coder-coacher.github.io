<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bayes Rule p2 - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Bayes Rule p2 - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bayes Rule p2 - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/m65_20ZH-iM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so let's look at the last quantity we
haven't talked about so far and that is
the probability of the hypothesis well
just like the probability of D is the
prior on the data this is in fact your
prior on the hypothesis so just like the
probability of D is a prior on the data
the probability of H is a prior on a
particular hypothesis drawn from the
hypothesis space so in other words it
encapsulates our prior belief that one
hypothesis is likely or unlikely
compared to other hypotheses so in fact
what's really neat about this from a
sort of AI point of view is that the
prior as it's called is in fact our
domain knowledge so in every algorithm
that we've seen so far everything that
we've said there's always someplace
where we stick in our domain knowledge
our prior belief about the way the world
works whether that's a similarity metric
for kN
or whether it's it's something about
which features might be important so we
care about high information gain and
decision trees or our belief about the
the structure of a neural network those
are prior beliefs those are that
represents domain knowledge and here in
Bayesian learning here in this notion of
Bayes rule all of our prior knowledge
sits here in the probability our prior
probability over the hypotheses does
that all make sense yeah it's really
interesting I guess so we talked about
things like kernels and similarity
functions as ways of capturing this kind
of domain knowledge and I guess I guess
what it's saying is that it's maybe
tending to prefer or assign higher
higher probability to hypotheses that
tend to group things a certain way
exactly right so in fact when you use
something like Euclidean distance and
came in what you're saying is well
points that are closer together ought to
have similar labels and so we would
believe any hypothesis that puts points
that are physically close to one another
to have similar outputs we would say are
more likely than ones that put points
that are very close together to have
different outputs 8 so let me just
mention one last thing before I give you
a quiz okay so see if this makes sense
I'm gonna see if you really understand
Bayes rule so let's imagine that I
wanted to know under what circumstances
the probability of a hypothesis given
the data goes up
but on the right side of the equation
would you expect to change go up or go
down or stay the same that would
influence whether the probability of a
hypothesis goes up so the probability of
the hypothesis given the data what could
make that combined quantity go up so one
is looking at the right hand side the
probability of the hypothesis so if you
have a hypothesis that has a higher
prior has is more likely to be a good
one before you see the data then that
would raise it after you see the data to
right and I guess the probability of the
data given the hypothesis should go up
Oh which is kind of like accuracy right
it's kind of like saying that if you
pick a hypothesis that does a better job
of labeling the data then also your
probability of the hypothesis will go up
right anything else I guess the
probability of the data going down but
that's not really a change from the
hypothesis right but it is true that if
this goes down then the probability the
hypothesis given the data will go up but
as you point out it's not connected to
the hypothesis directly and I'll write
an equation for you in just a moment
that'll kind of make that I think a
little bit clearer okay but you've got
all this right so I think you understand
it so we got Bayes rule and notice what
we've done we've gone from this sort of
general notion of saying we need to find
the best hypothesis they're actually
coming up with an equation that sort of
makes explicit what we mean by that then
what we care about is the probability of
some hypothesis given the data that's
what we mean by best and that that can
be further thought as the probability of
us seeing some labels on some data given
hypothesis times the probability of the
hypothesis even without any data
whatsoever normalized by the probability
of the data so let's play around with
beige rules a little bit and make
certain that we all we all kind of get
it okay
sure okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>