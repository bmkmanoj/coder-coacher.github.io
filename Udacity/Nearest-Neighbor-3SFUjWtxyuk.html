<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Nearest Neighbor | Coder Coacher - Coaching Coders</title><meta content="Nearest Neighbor - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Nearest Neighbor</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3SFUjWtxyuk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">What about those discriminative
methods we were talking about?
As we said before they find a division,
a surface between the classes.
We're going to talk about
a couple of different methods.
The one that I want to talk about
today is nearest neighbors.
And then, we'll do separate lessons for
each of the, the other two.
So the simplest method we're going to
talk about is nearest neighbor
classification.
And it is a discriminative method
because we are using the boundaries
between I wont say classes,
I'll say examples, right.
The boundary is always between
examples of the classes.
But we actually don't have
to do very much training.
So the idea is we have some future
space here labeled as blank.
It should be x1 and x2.
This picture by the way is taken
from a classic book, due to Hart.
It was, the original was due to Hart,
the new version is due to Hart and
Stork.
And it's a good way of learning sort
of classical pattern recognition.
So the idea is I've got
a bunch of examples.
The negatives, so the not a's
are these little black points, here.
So the, the, the black thing.
And then I've got some
positive examples, right?
And those are the little red ones,
there.
And when I come up with a new point,
all right?
So there it is in the green.
Basically, that's my
new novel test example.
And how am I going to label that?
I'm going to find the closest
training example, so in this
case the closest training example
is that one, and so I say, ah-ha.
The closest one is a positive, so I'm
going to label this new one a positive.
All right, now drawn on this picture
actually are the divisions themselves,
and the computer scientists
among you will recognize that as
a Voronoi partitioning.
Right, when you partition a space
using a, a Voronoi method,
you essentially carve the space
up into these little chunks.
Where this chunk means that
if you're in that chunk,
this black point is
the closest one to you.
And so all nearest neighbor
is doing is giving you
a Voronoi partitioning of the space.
It's shall we say pretty
easy to implement.
It has a couple problems,
one is it doesn't work very well.
We'll get to that in a second.
It's also, very data intensive in terms
of the memory of what you need to know.
Right, so if I give you
a million trainee examples,
how many of them do you
have to remember, Megan?
&amp;gt;&amp;gt; A million.
&amp;gt;&amp;gt; A million, that's exactly right.
Furthermore, every time a new point
comes in I gotta find the closest one.
So if I was really dumb,
you know I don't have a compute, Masters
of Computer Science Degree from Georgia,
I might list go through
all of them one at a time.
If I'm a little smarter I'll use
something called a KD tree or
some other hierarchical
representation but
you're still searching through
a lot of these things.
So at test time,
it's very painful as well so
not only is it a lot of memory stored,
etc.
But let's get back to that
doesn't work all that well thing.
Well one of the things that might
happen is that I might have, you know,
an occasional kind of spurious point.
Or I might be in an area where I really
don't have too many points nearby.
And what I'd like to do is be able
to make a more robust decision.
Okay, and the way of doing
that is referred to as k-NN or
K-Nearest Neighbor.
And it's really very simple.
It's basically the idea, if I've got
some new point, and it's written here as
an x, so that point right there,
I don't just find the nearest point.
You gotta think like
a computer scientist.
You find k.
So k might be one, might be three,
might be five, might be seven.
Whatever choice you choose to make,
you would look for k.
So, in k-NN, in 5-NN, for example,
you know, if I have this point x, okay?
What I'm going to do is,
I'm going to look for
the five nearest neighbors, right?
And I'll just, one,
two, three, four, five.
Okay, and in this particular case three
of them are black, two of them are red,
black is negative, red is positive so
I would classify it as what?
Negative.
Okay?
One of the funny things, well,
one of the interesting things about
k-NN is it works really well, okay?
It does have this data intensive
problem, and there are methods that we
now use that are kind of,
they're sort of related to k-NN.
But this idea of getting a loose
consensus is very effective.
We're not actually going to talk about
a thing called random forest later,
but this notion of consensus.
So I don't get the support from just one
place or even one classification method
little classifier, this notion
of consensus is very powerful.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>