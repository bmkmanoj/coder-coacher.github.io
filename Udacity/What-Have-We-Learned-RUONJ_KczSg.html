<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What Have We Learned | Coder Coacher - Coaching Coders</title><meta content="What Have We Learned - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What Have We Learned</b></h2><h5 class="post__date">2015-07-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RUONJ_KczSg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I think that just about covers it as an
introduction to generalization so I
think maybe it's a good time to reflect
on what we've learned I'm willing to do
that especially if it means I don't have
to do another quiz okay well but I yeah
okay now i'm not going to another quiz
but i am going to ask you what we
learned okay i think i can remember some
of that so i think the big thing we
learned is that there is both a need for
and a way to do generalization in
reinforcement learning yeah and the need
part comes from the fact that for a lot
of problems we care about there are
zillions and zillions of states you
could be our generations Carl Sagan yes
zillions and zillions of states and
that's a problem because of course will
visit states that we've never seen
before and the chances of us seeing
every state and infinite number of times
and all that other stuff that the theory
requires is just kind of unreasonable or
at least slow so it'd be nice to
overcome that good okay and message for
you talked about a few methods but I
think at the end of the day it really
boiled down to let's just take all that
stuff we learned in supervised learning
and apply it cool but we did talk about
some specific ones do you remember any
particular supervised learning methods
that we looked at in the context of
generalization in reinforcement learning
sure we looked at linear function
approximation which makes sense because
you know Linear's nice and well behaved
or at least we hoped it was and it does
seem like the simplest most
straightforward thing to do based on
what we know but I think it's worth
mentioning that we were function
approximating something in particular
right so we were deciding to do the
value functions and to try to learn
those that's how we were doing our
journals a shin right that's a good
point we actually looked at different
functions that we might approximate
value functions policies and models and
we focused primarily on value functions
models you know there's interesting
things to say about that but it's not
nearly as well studied or as well
understood the value function case would
just focused on representing like the v
function or the Q function and that
could be done in a number of ways and we
looked at kind of a general gradient
form where we turned the bellman
equation itself into a kind of an error
measure and then we look specifically at
linear value function approximation how
you could if you have a set of features
you can map them to values using a
linear function and how we
I learned that linear function right and
that's pretty much we spent almost all
did we do I don't think we did hardly
anything with policy function
approximation nope and that's okay and
one of the reasons it's okay is because
you're able to talk about lots of
successes that people have had doing
function approximation with value
functions yeah and there are there's
some successes on doing function
approximation with policies as well in
fact a lot of the coolest stuff in
robotics in the reinforcement learning
setting has happened where there's an
actual gradient taken with respect to a
representation of the policy it's not
exactly clear why it seems to be the
more effective way to use feedback in
the robotic setting but but it you know
it's worth noting at least that this is
you know doing policy gradient methods
do seem to work well in the in the
robotic setting that's a good point but
you know doesn't really matter mean it's
not like robotics is the future anything
robotic is totally the future hmm we'll
see we'll have this conversation in 2050
and see what we think in 2015 it is 2015
no 2050 Oh 2050 Oh Hugh I'm like oh my
gosh it's the future and I wasn't even
paying attention well that's okay I for
one welcome our new robotic overlords
okay in the meantime we had all these
successes and you named a few right now
the one I remember the most was a TD
gammon but there were others which if I
went back and looked at what we recorded
i'm sure i could find out with work or
maybe you'll just remember well i
mentioned several but atari is is one
that actually came up relatively
recently and and hopefully will actually
change the way that people think about
this whole topic of generalization but
bull see and it's still too new to know
oh very good prints ok so now those
successes of course were followed by the
downer of problems and sort of problem
cases where things that seem like they
would obviously work do not obviously
and easily work even in the case of
linear function approximation yes that's
exactly right the good news is in the
last few years there's a there's a new
set of methods that are based on TD and
a gradient but it's a different kind of
gradient that actually uses the function
approximator as part of the error metric
that's being resolved
and that these can actually these
provably converge for the case of linear
in particular the paper that talks about
GTD to actually shows what happens when
you apply this particular problem case
the Baird counter example that we looked
at and you can actually correctly learn
it if you use this other update rule
instead of the kind of the classic
bellman residual minimization kind of
thing</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>