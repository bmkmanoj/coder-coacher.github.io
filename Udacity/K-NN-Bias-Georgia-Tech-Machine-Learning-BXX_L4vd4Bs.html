<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>K NN Bias - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="K NN Bias - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>K NN Bias - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BXX_L4vd4Bs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Michael sir I'm gonna talk a little
bit about bias in a particular the
preference bias for Canyon so let me
remind you what preference bias is
preference bias is kind of our notion of
why we would have prefer one hypothesis
over another I say all things other
things being equal and what that really
means is it's the thing that encompasses
our belief about what makes a good
hypothesis so in some of the previous
examples that we've used it was things
like shorter trees smoother functions
simpler functions Occam's razor those
sorts of things were the ways that we
expressed our preferences over various
hypotheses and canon is no exception
it also has preference bias built in as
does every algorithm of any node so I
just wanted to go through three that I
thought of as being indicative of this
pies and they're kind of all related to
one another so the first one is a notion
of locality right there's this idea that
near points are similar to one another
that make sense you yeah yeah that was
really important and came out nicely in
the the real estate example right so
that the whole idea the whole thing that
we're using to generalize from one thing
to another is this notion of nearness
right and exactly how that notion of
nearness works out is embedded in
whatever distance function we happen to
be given and so there's further bias
that might come out based upon exactly
the way we implement distances so in the
example we just did Euclidean distance
is making a different assumption about
what nearness or similarity is compared
to Manhattan distance affordance is
there like a perfect distance function
for a given problem there's certainly a
perfect distance function for any
particular problem yeah that's what I
mean not one that works for the universe
but one like you know if I give you a
problem and you can work on it all day
long can you find is there is there a
notion that there's a distance function
that would capture things perfectly well
it has to be the case for any given
fixed problem that there is some
distance function that minimizes say sum
of squared errors or something like that
versus some other distance function
right okay that has to be the case so
there there always has to be at least
one
best distance function given everything
else is fixed that makes sense right now
what that is who knows maybe you finding
it might be arbitrarily difficult
because there's at least an infant
there's at least an infinite number of
distance well yeah I mean I was thinking
that if we're allowed to define distance
functions to be anything we want what
about a distance function that said the
distance between all the things that
have the same answer is zero mm-hm and
the distance between them and the ones
that have different answers is you know
infinity or something big yeah and then
then the distance function like somehow
already has built in the solution to the
problem
because it's already put the things that
have the same answers together right you
could do that and of course doing that
would require again solving the original
problem but yeah so so such a function
has to exist or well you know there's
always noise but there's noise in your
data but some such function like that
has to exist the question is finding it
but I think the real point to take there
is there are some good distance
functions for a problem and there's some
bad distance functions from a problem
and how you pick those is really a
fundamental assumption you're making
about the domain that's why it's domain
knowledge yeah okay so locality however
it's expressed to the distance function
that is similarity is built into K n n
that we believe that near points are
similar kind of by definition that leads
actually to the second preference bias
which is this notion of smoothness right
that we are by choosing to average and
by choosing to look at points that are
similar one another we are expecting
functions to behave smoothly all right
so you know in the 2d case it's it's
kind of easy to see right you you you
you have these these sort of points and
you're basically saying look these two
points should somehow be related to one
another more than this point and this
point and that sort of assumes kind of
smoothly changing behavior as you move
from one neighborhood to another I mean
it seems like we're defining to be
pretty similar to locality in this case
so I'm drawing an example such that you
know whatever we meant by locality has
already been kind of expressed in the
graph okay and you know by picking you
know this is really for pedagogical
reasons you can imagine there
you know these are points that live in
77,000 dimensions and it's impossible to
actually visualize them much let's draw
them I'm a cat right and here's here's
three dimensions and here's the fourth
dimension and I think I'm gonna get
tired before it's 77,000 but you know
you kind of you kind of get the idea
right there if you know if you can
imagine in your head points that are
really near one another in some space
you kind of hope that they behave
similarly right right okay so
low-calorie and smoothness and I think
these makes sense out of these it's
hardly the only algorithm that makes
these kind of assumptions but there is
another assumption here which is a bit
more subtle I think which is worth
spending a second talking about which is
for at least the distance functions
we've looked at before the Euclidean
distance in the Manhattan distance they
all kind of looked at each of the
dimensions sort of and subtracted them
and squared them or didn't or took their
absolute value and added them all
together but what that means is we were
treating at least in those cases that
all the features mattered and not only
do they matter they mattered equally
right so think about the the last quiz I
gave you right I said Y equaled x1
squared plus x2 and you notice we got
answers that were wildly off from what
the actual answer was well if I know
that the first dimension the first
feature is going to be squared and the
second one is not going to be squared do
you think either one of these features
is more important or more important to
get right hmm okay trying to think about
what that might mean
so if yeah it's definitely the case that
you when you when you look for similar
examples in the database you want to
care more about x1 because a little bit
of a difference in x1 gets squared out
right it can be lead to a very large
difference in the corresponding Y value
whereas in the X twos it's not quite as
crucial the little if you're off a
little bit more than you're off a little
bit more it's just a linear relationship
so yeah it does seem like that first
dimension and needs to be a lot more
important I guess when you're doing your
matching in the second one right so in
fact we probably would have gotten
different I'm not going to go through
this but we probably would have gotten
different answers if in the Euclidean or
the Manhattan case we had instead of
just taking the difference between the
first two the first dimensions we had
taken that difference and squared it
and then in the case Euclidean distance
squared it again and then some of those
things that were closer in the first
dimension and said the second dimension
would have looked more similar than we
might have gotten better answer so it's
probably a good exercise to go back and
do for someone else yeah I was thinking
of doing it right now but yeah probably
should leave it for other people well
you can do it if you want to so did you
do it Michael I did am so this is kind
of now a mix between the Manhattan
distance and the Euclidean distance so
I'm taking the first component take the
difference square it think the second
component take the difference absolute
value it and add those two things
together sure all right so if I do that
with one nearest neighbor I still get
that tie but the output answer ends up
being 12 hmm which was better than 24
point seven and that's better than eight
which is what it was before so the eight
has gone up to 12 and the other one
which was I think thirty five point five
comes down to twenty nine point five
closer again to the correct answer which
is 18 so in both cases it kind of pushed
in the right direction it was using more
of the answers that were relevant and
fewer the answers that were not relevant
right there you go so that notion of
relevance by the way turns out to be
very very important and highlights a
weakness of KN so this brings me to a
kind of theorem or fundamental result of
machine learning that is particularly
relevant to KN but is actually relevant
everywhere you think it's worthwhile to
mention it sure it sounds very relevant
all right let's do it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>