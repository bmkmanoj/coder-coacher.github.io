<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Principal Components Analysis Two - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Principal Components Analysis Two - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Principal Components Analysis Two - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_nZUhV-qhZA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this is called the first component or
the principal component and this is
called the second or second principal
component of this space okay does that
make sense yep now here's what's
interesting about principal components
analysis you might ask me exactly how
you do this
there are several several mechanisms for
doing it for those of you who have dealt
with linear algebra for something like
this singular value decomposition might
be familiar to you it's one way of
finding the principal components but
principal components analysis basically
has a lot of really neat properties so
let me just describe some of those
properties to you the first property is
well the two that I've written here it
finds directions that maximize variance
and it finds directions that are
mutually orthogonal mutually orthogonal
means it's a global algorithm and by
global here I mean that all of the
directions all of the new features that
they find have a big global constraint
namely that they must be mutually
orthogonal it also turns out that you
can prove which I will try to give you a
little bit of a evidence for but I'm not
going to prove formally that the pca
actually gives you the best
reconstruction now what do I mean by
best reconstruction what I mean is if
you think of each of these directions
that it's found in this case it found
this one first and found this one second
the first thing you see is that if I
return these two dimensions I have
actually lost no information that is
this is just a linear rotation of the
original dimensions so if I were to give
you back these two different features
you could reconstruct all of your
original data you see that weight when
you say feature you mean what we're
going to give it is for each of those
little black data points we're going to
say how far along the red axis is it and
how far along the orange axis is it
right it really is just a kind of a
relabeling of the of the dimensions
right so just like when I think about
these points in X and Y space the
original feature space whenever I give
you a value here for this feature I'm
just describing how far along a black
dot is on this dimension or this
projection and the second dimension or
the second feature just tells me how far
along a dot is on this particular
dimension or axis and similarly by
projecting on to red and on to orange
I'm telling you how far along a point is
along this axis and along that
so if I were returned if I were to take
X&amp;amp;Y and transform them into this new one
and two I would have given you different
values than I get from X or Y but
they're actually just the same point and
so I've thrown away no information so
that's a pretty good reconstruction
that's a pretty good reconstruction but
what principal components analysis does
for you is if I take just one of these
dimensions in particular the first one
the principal component I am guaranteed
that if I project only into this space
and then try to reproach actin to the
original space I will minimize what's
called l2 error so they understand that
I'm not sure let me let me think so
you're saying if we instead of all right
we take little black dots we they now
have an orange a red dimension in our
dimension one two now if we reconstruct
using only the first dimension I guess
it just puts the black dots on the red
line yep and so there is no they have no
existence in that second dimension
correct and now you're saying of all the
different ways that I could do that kind
of project it to a linear sequence this
is the one that's going to have the
smallest l2 error which if I'm not
mistaken is the same kind of
reconstruction error we talked about in
all the other times we talked about
reconstruction so it's like squared
error that's exactly right it's squared
error this particular notion is called
the for being snore but that's really
just talking about distance what this
means is that if I project onto this
single axis here and then I compare it
to where it was in the original space
the distance the sum of all the
distances between those points will
actually be the minimal that I could get
for any other projection well and you
can sort you can sort of prove this it
kind of makes sense if you just think
about the fact that points always start
out in some orthogonal space and I'm
basically finding a scaling and a
rotation such that I don't lose any
information and I maximize variance
along the way by maximizing variance it
turns out I'm maximizing or maintaining
distances as best I can in any given
dimension and so that gives me the best
reconstruction that I can imagine now
you might ask yourself is there anything
else nice about principal components
analysis given this reconstruction error
and there's another proper
PC a that is very very useful and it
boils down to the fact that it's an
eigen problem what happens when you do
principal components analysis is you get
all of these axes back and in fact if
you start out with in dimensions
you get back in dimensions again and the
job here for future transformations you
might recall is you want to pick a
subset M of them hopefully much smaller
than n well it turns out that associated
with each one of these new dimensions
that we get is its eigenvalue that
eigenvalue is guaranteed to be
non-negative it has a lot of other neat
properties but what matters to us here
is that the eigenvalues monotonically
non-increasing
move from the principal to the second
principal to the third to the fourth to
the fifth to the sixth and so on to the
nth dimension and so you can throw away
the ones with the least I convey oh and
that's a way of saying that you're
throwing the projections or the
directions or the features with the
least amount of variance</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>