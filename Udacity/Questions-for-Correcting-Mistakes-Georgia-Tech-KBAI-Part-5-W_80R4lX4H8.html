<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Questions for Correcting Mistakes - Georgia Tech - KBAI: Part 5 | Coder Coacher - Coaching Coders</title><meta content="Questions for Correcting Mistakes - Georgia Tech - KBAI: Part 5 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Questions for Correcting Mistakes - Georgia Tech - KBAI: Part 5</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/W_80R4lX4H8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so the question then becomes how might
an AI agent learn from his mistakes
learning by correcting mistakes are
learning from failures really entails
answering three separate questions the
first question is how can the agent
isolate the error in his former model so
the agent had some model of the world it
made a mistake based on its model how
can it identify the error in this model
note that this particular problem is
very closely connected the question of
diagnosis that we discussed earlier the
second question is how can the agent
explain to itself the default it has
identified the error in fact led it to
the problem to the failure having
identified default and explain how the
fault led to the failure the third
question is how can the agent repair
default in order to prevent a error the
failure from reoccurring you may have
noticed that earlier we had related
learning by correcting mistakes with
metacognition you can see how that
relationship occurs the agent has some
knowledge of the world that knowledge
leads to failure the agent is using that
failure to repair its own knowledge it
is as if the agent is looking into
itself looking into its own reasoning
into its own knowledge and correcting
itself note once again that the learning
here is incremental we are learning from
one example at a time however instead of
simply learning from an example we also
using explanation based learning we are
trying to explain why a particular fault
led to a particular failure that
explanation connects then with
explanation based learning not just with
the notion of incremental learning let
us see how these three questions occur
in the example of the pail the first
question is how can the agent identify
the fact that this particular pail has a
movable handle not a fixed handle is why
this is not a good example of a cup the
second question is how can the agent
will an explanation that proves why
having a movable handle makes this to
the poor example of a cup why does that
lead to a failure the third question is
how can the Asian changes model up
so that it never again picks a object
with a movable handle as an example of a
cup so when we talked about explanation
based learning
we used another example of this as well
we imagined a desktop assistant that I
can just say hey fetch me that important
file from last Tuesday and it can
construct its own understanding of what
file I might be talking about it has a
notion of what files have been important
in the past and certain criteria of
those files so it constructs an
understanding of what important is and
it tries to transfer that onto files
from last Tuesday now imagine that I
told this agent hey fetch me that
important file from last Tuesday and it
returns me a file that actually wasn't
important and I say hey that file isn't
actually important at all the agent
would first try to isolate what error it
made in diagnosing that particular
document as important it might for
example notice that every other document
I've ever labeled as important was very
recent whereas this one was very old so
even though it met all the criteria for
an important document there might be
more criteria that didn't consider yet
and one of those might be that only new
documents are very important it would
then explain that the problem came from
the assumption that an old document
could be important and it would then
repair its model to say that in the
future old documents can't be important
even if they meet the other criteria for
importance this problem of identifying
the error in one's knowledge that led to
a failure is called credit assignment
blame assignment might be a better term
a failure has occurred what fault a gap
and once knowledge was responsible for
the failure that's blame assignment in
this lesson we'll be focusing on gaps or
errors and once knowledge in general the
error could be in one's reasoning or in
one's architecture credit assignment
applies to all of those different kind
of errors several EA theorists Marvin
Minsky for example consider credit
assignment to be the central problem in
learning this is because air agents live
in dynamic worlds therefore we will
never be able to create an air agent
which is perfect even if we were to
create an air agent which had complete
knowledge and perfect reasoning related
with some world the world around it will
change over time as it changes the agent
will start feeling once it starts
failing it must have the ability of
correcting itself so correcting listen
or knowledge correcting its own
reasoning correcting its own
architecture you can see again how this
is related a metacognition the agent is
not diagnosing some electrical circuit
or up
car or software program outside instead
it is self diagnosing self repairing</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>