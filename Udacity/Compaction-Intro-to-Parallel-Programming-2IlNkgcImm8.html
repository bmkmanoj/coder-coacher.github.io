<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Compaction - Intro to Parallel Programming | Coder Coacher - Coaching Coders</title><meta content="Compaction - Intro to Parallel Programming - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Compaction - Intro to Parallel Programming</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2IlNkgcImm8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and unit 5 remember that we learned that
the GPU Hardware processes groups of
threads called warps and that every
thread in a warp performs the same
instruction at the same time and this
means that if some threads are not
actively doing computation there will
they're still going to have to wait
while the active threads in the warp do
their thing now today's GPUs in fact all
GPUs that NVIDIA has ever made have 32
threads per warp in this first case only
four out of every 32 threads are
actually doing something and the rest of
the threads are just along for the bride
in a compacted case if we can compact
this down to a dense array where all
four of these elements are next to each
other and then there's the remaining
elements from elsewhere in the array all
kind of compacted in into one dense
array in that case all 32 threads in the
warp are going to be doing active work
and so you'll finish the total work
eight times faster so that's our answer
we're gonna go eight times faster if
every eighth element is active in the
original array and we compact it and
operate instead on the compacted array
and by the same reasoning if only one of
these threads in a warp operating on the
original array has anything useful to do
then you'll go 32 times faster which is
a big deal you can see why compaction
can lead to big speed ups but the other
subtlety to remember about warps is that
if all the threads in the warp take the
same branch so in the code snippet I
showed before if all 32 threads in the
warp check to see if they were active
decided they weren't and then exited
then you pay almost no penalty for that
and so that that warp is going to fire
up all threads are going to see they
have nothing to do it's going to
disappear and so actually the warps that
are primarily empty are that are
entirely empty will legs it immediately
and so the case where only one in 128
threads is doing something useful
actually still goes about 32 times
faster because there's one warp that's
going to have some work to do and 31 of
the threads in that warp are gonna be
sitting around waiting for the single
thread that has some useful work to do
but all of the threads and the entirely
empty warps that have nothing to do they
exit quite quickly so you'd pay a very
slight penalty for having launched those
warps and having them immediately exit
but unbalance is still going to be close
to 32 times as fast and
to operate on the compacted array then
it would have been to operate on the
original array</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>