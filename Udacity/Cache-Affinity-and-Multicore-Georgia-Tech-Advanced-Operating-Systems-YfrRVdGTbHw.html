<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Cache Affinity and Multicore - Georgia Tech - Advanced Operating Systems | Coder Coacher - Coaching Coders</title><meta content="Cache Affinity and Multicore - Georgia Tech - Advanced Operating Systems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Cache Affinity and Multicore - Georgia Tech - Advanced Operating Systems</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YfrRVdGTbHw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so let's talk about cache affinity and
modern multi-core processors in modern
multi-core processors you have multiple
cores on a single processor and in
addition to the multiple cores that are
on a single processor the processes
themselves are also Hardware
multi-threaded what hardware
multi-threading means is that ever if a
thread that is currently running on a
processor on a core c1 is experiencing a
long latency operation for instance it
misses in the cache and therefore does
you go out in order to fetch the
contents from memory that's a long
latency operation in that case the
hardware may switch to one of the other
threats and run those so in other words
it wants to keep this core busy there's
only one execution engine this core but
it has four threads that it can run on
this core and depending on what these
threads are doing if they're involved in
long latency operation meaning they're
going out they are not switched out of
the processor in terms of operating
system scheduling it is just that these
are the the threads that have been
scheduled to run on this core and the
hardware is switching among these
threads by itself without the
intervention of the operating system it
is automatically switching among these
threads depending on what these threads
are doing if this thread does a memory
access which is going outside the
processor then the hardware is going to
say well you know this guy is going to
be waiting for a while so let me switch
to this guy and let him do it do its
work because it's possible that what he
needs is available in the cache and if
this guy also makes a long latency
operation like a memory access then the
hardware can switch to this guy and to
this guy so if all of these guys are
waiting on memory then of course the
core is not able to going to be able to
do anything useful till at least one of
these memory accesses are complete so
that's the idea behind Hardware
multi-threading so it is not very
unusual for modern multi-core processors
to employ Hardware multi-threading so in
this example I'm showing you there are
four cores and in each core I have four
Hardware threads so it is 4-way Hardware
multi-threaded core and I'm showing you
two levels of caches l1 and l2 cache l1
cache is specific to this
particular core c1 shared by these sleds
similarly l1 cache here is specific to
this course e to shared by the threads
that are on it on the other hand is l2
cache is common for all the course so if
there's a miss in any one of these l1
caches the hope is that they may be able
to find it in the l2 cache if the
processor has only these two levels of
caches l1 cache and l2 cache missing an
l2 cache is really bad news because then
you're going out to out of the chip it's
a long latency memory operation and
modern multi processors may in fact even
employ even more levels of caching in
addition to l1 and l2 they may be in l3
cache it's normal to have modern
processors having at least three levels
of caches on the chip and I'll 1 cache
associated of the core and a shared l2
cache and a shared l3 cache so that's
the structure that you might see in
modern multi processors so what we have
to think about now is thinking about
this cache affinity and the modern
multi-core processors and how the
operating system should make it
scheduling decisions so here again
there's a partnership between the
operating system and the hardware the
hardware is providing these Hardware
threads inside each core and what the
operating system is doing is picking
which threads that it has in its pool of
runnable threads and map them on to the
threads that are available in the
hardware and clearly the scheduling
decision what it tries to do is to make
sure that most of the threads that are
scheduled a particular core may find
their working set in the l1 cache if
possible and similarly the threads that
are scheduled on this may find it's
working set in the l1 cache of c2 if
possible and so on and also the other
thing that the operating system may try
to do is if you just take the universe
of all the threads that are currently
scheduled by the operating system to run
on all these four cores you want to make
sure that the working set of all these
threads are likely to be found in the l2
cache because if you miss an l2 cache
bad news right because then you're going
outside the chip and
a very long latency memory operation and
of course you can extend this idea if if
there is a third level of cache but to
make things concrete let's just stick to
two levels of caches l1 cache and l2
cache and the criterion for operating
system is to make sure that the threads
that are currently scheduled on the
processors that's available all the
cores are available what it wants to try
and do is make sure that all the threads
will find the contents in the l2 cache
because missing in the l2 cache is going
to be a long latency memory operation</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>