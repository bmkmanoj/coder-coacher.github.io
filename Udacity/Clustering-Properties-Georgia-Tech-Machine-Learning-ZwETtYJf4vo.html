<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Clustering Properties - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Clustering Properties - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Clustering Properties - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZwETtYJf4vo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so that's all the algorithms
that we're going to talk about in terms
of unsupervised clustering algorithms
but I would like to kind of pop up a
level and talk a little bit about
different properties that clustering
algorithms might have desirable
properties so the three that I'm going
to talk about are richness scale
invariance and consistency and these are
good things to have right I'd like to be
rich and consistent and Lord knows you'd
like to be scaling variance I would I
wish I were scale invariant I guess it
would be very hard for me to gain weight
if that were true mm-hmm no matter what
the scale is the numbers always the same
all right so if you have a clustering
algorithm what does it do it takes a set
of distances D and maps them to a set of
clusters or partition right or a
partition right and these are three
properties of those kinds of mappings so
richness is the idea that for any
assignment of objects to the clusters
there's some distance matrix that would
cause your clustering algorithm to
produce that clustering for any
clustering that you want to achieve
there is some distance matrix where P of
D your clustering algorithm your
clustering scheme produces that
clustering so that's like saying all
inputs are valid and all outputs are out
all inputs which is to say that the
distance matrices sure those are valid
and anything could be an output any way
of clustering could be an output because
you know think of the alternative the
alternative is there's certain clusters
that you just can't produce and that
seems wrong doesn't it it ought to be
the case that your clustering algorithm
should produce whatever is appropriate
and shouldn't be limited in what it can
express sometimes you'd even want to
algorithm say you know what there's only
one cluster here yeah I mean totally if
I showed you a picture you might look at
it and you'd say I just see one cluster
and it looks like a cloud the second
property that we're talking about is
scale invariance and this is I think
much more straightforward at least in
terms of conceptually so it just means
that if I give you a distance matrix and
I double all the distances or have all
the distances it shouldn't change what
the clustering is that the clustering
algorithm should be invariant to what
the space of the points is assuming that
we keep the relative distances the same
so this is the NASA problem so this says
that if I come up with a bunch of
clusterings because I've been measuring
points in miles if I suddenly start
measuring them in kilometers it
shouldn't change anything that's right
yeah change of units yeah that's a
really good way thinking about it it's
not even that I'm ski
the distances I'm just using inches
instead of feet it should be the case
that it's still the same data all right
and then the last one is maybe the
hardest one to visualize but it's a
really reasonable thing and you would
expect clustering to act this way so
consistency says that if we have some
clustering if you're clustering
algorithm produces some clusters so
let's let's do a little example of that
then shrinking the intra cluster
distances and expanding the inter
cluster distances does not change the
clustering let me show you that alright
and now I've edited this so that within
the clusters the points have gotten
closer together more so in this one than
the other or not changing them at all
but in this particular case I shrunk
this one a lot it shrunk this one a
little and then I moved the clusters a
little bit farther apart
this changes the distances a bit and
we'd like our clustering algorithm to
continue to consider these clusters
right it shouldn't introduce new
clusters because we shrunken things and
it shouldn't want to join these things
together because they've gotten further
apart so that's this notion of
consistency and a little bit more
cumbersome to describe but very natural
thing to think about in the clustering
setting well that makes sense right so
this is where our domain knowledge comes
in so distances are a measure of
similarity right yeah so what
consistency says if you found that a
bunch of things were similar and a bunch
of other things were dissimilar then if
you made the things that were similar
more similar than the things that were
not similar less similar it shouldn't
change your notion of which things are
similar and which things are not
yeah which things are alike which things
want to be grouped together yeah uh-huh
good so you think you understand these
three clustering ideas I believe I do
all right so let's put them into play a
little bit okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>