<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Choosing Actions - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Choosing Actions - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Choosing Actions - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XVXi55PX4sw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so Charles like kind of cheated Oh tell
me more so Q learning isn't really an
algorithm Q learning is actually a
family of algorithms there's lots of
different reinforcement learning
algorithms specific reinforcement
learning algorithms that can be
reasonably called Q learning and they
very typically along these three
dimensions how do we initialize our
estimate Q hat how do we decay our
learning rates alpha sub T and how do we
choose actions during learning and
different ways of making these choices
actually lead to algorithms with fairly
different behaviors in particular when
we use this in the context of an MVP
well let's let me let me ask you so like
what do you think might matter about
let's start with the last one choosing
actions well it seems well there's a
bunch of dumb things you could do right
you could just just pick an option an
action every single time like the same
action every single time independent of
what you've learned that's kind of dumb
but it seems like the obvious smart
thing to do is say look I'm learning I'm
getting better and better so what I'm
gonna do is at this nine next time step
says I have to actually take an action
I'll just pick the action that my Q hat
tells me is the best action to take and
I'm done so all right well let me let me
see if I can capture some of what you
just said there so one way to choose
actions really badly is say pick some
action call it a Sub Zero and no matter
what state you're in no matter what's
happened so far always choose that
action mm-hmm so this can't possibly
work it's going to violate the Q
learning convergence that says that we
have to visit each state action pair
infinitely often and update them to
converge and you know and it makes
perfect sense like if we never try
something like how do you know that you
don't like something if you've never
even tried it like spinach exactly
another idea would be to choose randomly
and this seems kind of good in that we
are going to visit you know all the
states that are visible and we will try
all the actions that are actionable and
we could actually learn Q this way but
as you pointed out this is not a great
idea because we may have learned Q but
we haven't really used it we haven't
really chosen actions using what we've
learned so it's like we're wise but we
are impotent I think it's more like
we're wise but we're stupid
remember wise but which do we know a lot
but we refused to actually do anything
about it all right in particular that in
some sense there's the only difference
between the two is the the theorem right
if i if i'm just choosing randomly so
that what's the problem with always
choosing a subzero well you don't going
to converge but the real problem is you
don't learn anything or you don't take
advantage of anything you learn choosing
randomly is the same thing you basically
never take advantage of anything that
you learn what's the point of learning a
q function if you're always going to
behave randomly you've learned enough or
you've learned but you actually write
you've learned you actually learn the
optimal policy but you're not following
it so you're not actually using what you
know so you can't that you don't doesn't
it doesn't work all right so so then you
had another idea
which was to use our estimate to choose
actions yeah and that seems like a good
idea
in that we will use it is it possible
that it won't learn well it'll learn
something well yeah it might not learn
anything all that good though so for
example what if we do something like
this
so we initialize now we're back up to
this this first point here we initialize
the estimate you had so that for every
state a zero looks awesome and all the
other actions look terrible waiter is
that metric awesome or English awesome
oh I I'm sorry you're right I didn't put
units on that that's in jillion dollars
okay so it's pretty awesome then okay so
if you do that then well let's see what
happen so it's almost like always taking
a zero the only thing that would save
you from taking a zero forever is if as
you take a zero you learn that you
update your Q's and you keep getting
really really bad results really really
bad results in fact worse than terrible
ah yes well let's imagine the terrible
is worse than terrible No so but you're
right yeah you're right that's so so
there's there's at least the case that
if this if this terrible value is
actually lower than the value of always
choosing a zero then it will continue to
choose a zero forever this is this is
called the greedy action selection
strategy mm-hmm and that this is the
problem that runs into it's a kind of
local min oh I see oh okay I see though
so you didn't even have to come up with
this ridiculous example if you
if you want that ridiculous but doing
stream I'm gonna say I'm sorry
ridiculous how well I'm sorry it's a
ridiculous situation to be in
it's a sort of the extremely unlucky
example I think what you're saying is
that you don't like people from Chile oh
no I love people Fred I love Chile
especially with you know just some good
beans and some nice meat but the thing
is that you if you randomly set your cue
hat in such a way that a bad action or
let's say a suboptimal action ends up
looking much better to begin with in the
optimal action you can get in a
situation where you'll keep choosing the
wrong action anyway and so you're only
going to learn things that reinforce
that action which might be good just not
optimal and so you won't actually end up
converging on to the true cue and that's
how you'll get into a local myth that's
right and so in that bad situation and
you admit it is a little contrived
because I've never I've never been to
Chile is that it won't learn it will
actually be exactly the same as this
first case always choose a subzero
mm-hmm so that seems problematic too and
it interacts in an interesting way with
the initialization right so maybe we can
do this idea of using Q hat but we have
to be much more careful about how we
initialize so you know what I want to do
is something like random restarts</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>