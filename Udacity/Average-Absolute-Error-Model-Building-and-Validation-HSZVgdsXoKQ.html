<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Average Absolute Error - Model Building and Validation | Coder Coacher - Coaching Coders</title><meta content="Average Absolute Error - Model Building and Validation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Average Absolute Error - Model Building and Validation</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HSZVgdsXoKQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">now if you recall when we looked at our
histogram of signed error using our gas
of 1450 one and a half seconds we saw
that most of the histogram occurs before
zero this suggests that if we adjust
there a model just a little bit by
perhaps adding an offset we might be
able to improve our accuracy so let's
think about that problem a little bit so
our initial guess of an exponential fit
says that our initial guess for the PDF
of intertweet x has this form 1 over
beta times e to the power of minus T
over beta and then we found the value of
beta which is most likely to have
produced the data that we saw now our
insight that we might be able to add an
offset means that we're suggesting the
model might look something like this
instead and then in addition to finding
a value of beta we should also look for
a value of C that then makes it most
likely to have produced the data that we
saw we can actually generalize this to a
slightly larger class of exponential
models in addition to a constant C we
can also think about another constant a
where a and C are constants so what does
this mean exactly well we're still
trying to solve over the same problem we
have a collection of data points
describing a histogram and we're trying
to find a curve that best fits it so
previously we just used beta and we use
the knob of the beta parameter until we
found a PDF that best fit the curve now
in addition to using the knob of beta
we're going to attempt to use two
additional knobs a and C and the thought
is can we use the three of these
together to get a fit for the PDF which
is more accurate let's find out so we're
going to use precisely the same method
in ipython notebook to perform a fit
specifically we're again going to use
size curve fit in order to find the best
parameters
this time however fit thunk is going to
have a couple of extra parameters so in
addition to just having the constant B
now we're reasoning about the constant a
and the offset C and so fit func lets
you pass to it in addition to the
independent parameter time the parameter
which we had before B but as well as
that
and see other than that it's exactly the
same we're saying to curve fit we've got
this callable function with independent
value T and a bunch of parameters and
we're going to feed it these values that
I want you to fit to now given that
find me the parameters which best fits
those values and give me the covariance
matrix which describes how certain we
are about those values let's go ahead
and do that so here are the parameters
that it came up with for a B and C so
cerf it says the value for a is three
point three four times ten to the
negative one or point to be three four
we have a new B value of two point one
seven two times ten to the negative
three which means in order to get our
beta value we just find the reciprocal
of this and then the offset value C is
quite small three point one eight five
times ten to the negative sixth pretty
much negligible so what does that mean
it means that the adjusted model looks
something like this recall that the
generalized form of the adjusted
exponential model is eight times one
over beta times e to the power of minus
T over beta plus offset C except that
now we have actual values for a beta and
C so let's just plug those up so the
result of our new fit looks like this it
says that our new model is approximately
three point three four times ten to the
negative one times two point one seven
two times ten to the negative three
times e to the power of minus two point
one seven two times ten to the minus
three times T plus an off set that is
nearly zero so recall that once we've
got our model in order to go from our
model to our guess of the number of
seconds to wait we take the expected
value of this so what we want it's
expected value of intertweet time given
that the above is our model and so if we
were to evaluate that that comes to
about one hundred fifty three point
eight three seconds so that's to be
compared against our initial value of
1450 one and a half seconds quite a bit
smaller now let's see how well this new
guess of 150 three point eight three
seconds were
out so first things first let's evaluate
our adjusted histogram so just as before
the blue indicates the original
intertweet times and the yellow is the
initial exponential fit that we did to
that graph we've added the red which is
the generalized exponential fit using
the a and C parameters you can see that
it's a little bit of a tighter fit just
sort of qualitatively now let's form a
histogram of the signed error for our
updated guess of 150 three point eight
three seconds so again here we're
collecting signed differences and
absolute values of differences just now
for our updated guess instead of the
original casts so we see that the mean
of the absolute error has decreased to
1401 seconds from four thousand four
hundred and forty six point eight
seconds as it was before so it's gone
down quite a bit in addition to that
we've gone from being within about one
thousand four hundred and thirty-seven
seconds seventy-five percent of the time
to being within six hundred and seventy
six seconds seventy five percent of the
time of the actual intertweet time so
that's saying that with our adjusted
model were good to within about eleven
minutes
seventy-five percent of the time which
is actually not terrible
given that we still don't really know
anything about the dynamics of the
problem so before we proceed let's just
summarize where we are so we decided to
build a regression estimator to estimate
the number of seconds since the last
tweet we decided to model this
regression estimator using an
exponential fit our initial thick
suggested always guessing one thousand
four hundred fifty one and a half
seconds after forming some confidence
bands on this value of beta and taking a
look at the absolute and signed error we
decided we want to take a step back and
look at a slightly more generalized
version of that model with a greater
number of parameters after fitting the
new generalized model we came to a new
guess of one hundred fifty-three point
eight three seconds and discover that it
performed a bit better than the initial
fit now in the next few videos we're
going to dig deeper into this data and
the dynamics of the problem as we do a
more formal investigation</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>