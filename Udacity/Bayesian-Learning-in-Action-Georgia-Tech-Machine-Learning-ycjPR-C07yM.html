<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bayesian Learning in Action - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Bayesian Learning in Action - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bayesian Learning in Action - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ycjPR-C07yM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Michaels let's see if we can
actually use this as a way of deriving
something maybe that we already knew so
I'm gonna go through a couple of these
because I actually think well frankly I
just think it's kind of cool but I'm
hoping I can convince you it's sort of
cool too and then we get something out
of it okay so let me save the world I'm
gonna set up a problem and it's gonna be
a kind of generic problem and I'm gonna
see what we can get out of it okay so
this is machine learning so we're gonna
be given a bunch of data so there are
three assumptions that I'm gonna make
here the first is that we're gonna be
given a bunch of label training data
which I'm writing here is X sub I and D
sub I so exercise whatever the input
space is and D sub I are these labels
and let's say it doesn't actually even
matter what the labels are but let's say
that the labels are classification
labels okay all right and furthermore
not only we given this data as examples
drawn from some underlying concept C but
they're in fact noise-free okay so there
are true examples that tell you what C
is okay so I'm gonna say in fact let me
write that down because I think it's
important there are noise free examples
okay like di equals C of X I that's
right for all X I so the second
assumption is that the true concept C is
actually in our hypothesis space
whatever that hypothesis space is and
finally we have no reason to believe
that any particular hypothesis in our
hypothesis space is more likely than any
other and so we have a uniform prior
over our hypotheses so it's like the one
thing we know is that we don't know
anything that's right so sometimes
people call this the uninformative prior
because you don't know anything except
of course I've always thought that's a
terrible name because it's a completely
informative prior in fact it's equally
as informative as every other prior and
that it tells you something that all
hypotheses are equally likely but that's
a thought it was called an uninformed
prior is it so it's just an ignorant
priors what you tell me yeah okay well
maybe that's the problem it was always
had a problem with because people keep
calling it uninformative him they really
mean uninformed okay in any case so
these are our these are our assumptions
we've got a bunch of data it's noise
free the concept is actually in the
hypothesis space we care about and we
have a uniform prior so we need to
compute the best hypothesis so given
that we want to somehow compute the
probability of some hypothesis
given the data right that's just Bayes
rule so Michael you've got the problem
right yes so in order to compute the
probability of hypothesis given the data
we just need to figure out all of these
other terms so let me just write down
some of the terms and you could tell me
what you you think the answer is okay
well what was the question the question
is well we're gonna want to compute some
kind of expression for the probability
of hypothesis given the data so given
some particular hypothesis I want to
know what's the probability of that
hypothesis given the data okay
yeah okay and you got the setup so we're
gonna compute that by figuring out these
three terms over here so let's just pick
one of them to do let's try the prior
probability so Michael what's the prior
probability on H did we say that it was
a finite hypothesis class it is a finite
hypothesis class then it's like 1 over
the size of that hypothesis class
because it's uniform exactly right
uniform means exactly that okay so we've
got one of our terms good job let's pick
another term how about the probability
of data given hypothesis what's that so
I guess noise free and we know that it's
noise free so they're always they're all
gonna be zeros and ones mm-hmm so and
it's gonna be a question of whether or
not the data is consistent with that
hypothesis right if the labels all match
right what we'd expect them to be if
that really were the hypothesis then we
get a 1 otherwise we get a 0 that's
exactly right so let me see if I can
write down what I what I think you just
said the probability of the data given
the hypothesis is therefore 1 if it's
the case that the labels and the
hypothesis agree for every single one of
the training examples right yep that
what you said good and if any of them
disagree then the probability is 0 so
that's actually very important it's
important to understand exactly what it
means for to have the probability to
give my pollicis as we mentioned before
that the English version of this is
what's the probability that I would see
data with these labels in a universe
where H is actually true which is
different from saying that H is true or
H is false it's really a comment about
the labels that you see on a data in a
universe where H happens to be true okay
but you know it's occurring to me now
that you wrote that down that we've
talked about this idea before when well
so like there's a shorter way of writing
that which is D of H equals 1 if H is in
the version space of D that's exactly
right that's exactly right so in fact
that will help us to compute the final
term that we need which is the
probability of seeing the data labels so
how would we go about computing that
well it's exactly going to blow down to
the version space as you say let me just
write out a couple of steps so that it's
it's pretty kind of easy to see it's
sometimes easier in these situations to
kind of break things up so the
probability of the data sort of formally
is equal to just this so we can write
the probability data as being basically
a marginalized version of the
probability of the data given each of
the hypotheses times the probability of
the hypotheses now this is only true in
a world where our hypotheses are
mutually exclusive okay so let's assume
that we're in that world because frankly
that's what we always assume and this
little trick is going to work out for us
because we're going to get to take
advantage of two terms that we've
already computed namely the probability
of the data given hypothesis and the
probability prior probability of a
particular hypothesis so we know that
the prior probability of hypothesis is
right it's just 1 over the size of the
hypothesis space and how am I going to
substitute in this equation for the
probability of the data given the
hypothesis so I don't know I would write
that differently I mean it's basically
it's like the indicator function on
whether or not H is in the version space
of D right that's exactly right so in
fact this is not a good way to have
written it let's see if I can come up
with a good notational way of doing it
let's say for every hypothesis that is
in the version space of the hypothesis
space given the labels that we've got ok
how does that count okay so rather than
having to come up with an indicator
function I'm just going to define V s as
the subset of all those hypotheses that
are can
system with the data yeah exactly okay
and so what's the probability of those
one it's one and it's zero otherwise so
then we can simplify this sum and it's
simply what the sum of the 100 the one
over H doesn't even depend on the
hypothesis oh yes I do I do it's one
over the size of the version space no
it's the size of the version space over
the size of the hypothesis space that's
exactly right
basically for every single hypothesis in
the version space we're going to add one
and how many of those are there well the
size of the version space number of
those and multiply all that by one over
the size hypothesis space and so the
probability of the data is that term so
now we can just substitute all of that
into our handy-dandy equation up there
and let's just do that so the probably
the hypothesis given the data is the
probability of the data given hypothesis
which we know is one for all those that
are consistent zero otherwise the
probability of the prior probability
over the hypothesis is just 1 over the
size of the hypothesis space and the
probability of the data is the size of
the version space over the size of the
hypothesis space which when we divide
everything out is simply this got it got
it so what does that all say it says
that given a bunch of data your
probability of a particular hypothesis
being correct or being the best one or
the right one is simply uniform over all
of the hypotheses that are in the
version space that is are consistent
with the data that we see Thanks it's
kind of nice and by the way if it's not
consistent with it then it's zero so
this is only true for hypotheses that
are still in the version space and zero
otherwise now notice that all of this
sort of works out only in a world where
you really do have noise free examples
and you know that the concept is
actually in your hypothesis space and
just as crucially that you have a
uniform prior overall
now this is exactly the algorithm that
we talked about before right we talked
about before what would we do to kind of
decide whether a hypothesis was good
enough in this sort of noise free world
and the answer we came up with is you
should just pick one of them that's in
the version space and what this says is
there's no reason to pick one over the
other
from the version space they're all
equally as good or rather equally as
likely to be correct yeah that follows
yeah so there you go so it turns out you
can actually do something with this
notice by the way that we didn't pick a
particular hypothesis space we didn't
pick a particular form of our instance
space we didn't say actually anything at
all about exactly what the labels were
other than that they were labels of some
sort and the strongest assumption that
we made was a uniform prior so this is
always the right thing to do at least in
a Bayesian sense in a world where we've
got noise free data you have a finite
hypothesis space and you have uniform
priors just pick something from the
consistent set of hypotheses</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>