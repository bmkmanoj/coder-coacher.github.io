<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Not Reward Shaping | Coder Coacher - Coaching Coders</title><meta content="Not Reward Shaping - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Not Reward Shaping</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pIRQRgiNWoU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Michael so I have brought back yet
another slide slightly modified from the
discussion on options and constraints do
you recognize it you remember this well
its got the pac-man thing on it and it
has the four options or whatever that we
just talked about and it has the
beginnings of a stoplight right exactly
right well so in fact I told you a
little about this experiment that we did
where we asked people to come up with
these options but there was a second
part to the experiments there are two
buttons this is me attempting to draw
what a button looks like there's a green
button and there's a red button and what
you're being asked to do is to watch
pacman over here run around executing
actions including these options you came
up with and then occasionally you're
supposed to hit a green or red button
and what do you think that the green in
the red stand for probably green is the
pac-man did something good and the red
is the pac man did something not so good
right that's exactly right so this is
supposed to mean good and this is
supposed to mean bad just like you would
expect we have people do this and they
did it they would say that's good that's
bad that's bad that's bad that's good
that's good that's good that's good
that's good that's good that's good
that's bad and so on and so forth now
here's my question to you given that
people have done all of this and they've
given us lots of data on whether pac-man
did well or did poorly what would be
kind of the obvious thing to do with
that information the obvious thing would
be it seems like the right thing which
is to if you treat the good as a like a
plus one and the bad as a minus one and
have the the agent just maximize reward
that should basically be reinforcement
learning right and that makes a lot of
sense right so since we do reinforcement
learning we'd want to turn this into a
reinforcement learning problem and we
would say well every time you said
something was good I'm going to treat
that as a reward of say one it doesn't
really matter and bad say it's something
like that and you get a reward for say
being in a state in taking a particular
action and that's a perfectly reasonable
thing to do and in fact this ends up
being a way to get human beings to help
us to sort of automatically do reward
shaping and in fact you can show this
works pretty well and there's again
there's been a lot of work on this we
provided some papers on this for the
students to read about and you do better
as you might imagine by treating these
things as reward shaping then if you
didn't because really they're hints as
you sort of move along about where you
ought to be in the kinds of things you
ought to do but remember what I said
before that human beings
actually are trying to tell you
something and they might be trying to
tell you something different from what
you want them to be so if I think back
to what you just said what you kind of
said is a look it's a reinforcement
learning problem reinforcement learning
problems need rewards and so we should
convert these things into rewards well
it turns out that in practice when you
actually talk to humans and kind of work
out what they do they're not actually
doing this when they say good and they
say bad they're not thinking in terms of
actual rewards plus 1 minus 1 or plus 10
and minus 9 and and these kinds of
things because of course they're not
reinforcement learning experts they're
not thinking about rewards they're
actually giving you a different kind of
information so here let me just ask you
if i told you in English that you should
hit the green button whenever the agent
does something right and the red button
when the agent does something wrong then
what kind of information by conveying
other than a reward value well one way
to put it is you're you're giving
commentary on the behavior or the policy
that's exactly right that really in fact
it turns out you could convert these
things into rewards but you could also
convert them into something that's a bit
more direct you can just say well you're
actually telling me that in this
particular state I should in fact take
this action or that in this particular
state I should not take this action and
this is direct policy advice as opposed
to reward advice and so maybe what we're
really doing is not reward shaping or
what humans are doing is not reward
shaping it's actually policy shaping</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>