<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Eigenfaces | Coder Coacher - Coaching Coders</title><meta content="Eigenfaces - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Eigenfaces</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jQOZrXZTXcw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let's apply this to face recognition and
it's referred to as using eigenfaces and
it comes from this paper by matt turk
and alex Pentland sandy Pentland it's
alex pennilyn but his name he goes by
Sandy they were at the MIT Media Lab at
the time and one of the reasons this
paper besides the fact that it works so
well is just known so well in my humble
opinion is because of this term
eigenfaces and face space what a great
what a great phrase in fact I can show
you the eigenfaces I will in just a
minute
all right so here's what they do they
assume that most face images are gonna
lie on some low dimensional subspace in
the big image space determined by I
don't know let's say K eigenvectors okay
k directions of maximum variance we're
Kaazing me way smaller than D so D might
be ten thousand or a million K is gonna
be like 20 or 200 both of those numbers
are way smaller than ten thousand or a
million so what it does is they use PCA
like I just showed you to find the
vectors or what are called the eigen
faces we'll look at the minute you won
through UK that span that subspace okay
so you take all your images you find
your eigenvectors and now what you're
gonna do and this is the really cool
part is you're gonna represent your face
images in that data set as just the
linear combination of those eigenvectors
or another way of saying it is i'm just
gonna have the coefficients of lifts
twenty of the twenty i gain vectors if
it's two hundred two hundred
eigenvectors and i'm gonna represent my
entire image all ten thousand numbers by
just the coefficients of these
eigenvectors and i multiply those
coefficients times AG measure sum them
up that would be my new image all right
so it's a tremendous data reduction so
take you through an example here so and
some of these images come directly from
the old paper some from some newer work
but makes the same point so suppose i
have some training images x1 through XM
so here's a picture of faces now notice
it's just the cropped part of the face
so they're gonna try to do recognition
just on the cropped part of the face so
the first thing that's kind of cool to
look at is this is the mean image means
not the mean taste the mean face the
average face now nobody wants to have
the average face too bad whoever that is
he or she has the average face why did I
say he or she well by definition it's
hard to tell right because it's an
average over everybody there is some
interesting work done on showing how
average faces are peered more beautiful
than real faces this was done a long
time ago is just study of the appearance
of beauty anyway it was interesting
image processing approach to thinking
about could you predict when a face
would be deemed beautiful and you can
imagine here's a question show a picture
to a computer face image part of the
digression and say on the average on a
scale of one to ten how likely are
people to rate this thing as beautiful
that this person is beautiful and could
you do that I think these days using
machine learning might be a different
approach but at the time there was work
done on looking at average faces sure
you wanted to know that anyway oh and so
what do you do with the mean face like
what you should do with the mean face
you throw it away
well what you really do is you subtract
it out of your population isn't that
great
we subtract off mean faces out of the
population no we subtract the mean face
from all the and what we have left is
our distribution and then we start
computing the eigenvectors and here they
are okay so here are some top
eigenvectors and one of the things
you'll notice right away in fact let's
take a look at this first eigenvector
actually the second one is that it's
much brighter on one side than the other
because some of the variation might just
be do the lighting coming from one side
or the other but later you have these
different eigenvectors which look like
these kind of ghostly images but these
are eigenvectors so they look to you
like pictures and they are but what they
are is they're a 10,000 dimensional
vector which is just an image remember I
can go between the image space 10,000
right and the images so I'm just showing
you these eigenvectors as images okay
and if I want to take a dot product of a
real picture and these that's easy all I
do is overlay them and multiply or
multiply every pixel and sum them up
because that's what a dot product is so
when I show this to students sometimes
they don't understand how come
images are eigenvectors and that's
because in image space that's what it
means a vector is an image</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>