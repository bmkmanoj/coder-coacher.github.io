<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Searching - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Searching - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Searching - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kXBHWf7olKY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so forward search simply means you start
with a feature let's see if I can draw a
picture so in forward search you simply
start with a feature of your in features
you look at all of your features in
isolation let's imagine we have five
features and I say well I'm just gonna
pass you the first feature that's the
first feature to the learning algorithm
I get a score then the second feature
then the third feature then the fourth
feature to the fifth feature and
whichever one of those features is best
I keep so let's say it's the first one
over here so now I keep the first
feature and then I say well now that I'm
given the first feature let's look at
each of the remaining features and see
if I add them which one does best so I
end up with 1 2 3 4 5 I picked the best
one and let's say that turns out to be
feature 1 and then I look at feature 1
in combination with 2 in combination
with 3 in combination with 4 and in
combination with 5 in which everyone has
the highest score that's the one I move
forward and so that would be let's say
in this case it worked out to be feature
5 and now that I have these two features
I pick a third feature of the ones that
are remaining 2 3 &amp;amp; 4 and whichever one
of those in combination with 1 &amp;amp; 5 I add
next so let's just say that turns out to
be feature 3 but it turns out the error
or the score that I get for adding the
third feature to 1 in 5 isn't
significantly better than just having
one in five and so I stop here and say
that's the subset of features that I
would think does that make sense yeah
that seems like a really neat idea and I
could see that as being a kind of
efficient polynomial way of searching
this space though couldn't could I think
of it as a kind of hill climbing though
where where my neighborhood relation is
basically adding one more feature sure
absolutely
and if it's only adding one more feature
that's exactly what you're doing you are
doing a kind of kind of hill climbing so
in fact the alternative or the sort of
complement to forward search is backward
search and backward searches I think by
the way you describe it is exactly
another kind of hill climbing where here
we start out with all of the features
and you say well which one can I
eliminate so I try 1 2 3 4 I try 1 2 3 5
I try 1 2 4 5 I try to 3 4 5 and
whichever
for work out best to the ones that I
keep in the second round and let's just
say that turns out to be one two three
and five and then I see one of the
remaining four which one can I eliminate
and still have the best score and I keep
doing that until I get down to a subset
that still does pretty well so let's say
I get to this case and it turns out you
know what one two and five do bet just
almost as well as one two three and five
but once I go to just one in five it
does much worse and so I'm gonna stick
with that so here you're right I like
the way you describe this michaelis hill
climbing here I'm doing hill climbing
except my neighborhood is removing a
single point instead of adding a single
point yeah that's cool that's it it's
for some reason reminding me of like
making a team like you could I can
imagine tryouts where you put everybody
on the field and you have them play and
whoever is helping the least you kick
them off the team so that's kind of the
backward case we just keep kicking
people out until we have the team we
want and then the forward case is kind
of like the dream team model where you
start off saying I got nobody let's take
the best person around add that person
to my team and keep doing that until we
have a full set of players okay so this
is a lot make sense yes okay
so just for just to summarize filtering
algorithms don't necessarily don't
really care about the learner and so you
have to come up with criteria that all
probably make sense like the ones you
came up with information gain entropy
variance and you can tell stories for
why all of these make sense this by the
way is where your domain knowledge comes
in it's the choice of the criterion or
the optimization function that you're
using but they have a problem in that
they don't actually take advantage of
the bias of the learner we're wrapping
definitely takes advantage of the bias
of the learner and then you just simply
have a question of how to deal with the
exponential search problem by doing I
think in all of these cases what looks a
lot like hill climbing and just have
different methods for thinking about how
to do the hill climbing do you add
variables or remove variables or do you
do some kind of random search but
ultimately in both cases you have some
function you're trying to optimize over</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>