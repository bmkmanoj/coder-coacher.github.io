<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Practical Matters - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Practical Matters - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Practical Matters - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oT_8ucSvClI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so here's some practical matters
Michael I mentioned one of them before
and that is that mimic does well with
structure when the optimal values that
you care about depend only on the
structure as opposed to specific values
mimic tends to do pretty well
by contrast randomize hill-climbing
genetic algorithms these other these
other things that we looked at before
can sometimes get confused by two
different values that are both Optima
but look very different from one another
where it's the structure that matters
and not the actual values so the chain
example before where you got alternating
values it's one of those cases where
it's easy for randomized algorithms that
only look for point values to get
confused because they're basically being
drawn in multiple directions at once the
quiz also brought up another point which
is worth mentioning here with with mimic
and with anything that tries to do this
kind of search through probability space
is that it's an issue of representing
everything that is it's not enough just
to be able to represent a probability
distribution of the Optima you really
want to be able to represent everything
in between as you move through
probability space towards your answer
this is the universal symbol for moving
through probability space you don't just
want to represent here at the end and
here at the beginning which is pretty
easy because the uniform distribution
but can you represent this point can you
represent this point and if you can't
are you gonna end up getting stuck and
it actually turns out that mimic can get
stuck in local Optima though it
typically does not in part because you
get randomized restarts for free but the
problem of local Optima is still a
problem of local Optima now when I say
something like you get randomized
restarts for free I'm actually cheating
a little bit and hiding something which
is a little bit more important which is
that what you really get for free is
probability theory so there's a hundred
literally hundreds of years of work on
how to think about representing
probability distributions and what you
can do with them and there are terms
like importance sampling and rejection
sampling and all these kinds of tools
that we have for representing
probability distributions that you can
actually inherit with something like
mimic for dealing with these painful
cases where you might not be able to
represent distributions but the single
most important thing to me
about mimic or what to get out of here
is that representing structure does
matter but you pay a price and that
price basically boils down to time
so the question we might ask ourselves
is what is the sort of practical time
complexity of mimic and it really boils
down to something very simple let me
just share with you a fact Michael okay
I have run this algorithm on many many
many examples and I've compared it to
simulated annealing I've compared it to
genetic algorithms I've compared at the
randomized hill climbing and it works
pretty well for the sorts of examples
I've come up with and here's a little
fact that I just want to give you mimic
tends to run orders of magnitude fewer
iterations and I'm not exaggerating here
I mean that if I run simulated annealing
it might take a hundred thousand
iterations or something like simulated
annealing
but for mimic it might take only a
hundred and this is consistently true so
it turns out that that's not good enough
it turns out that the fact that mimic
can do something in three four five six
seven orders of magnitude fewer
iterations isn't an argument for always
using it can you imagine one huh because
it might give a worse answer well in
practice it doesn't so these are cases
where both simulated annealing and mimic
or randomized hill climbing or genetic
algorithms actually eventually do find
the answer mimic just finds it in orders
of magnitude fewer iterations but you're
counting iterations you didn't control
for the fact that different algorithms
can take different times for a single
iteration that's exactly right so what
do you think if I were to compare
simulated annealing to mimic which do
you think takes the takes more time for
any given iteration
well simulated annealing just does this
little tiny step right it like computes
a bunch of neighbors and then does a
probability comparison and then takes a
step mimic is drawing this sample
estimating a bunch of parameters then
yeah I guess the other way around it's
drawing from a distribution it's
computing which things are say above the
median performance and then it's reading
a new distribution and then that's the
end of the iteration depending on how
many samples it takes to do that it's
it's it could take a very long time and
in particular it's going to always be a
lot more samples than what simulated
annealing is doing almost certainly so
when would you think that mimic would
still be worth using in a case like that
where we know that we can get to the
answer
but simulated annealing will take orders
of magnitude more iterations mimic will
take fewer iterations so when would it
still be worth it to take the one with
fewer iterations
even though east each iteration is
expensive prims algorithm quadratic does
that in the other oh yeah about that
part</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>