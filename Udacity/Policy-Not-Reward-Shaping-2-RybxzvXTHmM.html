<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Policy Not Reward Shaping - 2 | Coder Coacher - Coaching Coders</title><meta content="Policy Not Reward Shaping - 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Policy Not Reward Shaping - 2</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RybxzvXTHmM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">policy shaping policy shaping did you
make up that word because I'm not sure I
understand what that word means ah so I
did not make up the word policy and I
did not make up the word shaping no I
know that no I'm sorry that the
combination of those two words oh well
I'm not going to say I made it up
although maybe I did one of the the
papers that were going to read is
actually a paper that I was a part of
the author team on and it's all about
introducing the notion of policy shaping
and it's really a very simple idea it
sort of what I just described that what
human beings are actually doing is
giving you direct policy advice and so
they're trying to shape what the policy
should be directly as opposed to trying
to say something about what rewards
ought to be does that make sense yeah I
think so I mean I guess now I'm before I
was thinking about shaping differently
but now it's sort of feeling like
sculpting to me like you have a policy
out there and you're going to apply
pressure and and and smoothing in
different places too to have it be a
little bit different right so think
about it this way there's actually I
think a direct way of doing this when
you do policy shaping what have you
actually done what you've actually done
is you've taken whatever the reward
function was and say some particular
state for some particular action and
let's say it was zero and what you said
is no this is the wrong answer the wrong
value it should not be 0 here it should
actually be something else it should be
oh I don't know three so you've actually
replaced the original reward function
are with another reward function R Prime
and said this is actually the reward
function that you should be learning
over and that's really what reward
shaping does for you right I mean if you
think back to the examples that you were
giving with all the reward shaving you
were replacing reward values agreed yea
though in the in the case where we were
doing potential base shaping we also had
to make sure that whenever we sprinkled
some positive reward in there was
negative reward someplace else to to
balance it out so you didn't get new
positive reward cycles happening right
now that's true but regardless of where
you sprinkle them and what kind of
constraints you put up you still took
what was originally a reward value of
something like zero and replaced it with
another reward value I great ass 3-3
seven does matter so you change the
reward function so when exactly the same
way policy shaping is about changing the
policy there's some policy that the
agent currently has and you're saying
actually
this state rather than whatever it was
you think you should do you should do in
fact this so maybe I'm confirming what
you already had maybe I'm telling you to
do something different but regardless
I'm overriding your policy function in
the same way that in reward shaping
you're overriding the reward function
okay that makes sense right so this is
what polishes shaping is all about and
it happens to line up with it turns out
in practice the way people are actually
trying to convey information and so then
the real question is what do you do with
this how do I take this information
that's being given if it's not being
given in the form of rewards we know how
to do that but it's actually being given
in the form of a policy what should I
actually do about so I was interpreting
what you described as saying if a person
gives positive feedback for an action in
a state and it's not the action that the
agent would have taken in that state
then just replace it just just make a
modification of the policy so that
that's the action that's taken cuz is
that the way to do it well ideally that
is what you would do right if a person
tells you look you should go up here or
you should eat a ghost here you should
just do it but that actually requires
something be true which as the lawyers
would say assumes facts not in evidence
and that is that the person's actually
right what if the person is wrong yeah
but certainly we should at least have
that as our assumption I mean if we
don't think the person's right then this
is a really easy thing to do policy
shaping just ignore the person but what
if the person's right most of the time
but can occasionally be wrong I see or
what if the person's right but actually
meant to say at the time step before I
see so you so maybe there's a way to use
all these different things as as
evidence and then to combine it together
to get I don't know kind of a best guess
as to what the right answer is exactly
and that's why its policy shaping and
not policy doing so I'm going to try to
give you a little quiz and see if we can
kind of work out what the right thing to
do is under the circumstances where a
person might tell you to do something it
might not always be right about it seem
fair yeah</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>