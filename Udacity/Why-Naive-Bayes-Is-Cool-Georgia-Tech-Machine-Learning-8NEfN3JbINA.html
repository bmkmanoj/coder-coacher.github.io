<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Why Naive Bayes Is Cool - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Why Naive Bayes Is Cool - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Why Naive Bayes Is Cool - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8NEfN3JbINA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so the same idea of naivebayes where you
have a network that has a label
producing or conditionally producing a
bunch of attribute values is just a
really cool and powerful idea so one of
the one of the issues is that even
though inference in general is is is a
very difficult problem it's NP hard to
work out what these probabilities are
when you have a naive Bayes structure
it's cheap it's it's the formula that we
had on the previous slide the number of
parameters that you need to write down
again even if you have a very large
number of variables it's not exponential
in the number of variables it's just
linear there's two probabilities for
each of the attributes and one
probability for the class we can
actually estimate these probabilities so
so far we've only been talking about
Bayes nets in not in a learning setting
but in a setting where we just write
down what all the numbers are we can
actually very easily estimate these
parameters how would we do that well
that way to do it is you count when
you're trying to estimate the
probability of a particular attribute
value given a class it's really just in
your in your label data how often do you
have an example that has that attribute
value in that class and then divide by
the number of times that you have that
class at all and that gives you the
conditional probability so this is you
know and in the case of infinite data
this is actually going to give you
exactly the right number it also
connects this notion of inference that
we've been talking about with
classification which is mostly what this
this mini-course has been about so
that's really great to have a connection
it actually allows us to do all kinds of
interesting things like instead of only
generating what the labels are we can
actually generate what attributes are we
can do inference on in any of these
directions and it turns out it's wildly
successful empirically so my
understanding is that Google uses a
tremendous amount of naive Bayes
classification in what they do if you
have enough data you can estimate these
values really well and naive Bayes is
just remarkably good so yeah so it's
like unclear why we'd even have any
other algorithms right Charles well
there's no free lunch but I gotta say I
you know there's as as a famous man once
said it works in practice but does it
work in theory and I'm trying to figure
out
how this can possibly work so I noticed
it's called naivebayes
and I think I know why now all right
one is that it's well it's naive and in
fact painfully ridiculous to believe
that the Bayesian net that you wrote up
there in the upper right hand corner
represents the word world most of the
time mmm
I see and why is that well because with
that what the network says is that all
of the attributes are conditionally
independent given that you know the
label that just can't be true we talked
about this before where we're using
Bayesian inference to derive some
squared errors that it makes a very
strong assumption about where your
errors come from and an even stronger
assumption about where your errors don't
come from so you're not modeling any of
the interrelationships between the
between the different attributes and
that just doesn't seem right so one
question I have I have to I will save
the second one though one question I
have is how in the world can it possibly
be the case that this works in practice
hmm that's a good question
it does moving on no that's not
satisfying now well how about how about
I give it a guess okay now that I've
yelled at you when I get one I give it a
guess so I think it comes back to the
conversation we had in the previous
slide when I was saying well we don't
have to count we don't care about
probabilities and you said we do care
about probabilities because it was the
question you asked him and that was fair
but once we were down to classification
the probabilities really don't matter
right all that matters is that you get
the right answer so it's okay I guess if
the probabilities you get are wrong so
long as they're sort of in the right
direction right that you end up getting
the the right label as a result yeah
that's a good point that in fact even
though we're introducing this idea in
the context of Bayesian inference it
might actually not be so good at that
even if it is particularly good at
classification well actually I think I
have a good example so so here here
write this down so let's imagine there
are four so actually you can use the
network that you have up there okay good
so let's say that the first attribute
I'm
is gonna call it a and the second
attribute I'm gonna call B and let's say
we're really we're lucky and our naive
assumption is right and they really are
conditionally independent but let's say
the third attribute is actually just
another way of writing down a and the
fourth attribute is just another way of
writing down B so clearly there are
interrelationships between the
attributes right the third attribute is
the first one the fourth attribute is
the second one there's no way around
that and so you think naive Bayes would
fail but actually looking at your your
equation right below there where you're
doing counting I actually think it'll
work just fine
why because all you're really doing is
double counting the sort of weight of
attribute a but you're also double
counting the weight of attribute B and
they'll cancel each other out and you'll
get the right answer when you do the arc
max you get bad probabilities the
probabilities end up being kind of
squared of what they should are supposed
to be but that's okay cuz the ordering
is preserved right exactly and so even
if you were unlucky and the fourth
attribute wasn't B but it was something
else C it doesn't matter if you're
double counting a so long as it still
gives you the right label and you could
imagine that if you have weak
interrelationships or you know you have
enough attributes and and so on that you
would still get the right you know yes
this is the correct label even if you
got the probabilities wildly wrong okay
so I'm willing to believe that that
could happen in practice okay so in fact
my guess is that naive Bayes believes
it's answer too much but it doesn't
matter if it happens to be right all
right and did you have other issues with
it so the second problem I have actually
boils down to that equation you wrote
there so it's really nice and neat that
you can compute the probabilities of
seeing an attribute given a value by
just doing counting but I don't have an
infinite amount of data right not on a
bad day no no we're even on a good day I
usually have an infinite line of data so
what if I'm unlucky enough that for some
particular attribute value I have never
seen it paired with that label V right
so then that means this numerator will
be zero right
so well that numerator 0 but since the
computation involves a product by just
having one attribute value that I've
never seen before
I'm gonna end up saying well the the
probability of that entire product of
seeing that value given a set of
attributes is also going to be 0 so one
unseen attribute basically says it
doesn't matter what else is going on
which seems a little weird right you you
think that you if all the other
attributes are screaming yes yes yes yes
yes it should be positive then just
because you haven't happened to have
seen any examples of some other one
single attribute that shouldn't be
enough to do veto good point so in fact
that's not what people often do people
will often what they call smooth the
probabilities by essentially
initializing the count so that nothing
is 0 everything has a tiny little
nonzero value in it and there's they're
smarter and less smart ways of doing
that but no you're absolutely right that
that is that zeroing out problem is a
real thing and you have to be a little
bit careful hey so if you you have to do
that because if you don't do that then
you're believing your data too much
you're kind of overfitting overfitting
comes up again oh oh ok okay so bear
with me on this Michael so if you're
overfitting by believing the data and
you're fixing it by being smooth I
usually spell it with a V but whatever
if you you fixed about being being
smooth then you're making an assumption
there's a kind of inductive bias right
you're you're saying that I go in with
the assumption that there's sort of all
things or at least mildly possible good
oh yeah that's right that's right
ok naive Bayes is cool you've convinced
me nice</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>