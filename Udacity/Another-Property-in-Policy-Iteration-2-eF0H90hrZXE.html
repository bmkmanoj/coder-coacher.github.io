<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Another Property in Policy Iteration - 2 | Coder Coacher - Coaching Coders</title><meta content="Another Property in Policy Iteration - 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Another Property in Policy Iteration - 2</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eF0H90hrZXE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the really important thing here is that
policy iteration does not get stuck in
the local optimum that is to say if
there's any way to improve it will
actually improve it won't get stuck
before so the important step in the
proof for that is actually back one
slide when we talked about value nandi
provement or what I call value
improvement so I went back a slide so we
can look at this together so so this is
this is the claim that i'm making
charles that that if it's the case that
pi 1 is optimal then the greedy policy
pi 2 with respect to the fixed point of
the pie one is going to be the same
we're gonna we're actually going to be
stuck at the fixed point because that's
the whole thing about being a fixed
point but if it's not if it's not then
there has to be at least one state some
place where this value nandi provement
is actually value improvement like
strict value improvement for some state
like that boom i see so so what's really
going on here is we should be more
precise maybe about what value
improvement really means what value nan
de provement means okay we're talking
about it every time we talk about it we
talk about it in terms of one value
function better than another and so it's
easy to think of it that way but if that
were the case then I could have sort of
cycles right where you know I have two
states where I keep on each iteration
swapping back and forth which one is
better than the other and you'd still
have domination in both cases right does
that make sense what I just said wait no
because domination is point wise its
state wise yes it is state wise but
imagine I had two states it's call them
s1 and s2 and let's say for one
particular value function if I were
going to compare let's say q1 and q2 or
v1 and v2 it's going to compare v1 and
v2 it turns out that in s 1 V 1 is
greater than it is in v2 so just make up
some numbers right and in the other case
they're the same the same yeah okay then
I could apply an operator to both of
them it would also be the case so here
v1 dominates me too right right so
here's another set of numbers i could
write down where the one still dominates
v2 okay so rewrite the s1 and s2 again
underneath all right so v1 still
dominates v2 right so v1 still dominates
v2 if I just swap the the two wrote the
two but rose around and so if you
weren't
thinking carefully about what value
improvement or value nandi provement
meant you would think that you could
just go back and forth between these two
different values where 10 6 and 3 3
those rows sort of swap because in both
cases you're still getting v1 dominating
v2 but it's actually the case that the
way this value improvement works is that
it's true not just for the overall value
functions it's true at every state I see
yes that where that it can't be the case
that we go from v1 have s1 being 10 2 v1
s1 being 3 because it's actually nandi
proving right in other words we can't we
can't go down we can stay the same but
we can't go down so the statement has to
be the case for all the states as well
as the overall set of states so once v1
is better than v2 in some state it
basically has to stay that way got it so
this keeps us from cycling back and
forth and this is what keeps us from
getting and kind of low get sort of
locally stuck that's my claim okay all
right so local stuckness doesn't happen
and in particular we get strict value
improvement in at least one state if we
haven't actually reached the fixed point
yet and therefore each time we go
through policy duration is actually
getting better until it can't get better
anymore so one other interesting thing
to point out is that the number of
policies each time that we that we go
through an iteration of policy iteration
we have a new deterministic policy
there's just a finite number of them
therefore at some point we exhaust the
the set of possible improvements that we
can make and we have to have found the
optimal policy oh that reminds me of e m
00 though okay reminds me of the proof
we had for YK means had to converge oh
yes it's yes it's exactly the same
argument yeah when we talked about
k-means in the other class we talked
about how there's a finite set of
assignments that can be made and each
time you do an iteration of k-means we
get either the same assignment or one
that has a better score and therefore
we're going to find the one with the
best score well we're going to find a
one that doesn't change any more but the
difference between kk means proof and
this is that you could get stuck in the
local optimum in k-means k-means but
here you can and i think
the big difference is that you basically
it statement of value not approve Minh
at every single point every single state
in this case whereas with k-means that
wasn't necessarily the case okay super
so do you feel like you have a your
drokking policy iteration now yeah and
particular I'm rocking that it actually
kind of has to work that's sort of cool
cool I think I think we can sum up then
let's sum up I like something up</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>