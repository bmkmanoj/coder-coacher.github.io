<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Transpose Code Recap - Intro to Parallel Programming | Coder Coacher - Coaching Coders</title><meta content="Transpose Code Recap - Intro to Parallel Programming - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Transpose Code Recap - Intro to Parallel Programming</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RdFrnh5fhKc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so let's quickly recap we had a
serial version of the code that did
everything in a single in a single
thread trivial to write that code zero
parallelism and pretty terrible
performance then we had a parallel per
row version of the code that was also
pretty simple to write and assigned one
thread per row and a single thread block
again did not get great performance
although a huge improvement factor of
about a hundred then the third version
we extracted pretty much all the
parallelism that the problem has to
offer and assigned one thread per
element of the matrix so we did this
transpose operation now in sort of
massive parallel and now we got you know
quite a bit better performance remember
that all of this is happening in the
context of a pod analyze parallelized
optimize and deploy and this is actually
might be fast enough that you're ready
to deploy it right so we started by
looking at analyzing the code and
deciding that this function of
transposing matrix need to be sped up we
started exploring ways to parallelize it
and this might be fast enough this is
already a place where you ought to look
at this and say is this the bottleneck
anymore if speeding this up going to
make a big difference to my application
if not then deploy if so then we can
start thinking about optimizing it and
that was the next thing we did we looked
at the performance here we saw that we
were getting pretty poor dear am
utilization and we diagnosed that the
problem must be that our global stores
we're getting bad coalescing in other
words when we were writing the output
matrix we were getting bad coalescing in
those accesses to global memory so to
fix that we came up with a tiled version
of our code where each thread block is
responsible for reading a tile of the
input matrix transposing it and writing
that transpose matrix to its location in
the output matrix and it can perform
these reads and writes in a coalesced
fashion we did this in blocks of 32 by
32 tiles and got excellent coalescing
this improved our speed a little bit but
we're still not getting great D Ram
utilization so we looked into why so
only thought a little bit more about why
we wouldn't have taken a great
improvement in bandwidth we concluded we
probably had too many threads
eating at barriers so we improved that
by shrinking the tile size experimenting
with different tile sizes led us to
conclude that a 16 by 16 thread block
writing into 16 by 16 tile and shared
memory struck a good balance between
achieving coalesced writes reads and
writes to memory and not spending too
much time waiting at barriers</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>