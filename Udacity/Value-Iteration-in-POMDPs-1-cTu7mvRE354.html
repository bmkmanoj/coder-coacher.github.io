<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Value Iteration in POMDPs - 1 | Coder Coacher - Coaching Coders</title><meta content="Value Iteration in POMDPs - 1 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Value Iteration in POMDPs - 1</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cTu7mvRE354" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so the trick for doing algorithms like
value duration in an infinite state
space like what you get from the belief
MDP is going to come only by looking
really really really carefully at the
infinite case and then showing that we
can re represent it in a finite way so
what we're gonna do now is actually step
through what value iteration would look
like in pom BPS and then piece by piece
we're gonna convert that to something
that actually can be computed so at
first it's going to be math and then
it's going to be an algorithm so here's
value iteration written out and it's in
Palm DPS but certain it doesn't really
matter yet we're gonna define the value
function at step zero to be you know as
a function of the belief to just be zero
everywhere and then we'll say for T
greater than zero the value function for
time step T as a function of the belief
state B is going to be the max over all
actions the reward for taking that
action in the context of that belief
state plus the discounted expected value
of where we end up right so that what
we're going to observe is an observation
Z and so we're gonna let's sum over all
the possible observations the
probability that we make that
observation times the value function at
the previous time step for the resulting
belief state B prime which is the what
we get by doing what we talked about on
the previous slide which we'll call the
state estimation for be a Z all right
yeah I mean other than the fact that R
is not a function of B and a you just
that just means the reward you actually
got right so let's sum what is this what
is this like what is the reward for
being a belief state B given that we
take action a how can we represent that
in terms of quantities that were more
familiar with by unrolling probabilities
or something the same way we did last
time yeah and in particular well all we
have to do is say well well get you know
we would get the reward for state s if
we were in state s so let's just sum
over all states the probability were
actually in that state the reward that
we would get from that state and of
course this is just the belief state for
for state s so this is really just the
dot product of the belief state with
kind of a vector that you could make out
of the rewards for a given action so
it's the it really is just the average
reward you would expect given the belief
state exactly okay and so again you know
now that we've got that out of the way
the scary thing here is this function V
this value function is defined over an
infinite set of belief states the these
vectors of probabilities over States so
again we're not going to let that scare
us quite yet but but it is something we
have to keep in mind otherwise we could
just implement this directly all we
would need is a loop that would say for
all belief states do this update it's
just that there's an infinite number of
them so that would not turn me yeah but
you could write it in really just a few
lines of code yes you could write an
infinite loop in very few lines of code
beautiful really this is how we're gonna
jump from the infinite to the finit to
the finite is we have that we're gonna
make a claim and the claim goes like
this that for all T the value function
of time steps T over all belief states B
wait this is right so this is over over
the infinite set of possibilities states
can be written in a finite way in
particular the maximum over some set of
vectors gamma sub T that we're gonna
have to deal with later but the point is
that it's a finite set of vectors the
maximum of the dot product of the
vectors in that set with the belief
state okay where a dot product is really
just the sum over all the states the
weighted probability being in that state
times this this alpha of s so this this
kind of function is sometimes called
piecewise linear and convex and the
reason is that if we actually if we
think about belief space as being well
it's just a really simple one like the
space of probability distribution is
over just the two to two possible states
so we have some probability between 0
and 1 and if we are probability one
represents we're totally definitely in
state s 1 otherwise if the probability 0
we're totally definitely in state s 0
and in between where in between there's
some probability of s 0 some probably
that's one in halfway in between would
be half of each okay so what what
functions have this kind have in common
is that each of these alpha times B is a
linear function of the belief state all
right so each of the alphas in this set
capital gamma sub T is is some linear
function of the belief state
and what we're gonna do is we have a
whole bunch of them at any given belief
state B what we're doing is we're taking
the maximum value of all these possible
linear functions so at the end of the
day the actual function that we've
represented is this upper surface of
these linear functions each of which is
defined in a set capital gamma is that
does that make some sense sure it makes
me think of something we did in the last
class what was that well when we did the
game theory stuff remember that yeah so
right so maximums / linear functions
comes up in game theory as well yeah
it's a good point and there they end up
looking very piecewise linear yep and
convex right this is sort of open
upwards to the top and that's because of
the max and the piecewise Linear's
because it's a finite set of linear
functions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>