<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>More General Generative Models | Coder Coacher - Coaching Coders</title><meta content="More General Generative Models - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>More General Generative Models</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sPbCvWjstog" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">a couple of more points about generative
models they're pretty straightforward
first of all we've been doing the two
class case right you're either a skin or
not skin well suppose I've got a bunch
of classes C sub I and I've got a
measurement x which see should I choose
it's written here in green tea star so
very simply I'm going to take the
category that maximizes P of C given X
that is which category is most likely
given X okay that makes some sense well
but I don't have that directly so in
order to do that i use the likelihood
times the what's PFC that's our prior
okay so i have to have my prior so
basically in generative models as you
add a new category you just need to
build a likelihood model and you need a
prior model and that'll allow you to
compare one category to the next in
deciding a new label the other thing is
you remember before we were drawing
these histograms to represent those
those probability distributions well
normally what you want is a nice
continuous generative model so what you
need is a likelihood density okay so you
don't just have a histogram you actually
want a density and remember its
likelihood so it's p of x given see not
PFC given x that we get from Bayes rule
all right so typically we represent
those likelihoods using some parametric
form here it's written as a mixture of
gaussians right so here this particular
class to see it says p of x given c2
that's it's like hood this is one little
Gaussian lump and we do that because we
set up that looks you know maybe most of
the distribution of those pixels values
is actually like that but over here for
class one you see that we've got two
humps okay and that's just a mixture of
gaussians and it's pretty it's not too
difficult to estimate a mixture of
gaussians from a modest amount of data
one is so one of the reasons to use
these kinds of parameterize
distributions is it allows you to use
much less data in order to fit your
density so some of you might ask well
wait a minute why am I doing this whole
continuous fitting why not just use some
like
k nearest neighbor or pars on window
method or something else that you
learned in probability class megan is
not going to remember that question so
I'm not going to ask her to write so why
won't you do that well you actually you
might do that if you've got a lot of
data you might actually try to do some
histogram KNN based method of estimating
your density but you would really need
lots and lots of data sort of everywhere
that you're likely to get a point
because you need the densities pretty
much everywhere you're likely to get a
data point and in some sense the whole
point of generative models is to be able
to use a modest amount of data so the
reason to use the parameterised thing is
that you don't need a lot of data ok and
it allows you to generate a model that
you can use for each category</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>