<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>ID3 - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="ID3 - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>ID3 - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IX0iGf2wYM0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so now we have an intuition of best and
how we want to split we've we've looked
over Michael's proposed sort of
high-level algorithm for how we would
build a decision tree and I think we
have enough information now that we can
actually do a real specific algorithm so
let's write that down and the particular
algorithm that Michael proposed is a
kind of generic version of something
that's called id3 so let me write down
what that algorithm is and we can talk
about it okay so here's the ID 3
algorithm you're simply going to keep
looping forever until you've solved the
problem at each step you're going to
pick the best attribute and we're going
to define what we mean by best or a
couple of different ways we might we
might define best in a moment and then
given the best attribute that splits the
data the way that we want it does all
those things that we've talked about
assign that as a decision attribute for
node then for each value that the
attribute a can take on create a
descendant of node sort the training
examples to those leaves based upon
exactly what values they take on and if
you perfectly classified your training
set then you stop otherwise you iterate
over each of those leaves picking the
best attribute and turn for the training
examples that were sorted into that leaf
and you keep doing that building up the
tree until you're done so that's the id3
algorithm and the key bit that we have
to expand upon in this case is exactly
what it means to have a best attribute
all right so what is exactly what
exactly is it that we mean by best
attribute so there are lots of
possibilities that you could come up
with the one that is most common and the
one that I want you to think about the
most is what's called information gain
so information gain is simply a
mathematical way to capture the amount
of information that one gains by picking
a particular attribute funnily enough
but what it really talks about is the
reduction in randomness over the labels
that you have with a set of data based
upon knowing the value of a particular
attribute so the formula is simply this
the information gained over SN a where s
is the collection of training examples
that you're looking at and a is a
particular attribute it's simply defined
as the entropy with respect to the label
the set of training examples you have s-
sort of the expected or average entropy
that you would have over each set of
examples that you have with a particular
value does that make sense to you
Michael so what we're doing we're
picking an attribute and that attribute
could have a bunch of different values
like true or false or short medium tall
right and that's represented by V okay
each of those is a different V and then
we're saying okay for over those leaves
we're going to do this entropy thing
again and we write so what and what is
entropy ah
entropy so we'll talk about entropy
later on in the class in some detail and
define it exactly mathematically and
some of you probably already know what
what entropy is but for those of you
don't it's exactly a measure of
randomness so if I have a coin let's say
a two-headed coin that could be heads or
tails and I don't know anything about
the coin except that it's probably fair
if I were to flip the coin what's the
probability that it would end up heads
vs. tail 1/2 it's 1/2 exactly if it's a
fair coin it's 1/2 which means that I
have no basis going into flipping the
coin to guess either way whether it's
heads or it's tails and so that has a
lot of entropy in fact it has exactly
what's called one bit of entropy on the
other hand let's imagine that I have a
coin that has heads on both sides then
before I even flip the coin I already
know what the outcome is going to be
it's going to come up heads and so
what's the probability be coming up with
heads it's 1 1 so that actually has no
information no randomness no entropy
whatsoever and has zero bits of entropy
so when I look at the set of examples
that I have in the set of labels I have
I can count the number that are coming
up let's say red X's versus the ones
that are coming up green O's and if
those are evenly split then the entropy
of them is maximal because if I were to
close my eyes and reach for an instance
I have no way of knowing beforehand
whether I'm more likely to get an ax
I'm more likely to get an O on the other
hand if I have all the exes and together
then I already know before I even reach
in that I'm going to end up with an X so
as I have more of one label than the
other the amount of entropy goes down
that is I have more information going in
does that make sense Michael I think so
so is there Kim maybe we can say what
the formula is for this or turn them
what is the formula for it you should
remember it's if we have well I'm not
sure what the notation ought to be with
these s's but it has something do it
with log P log wait no sorry P log P
mm-hmm so the actual formula for entropy
using the same notation that we're using
for information gain is simply the sum
over all the possible values you might
see of the probability of you seeing
that value times the log of the
probability of you seeing that value
times minus one and I don't want to get
into the details here we're going to go
into a lot more details about this later
when we get further on in the class with
randomized optimization where interviews
going to matter a lot but for now I just
want you to have the intuition that this
is a measure of information this is the
measure of randomness in some variable
that you haven't seen it's the
likelihood of you already knowing what
you're going to get if you close your
eyes and pick one of the training
examples versus you not knowing what
you're going to get if you close your
eyes and you pick one of the training
examples okay all right so well okay so
then in the practice trees that you had
given us before it was the case that we
were we wanted to prefer splits that I
guess made things less random right so
if things were all mixed together the
Reds and the greens
after the split if it was all Reds on
one side and all greens on the other
then each of those two sides would have
very what I would they'd have low
entropy even though we when we started
out before the split we had high entropy
right that's exactly right so if you if
you remember the the three examples we
had before one of them it was the case
that all of the samples went down the
left side of the tree so the amount of
entropy that we had didn't change at all
so there was no game in using that
attribute in another case we split the
data in half but in each case we had
half of the X's and half of the
together on both sides of the split
which means that the total amount of
entropy actually didn't change at all
even though we split the data and in the
final case the best one we still split
the data in half but since all of the
x's ended up on one side and all of the
O's ended up on the other side we had no
entropy or no randomness left whatsoever
and that gave us the maximum amount of
information game so is that how we're
choosing the best attribute the one with
the maximum gain exactly so the goal is
to maximize over the entropy gain and
that's the biggest attribute</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>