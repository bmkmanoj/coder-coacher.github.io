<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Eigenfaces Example | Coder Coacher - Coaching Coders</title><meta content="Eigenfaces Example - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Eigenfaces Example</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JqEI0-RCC8w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so it's a little bit easier to
understand what these eigenvectors do if
we look at it this way
here are the principal component
eigenvectors one through whatever here's
two different reconstructions alright if
I take the average face and I just add
three times and Sigma K is just the
standard deviation of the coefficient
don't worry about that if I take the
average face and I add in some of that
component I get this kind of thing and
if I subtract off that kind of component
I get that thing notice that that just
moved the live lighting from the left to
the right so one way of thinking about
it is that these components the first
components just changed whether the
direction lighting direction was from
the left to the right
whereas let's see let's take another one
here this component here if I add
positive I don't know it tends to look a
lot more feminine when I go negative it
tends to look a lot more masculine okay
so each of these different eigenvectors
are adding in some other different
variations some other different element
so how do you use these well a couple of
things have to talk about first is I
have to be able to get the coefficients
well that's real easy just like I said
before I take in some image X here it is
and I just subtract off the mean the
mean face and I take the dot product
with in this case the first eigenvector
second third all the way up through the
cave eigenvector those dot products will
give me these coefficients or weight w1
through WK what's cool is that vector of
numbers just K of them maybe it's 20
maybe it's 200 that's my entire
representation of this face so I've
taken this 10,000 dimensional vector
10,000 elements 10,000 pixels and I've
reduced it to 20 numbers so how do we
make use of this vector these w's that
are the coefficients well the first
thing is I can actually use them to
reconstruct the face right each one of
these w's tells me how much of each
eigenvector to add back in and in
general what I can do is I take a
picture and I say this pick
is represented by the mean plus the
double each of the W's times the
eigenvectors that's what this is and if
I were to keep a lot of them I would get
a really great reconstruction if I keep
some of them I get an OK reconstruction
well how many of them do I want to keep
well remember I wasn't doing this to do
reconstruction I was trying to do
recognition</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>