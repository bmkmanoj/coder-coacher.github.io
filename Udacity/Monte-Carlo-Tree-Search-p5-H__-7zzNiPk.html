<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Monte Carlo Tree Search p5 | Coder Coacher - Coaching Coders</title><meta content="Monte Carlo Tree Search p5 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Monte Carlo Tree Search p5</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/H__-7zzNiPk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so here you have goals you're trying to
accomplish sort of positive things
you're trying to do and then you have
constraints basically I want you to eat
little dots eat big dots and eat big
ghosts while honoring the constraint
that I don't want you to be eaten by a
non scared ghost okay yeah I mean now
from a logic standpoint we've covered
everything success and feel things are
defined by success and things that are
defined by failure right and but the
failure here I mean well write it
exactly that way so why did I bring all
this up I brought it up because one I
just think it's kind of neat that people
actually come up with these things but
the other thing that I think is kind of
neat is that it gives us a hint gives us
two hints actually gives us a hint about
how we might do a better job than just
simply behaving randomly in our rollout
policy so here's something you might do
you might say when I get down to here
rather than behaving randomly and we're
by the way I sometimes will get eaten by
a ghost even though I could have easily
avoided it I'm going to try to behave
randomly while still honoring whatever
constraints I happen to have defined so
here rather than trying to just walk
around until something bad happens I'm
going to try to stay alive as long as
possible which is what this constraint
tells me and that's a slightly that's a
different kind of robot policy but one
that I claim that gives you a sort of
better view of the goodness of this
state this state action pair is better
because you can avoid violating your
constraints for longer than if you are
say in this state and take this action
and there are lots of other things that
you might do I just sort of wanted to
give you a flavor of it which is that I
can use these sort of options if they
are a certain type in order to be
smarter about the way I do my robot
policy there's also another way that we
can do this I can still use options in
general in this entire search up here
before rather than taking primitive
action and doing my search through the
tree and my expansion I can replace this
with my actions with options as well and
what's the big win there well the big
one there is by following the options I
am getting deeper into the tree yeah I
mean it's it's the same kind of ideas in
the option case that we talked about
already it's except now we're in trees
instead of I guess grids
yes Patrese and grids it turns out
they're all the same thing so there's
lots of things you can talk about
there's been a lot of work in this space
some of them are available as readings
for the students I recommend thinking
about them but what's I think kind of
nice about it is that this general
notion of treating solving an MDP not as
just I've got to solve the MDP but as a
kind of tree search problem where we
take advantage of and use the randomness
is something that unifies a lot of
approaches that you might have okay cool
okay and I think that's pretty much I'll
say one more thing about this and then I
think we're done and that's just that
there are lots of ways of looking at
this we might ask yourself is this even
reinforcement learning where we just
gone back to searching and I'm gonna
claim it is I'm gonna claim it's a
policy search algorithm what's your
justification for that
well my justification for that is I'm
basically by building this up I am doing
I am searching over possible policies
and what I'm doing in the inner loop of
searching for policies is I'm doing a
kind of reinforcement learning step here
when I do my backups and in fact the way
you do the backups is you use the
bellman equation as a way of our es
tomato values and once I have those I
now have another policy that I'm looking
at and I just keep doing that over and
over again so I'm really searching
through policy space which is sort of
what the tree expansion is and I have a
sort of inner loop every time I run out
of my confidence about the policy of
doing a little bit of reinforcement
learning to figure out what the policy
ought to be so it's sort of policy
search with an inner loop of value
iteration sort of and and in policy
evaluation monte-carlo policy evaluation
somehow right exactly well that's really
what it is and the way I'm doing the
policy evaluation is by doing this well
doing the Montecarlo
so it really combines a lot of these a
lot of a lot of ideas that we've talked
about into kind of one algorithm does
this has just been successful it has
been successful I've actually used it in
my own work it has this really neat
feature that allows you to avoid
searching over lots and lots of the
state space it's actually really
wonderful it actually has a couple of
properties that are worth mentioning so
let me mention those properties and then
I think we're done</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>