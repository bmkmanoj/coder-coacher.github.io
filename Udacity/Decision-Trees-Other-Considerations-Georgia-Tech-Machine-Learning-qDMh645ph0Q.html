<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Decision Trees Other Considerations - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Decision Trees Other Considerations - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Decision Trees Other Considerations - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qDMh645ph0Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so we've answered the thing about
continuous attributes now here's another
thing when do we really stop when we get
all the answers right but when all the
training examples are in the right
category class right so the answer in
the algorithm is when everything is
classified correctly that's pretty good
answer Michael but what if we have noise
in our data what if it's the case that
we have two examples of the same object
the same instance but they have two
different labels then this will never be
the case oh so then our algorithm goes
into an infinite loop which seems like a
bad idea so we could have we could we
could just say or we've run out of
attributes or we run out of attributes
that's one way of doing it in fact that
was that's going to have to happen at
some point right it's probably a
slightly better answer although that
doesn't help us in a case where we have
continuous attributes we might ask an
infinite number of questions so we
probably need a slightly better
criterion don't you think so what god is
down this path was thinking about what
happens if we have noise why would we be
worried about having noise anyway noise
anyway well I guess the training data
might have gotten corrupted a little bit
or maybe somebody copied something down
wrong right so since that's always a
possibility does it really make sense to
trust the data completely and go all the
way to the point where we perfectly
classify the training data but char's if
we can't trust our data what can we
trust well we can trust our data but we
want to verify the whole point is
generalization and if it's possible for
us to have a little bit of noise in the
data an error here or there then we want
to have some way to be able to handle
that possibility right I guess so so
what would we do and we actually have a
name for this right when you get really
really really good at classifying your
training data but it doesn't help you to
generalize we have a name for that
right that sounds like overfitting
exactly we have to worry about
overfitting so you can over fit with the
decision tree - yeah what you don't
believe that
no no I was I was being naive I mean as
being pretty know that you can over fit
with the disagree I always took yeah but
your earth is the urn that you usually
give when you're like I don't believe
what you just said Charles but I'm gonna
go along with it anyway because I have
to get off the phone sir fair enough
I'll try to try to have a different
personality then okay that's step one
have a different personality with
maximal information game okay
so we don't want it we don't want to
overfit so we need to come up with some
way of overfitting now the way you over
fit and a decision tree is basically by
having a tree that's too big it's too
complicated right violates Occam's razor
so what's the kind of let's say
modification to something like id3 to
our decision tree algorithm that will
help us to avoid overfitting well last
time we talked about overfitting we said
cross-validation was a good way of
dealing with it which allowed us to
choose from among the different say
degrees of the polynomial right so maybe
we could do something like that I don't
know try all the different trees and see
which one has the lowest
cross-validation error maybe there's too
many trees maybe but that's a perfectly
reasonable thing to do right you take
out a validation set you build a
decision tree and you test it on the
validation set you pick whichever one
has the lowest error on the validation
set that's one way to avoid it and then
you have don't have to worry about this
question of stopping you just grow the
tree on the training set minus the
validation set until it does well on
that and you check it against across
value check it against the validation
set and you pick the best one that's one
way of doing it and that will work
perfectly fine there's another way you
can do it that's more efficient which is
you do the same idea of validation
except that you hold out a set and as
you every time you decide whether to
expand the tree or not you check to see
how this would do so far in the
validation set and if the error is low
enough then you stop expanding the tree
that's one way of doing it so is there a
problem in terms of I mean if we're
expanding the tree depth-first search
wise we could be you know looking at one
tiny little split on one side of the
tree before we even look at any anything
on the other side of the tree it's upon
important
so how would you fix that maybe expand
breath first yeah
that would probably do it anything else
you can think of well so you could do
pruning right you could go ahead and do
the tree as if you didn't have to worry
about overfitting and once you have the
full tree built you could then do a kind
of you could do pruning you could go to
the leaves of the tree and say well what
if I collapse these leaves back up into
the tree how does that create error on
my validation set and if the error is
too big then you don't do it and if it's
very small then you go ahead and do it
and that should help you with
overfitting
so that whole class of ways of doing it
is called pruning and there's a whole
bunch of different ways you might prune
but pruning itself is one way of dealing
with overfitting and getting you a
smaller tree and it's a very simple
addition to the standard high D 3
algorithm</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>