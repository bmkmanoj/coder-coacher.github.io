<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>General Sum Games - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="General Sum Games - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>General Sum Games - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/g8BlBBRCP3g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so okay so let's think about general
some games so not zero-sum anymore but
we're not you know restricted it could
be any kind of relationship between the
two players and so the first thing we
need to do is realize well well we can't
really do minimax here anymore right
because that doesn't make sense right
that only works with zero sum games well
it's all it yeah that's well it only it
sort of assumes that the other player is
trying to minimize my reward and that's
not that's not the concept of a Nash
equilibrium we would like to do
something analogous and find a Nash
equilibrium in this general some setting
so what what operator do you think we
would need in this context here that's
for equilibrium yeah so that would be a
very reasonable thing to do is instead
of computing minimax we actually compute
of the two matrix game right using q1
and q2 compute the Nash equilibrium of
that and propagate that value back it's
a well-defined notion right that we can
summarize the value of these two payoff
matrices with with a pair of numbers
which are the values of the Nash
equilibrium mm-hmm all right so so good
so we can do the same thing in the queue
learning setting substitute in a Nash
equilibrium and we can call that
algorithm Nash q which is appears in the
literature nice Oh minimax q by the way
is is something that I wrote about Nash
q is a different algorithm so it's not
as cool which is not well let's uh let's
see how it goes so so this this is now
an algorithm you can actually well this
is a set of equations it's not really
clear exactly what it means but we can
think about turning that into value
duration right by turning this into an
assignment statement mm-hmm so what
happens well valuation doesn't work no
so yeah so if you you repeat this over
and over again things weird things can
happen it doesn't it doesn't really
converge it doesn't really solve this
system of equations necessarily hmm and
if fortunately the the reasoning here is
even harder in the case of Nash q
because in the case of Nash q it's
really trying to solve this system of
equations using something like value
duration but with extra stochasticity
and so it also suffers the same problem
it doesn't necessarily converge there's
not really a unique solution to Q star
because the
you have different Nash equilibria that
have different values right so it isn't
really much hope of converging to the
answer because there isn't of the answer
the the policies cannot be computed
independently right so Nash equilibrium
is really defined as a joint behavior
and so we can't just have two different
players computing Q values even if we
could compute the Q values it wouldn't
necessarily tell us what to do with the
policies because if you take two
different policies that are both half of
a Nash equilibrium two halves of a Nash
equilibrium do not necessarily make a
whole Nash equilibrium because they can
be incompatible so you know so far so
good right yeah I can't wait to see what
happens next the update is not efficient
unless P equals P pad which is to say
computing a Nash equilibrium is not a
polynomial time operation as far as we
know it is as hard as any problem in a
class that's known as pee pad and this
is actually a relatively recent result
in that and last 5-10 years and this
class is believed to be as hard as as NP
so possibly harder so it doesn't really
doesn't really give us any leverage to
computational leverage to kind of break
it down in this way so that's
unfortunate and finally the last little
hope of well maybe we can define this
kind of learning scenario using Q
functions the same way we've been doing
q functions are not sufficient to
specify the policy that is to say even
if I could do all these other things
efficiently compute a solution of you
know build the Q values make them so
that they're compatible with each other
and now I just tell you here's your Q
function now decide how to behave you
can't it's there's not enough
information you're depressing me Michael
yeah so this is kind of sad we go to the
general some case which in some sense is
the only case that matters because zero
sum never really happens and what we
discover is that we lose all seemingly
lose all the leverage that we have in
the context of Q type algorithms mmm and
that's where we'll stop oh so we're
gonna end on a high note now maybe we
should say something positive before we
depart let's do that come up with
something positive to say okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>