<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Markov Decision Processes Four - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Markov Decision Processes Four - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Markov Decision Processes Four - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dkBZ9YKuOVA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">speaking of solutions this is the last
little bit of thing that you need to
know and that is this defines a problem
but what we also want to have whenever
we have a problem is a solution so the
solution to a Markov decision process is
something called a policy and what a
policy does is it's a function that
takes in a state and returns an action
in other words for any given state that
you're in it tells you the action that
you should take like as a hint
no it just tells you this is the ak-12 I
mean I suppose you don't have to do it
but the way we think about Markov
decision process is that this is the
action that will be taken I see so it's
more of it more of an order yes it's a
command okay so that's all the policy is
a policy is a solution to a Markov
decision process and there's a special
policy which I'm writing here is policy
star or the optimal policy and that is
the policy that maximizes your long-term
expected reward so if all the policies
you could take of all the decisions you
might take this is the policy that
optimizes the amount of reward that
you're going to receive or expect to
receive over your lifetime so it's like
at the end well yeah at the end or at
any given point in time how much reward
you're receiving from the Markov
decision process point of view there
doesn't have to be an end okay so in
this example you don't get anything and
then at the end you get paid off right
or unpaid off right if you fall into the
red square so actually your question
points out something very important here
I mentioned early when I talked about
the three kinds of learning that they're
supervised learning and reinforcement
learning was sort of similar except that
instead of getting wise in X's we were
given y's and x's and Z's and this is
exactly what's happening here here what
we would like to have if we wanted to
learn a policy is a bunch of sa pairs as
training examples well here's the state
in the action should have took taken
here's another state in the action you
should have taken so on and so forth and
then we would learn a function the
policy that map's States to actions but
what we actually see in the
reinforcement learning world in the
Markov decision process world is we see
states actions and then the rewards that
we received and so in fact this problem
of seeing a sequence of states actions
and rewards is very different from the
problem of being told this is
the correct action to take maximize the
function or find a function that maps
from States to action instead we say
well if you're in the state and you take
this action this is the reward that you
would see and then from that we need to
find the optimal action so PI star is
being the F from that previous slide
right and R is being Z yes and Y is
being a and s is being X or X is being s
gotcha right so put ok I'm a little
confused about this notion of a policy
so we have the the thing that we tried
to do to get to the goal was was up up
right right right yes I don't see how to
capture that as a policy it's actually
fairly straightforward what a policy
would say is what state are you in tell
me what action you should take so the
policy basically is this when you're in
the state start the start state the
action you should take is up and it
would have a mapping for every state
that you might see whether it's this
state this state this state this state
this state this state this state this
state or even these two states and it
would tell you what action you should
take and that's what a policy is a
policy very simply is nothing more than
a function that tells you what action to
take in every in any state you happen to
come across ok but the but the the
question that you asked before was about
up up right right right mm-hmm and it
seems like because of the stochastic
transitions you might not be in the same
state like you you don't know what state
you're in when you take those actions no
so one of the things for what we're
talking about here for the Markov
decision process is there are States
their actions their rewards you always
know what state you're in and you know
what rewards you receive does that mean
you can't do up up right right right
well the way it would work in a Markov
decision process so what you're
describing is what's often called a plan
you know it's tell me what sequence of
actions I should take from here what a
Markov decision process does and what a
policy does it doesn't tell you what
sequence of actions to take from a
particular state it tells you what
action to take in a particular state you
will then end up in another state
because of the transition model the
transition function and then when you're
in that state you have
the policy what action should I take now
okay right so this is actually a key
point although we talked about it in the
language of planning which is very
common for people who did for example
taking nei course to think about this in
terms of planning what are the things
that I can do to accomplish my goals the
Markov decision process way of thinking
about it the reinforcement learning way
of thinking about it or the typical
reinforcement learning way of thinking
about it really doesn't talk about plans
directly but instead talks about
policies which from which you can infer
a plan but this has the advantage that
it tells you what to do everywhere and
it's robust to the underlying stochastic
city of the world so is it clear that
that's all you need to be able to behave
well well it's certainly the case that
if you have a policy and that policy is
optimal it does tell you what to do no
matter what situation you're in okay and
so if you have that then that's
definitely all you need to behave well
but I mean could it be that you want to
do something like up up right right
right which you can't write down as a
policy why can't you write that down as
a policy because the policies are only
telling you what action to do as a
function of the state not like sort of
how far along you are in the sequence
right unless of course you fold that
into your state somehow but that's
exactly right I mean the way to think
about this is the idea of coming up with
a concrete plan of what to do for the
next twenty time steps is different from
the problem of whatever step I happen to
be in whatever state I happen to be in
what's the next best thing I can do and
just always asking that question if you
always ask that question that will
induce a sequence but that sequence is
actually dependent upon the set of
states that you see whereas in the other
case where we wrote down a particular
policy you'll notice that was only
dependent upon the state you started in
and it had to ignore the states that you
saw along the way and the only way to
fix that would be to say well after I've
taken an action let me look at the state
I'm in and see if I should do something
differently but if you're going to do
that then why are you trying to compute
the complete set of states or I'm sorry
the complete set of actions that you
might take okay okay so there you go now
a lot of what we're going to be talking
about next Michael is given that we have
MD P we have this mark
decision process defined like this how
do we go from this problem definition to
finding a good policy and in particular
finding the optimal policy that makes
sense good and there you go</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>