<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Greedy Exploration - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Greedy Exploration - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Greedy Exploration - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yv8wJiQQ1rc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so let's say something that we
actually know about this approach to
action selection which is called Epsilon
greedy exploration and what we know is
that if the action selection is Glee
which means greedy in the limit with
infinite exploration and that basically
means that we're decaying our epsilon
for Epsilon greedy that we start off
more random and over time we get less
and less random more and more greedy
then we have two things that are true
one is that Q hat goes to Q which is
which comes from kind of the standard Q
learning convergence result but we also
have something cooler in this case which
is that the policy that we're following
pi hat is getting more and more like the
optimal policy over time so not only do
we learn stuff but we use it too and
this is an example of the exploration
exploitation dilemma and exploration
exploitation is really really important
not just because there are two words
that surprisingly start with the same
five letters like unlikely letters but
also that it is strike it is talking
specifically about this issue
exploitation is about using what you
know and exploration is about getting
the data that you need so that you can
learn and this is one particular way of
doing it turns out there's lots of other
ways of of making this trade-off and the
reason that it's a trade-off is because
there's only one of you there's only one
agent acting in the world and it has
these two actually somewhat conflicting
objectives one is to take actions that
it doesn't know so much about so it can
learn about them and the other one is to
take actions that it knows are good so
that it can get high reward that makes
sense you know I didn't I never realized
that exploration and exploitation shared
the first five letters I always knew
that they shared the last five letters
oh that's interesting too huh so if you
take an R and turn it into an it you
move from exploration to exploitation
I feel like an entire political movement
could be founded on that I'm not sure
exactly what it would be I maybe I'll
work on that before our next lesson so I
have an algorithm there's a standard
lemma mm-hmm in reinforcement learning
theory called the exploration
exploitation dilemma sorry know lemma
the other kind of lemma the exploration
exploitation lemma which has to do with
taking
actions that are either exploring or
exploiting but I have one where you
actually do teaching you can actually
each time you take an action it's either
going to teach the agent something it's
going to explore or exploit so I call
that exploration exploitation or explore
exploit explain no nice but you could
have called it the exploration
exploitation dilemma because you use
dilemma and die means to sometimes and
you did the two things
well in fact dilemma does literally mean
- it's a choice between two things right
so it's a dilemma it's a limb about two
things all right so there's it turns out
there's a lot of other approaches to
exploration exploitation and some of
them in the in the model based setting
you can do a lot more with it a lot more
powerful things with it because you
actually can keep track of what you've
learned effectively in the environment
and what you haven't so the algorithm
can actually know what it knows and can
use that information to explore things
that it doesn't know and exploit things
that it does know q-learning doesn't
really have that distinction it's a much
harder thing to do hmm
so so that's what I wanted to tell you
in terms of you know thinking about
exploration exploitation does that make
sense you it doesn't make sense to me I
think what the main point I got out of
this or a main point I got out of this
other than our incredible ability to get
caught up in letters and coincidences of
spelling is that the exploration
exploitation dilemma really is a dilemma
it's like the fundamental trade-off in
reinforcement learning you have to
exploit because you have to use what
you've got but you have to learn
otherwise you might not be able to
exploit profitably so you have to always
trade-off between these things and if
you don't you're bound to either learn
nothing or to get caught in local
minimum I couldn't agree with you more
in some sense if you think of
reinforcement learning as being the
question of model learning plus planning
there's nothing new here because model
learning is well studied in the machine
learning community and planning is well
studied in the planning and scheduling
community and so like what are we adding
to this and what we're adding is the
fact that these two processes interact
with each other and depend on each other
and that's exactly the exploration
exploitation dilemma and that's
information has to go back and forth
between these two processes that other
people understand and we're the glue
or the Glee Glee glue I like it that's
beautiful</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>