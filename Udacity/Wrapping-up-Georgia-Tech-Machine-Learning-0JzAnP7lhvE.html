<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Wrapping up - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Wrapping up - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Wrapping up - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0JzAnP7lhvE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Michael so this is it why don't you
help me remember what we learned in this
one marathon session that was not all
distributed over multiple months or
years so MDP so we talked about Markov
decision processes and we said what that
meant okay so that's the first thing we
did is we talked about MVPs and that at
MVP consists of states and rewards and
actions and like that transitions
discounts so you said discounts and I
just want to point out that there are
some people who think that that's a part
of the definition of the problem and
there are some people who think that
that's more of a parameter to your
algorithm okay all right so there's some
people who think of it the right way
like me and some people think of the
wrong ways I tend to think of the
discount as being something that you're
allowed to fiddle with as opposed to it
being a fundamental part of the MDP well
then why not fiddle with the rewards
sure you actually are allowed to fiddle
with the rewards you're very very open
minded I am open minded I believe very
much that rewards should be able to
marry other rewards in any case the
important thing here is that there's
some underlying process that you care
about that's supposed to represent the
world States rewards actions and
transitions capture that fairly well
discount is in some sense an important
part of the problem because it tells you
how much you want to care about the
future versus the past but it's
reasonable to think of that as something
that you might want to change outside of
the kind of underlying physics of the
world I see okay that's fair yeah it's
some sense the states and the actions
and the transitions represent the
physical world and the rewards and the
discount represent the kind of task
description right and of course you say
that but if you you could decide to
define States differently and doing that
would impact both your actions and the
transition function and so on and so
forth
but the basic idea is right which is
there's some underlying process we're
trying to capture and I think it's
exactly right to say States actions and
transitions sort of capture that and
rewards and discounts capture more about
the nature of the task you're trying to
do in that underlying world
okay and in that context we talked about
two really important concepts policies
and value functions mm-hmm which we
sometimes called utilities right and how
do you Tillett E's differ from rewards
the utilities factor in the long term
aspects and the rewards are just telling
you the moment-to-moment right
utilities are like a group of rewards
like a gaggle or a murder of crows so
that we talked about how we can assign
value to an infinite sequence of rewards
but it helps if we use discounting to do
that so that we don't get infinitely
large sums and that allowed us to deal
with infinite sequences but treat them
as if their value is in fact finite the
solving the immortality problem and
let's see that we kind of and the
stationarity was a really important part
of that yep
in fact kind of drove everything yeah
and all these things were tied up
together in the bellman equation itself
yes which is an awesome equation and
deserves capital letters is that it and
then well then we solved the bellman
equation using value duration and policy
narration I think that was it all right
so we're any of these polynomial time
algorithms
well the way we've been talking about
them no but you can map these into
linear programs and turn them in the
polynomial problems or probably no mill
time algorithms so yes these problems
can be solved that way but actually that
reminds me of something that we haven't
said and that we haven't learned today
that I think is worth mentioning which
is we've been talking about you know
this section of the class is about over
the course is about reinforcement
learning but we actually haven't done
any reinforcement learning here because
we know everything we know the states we
know the rewards we know the actions we
know the transitions we have some
discount we have been solving MVPs but
that's not quite the same thing as doing
reinforcement learning however it's very
important to do these things to make it
easier to think about how reinforcement
learning works so in reinforcement
learning the difference is you don't
necessarily know the rewards and the
transitions or even the states in the
actions for that matter I see so okay so
when are we gonna get into reinforcement
learning that next time and when I say
we next time I mean you next time
that's your assignment learn all about
reinforcement learning and talk about it
next time all right I got to go and get
to work then
okay well you have a good one Michael
thanks thanks for all this this is
really cool it is cool bye bye bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>