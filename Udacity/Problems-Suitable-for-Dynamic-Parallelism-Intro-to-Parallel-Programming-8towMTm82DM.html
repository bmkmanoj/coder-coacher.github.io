<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Problems Suitable for Dynamic Parallelism - Intro to Parallel Programming | Coder Coacher - Coaching Coders</title><meta content="Problems Suitable for Dynamic Parallelism - Intro to Parallel Programming - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Problems Suitable for Dynamic Parallelism - Intro to Parallel Programming</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8towMTm82DM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so what are the kind of problems you've
looked at recently as you were designing
this where dynamic parallelism really
makes a difference either in terms of
usability or performance in terms of
usability obviously it's simpler to
program when you don't have to keep
going backwards and forwards to the CPU
so any kind of problem which dynamically
discovers the work as it goes
iteratively works its way through
something imagine you're constructing a
tree
you're potage partitioning something
into a into an octree which is like a
common a common 3d spatial problem you
don't necessarily know how many objects
are going into which part of your tree
until you until you reach that point in
your tree so typically the approach
would be to do this level by level by
level which is not the most balanced way
of doing things necessarily so as you
discover these type as you discover the
work that you need to do the for the
ability to just simply launch that work
is much much simpler so with that I was
we were very motivated by irregular
parallelism if you like problems which
did not have nice well balanced things a
similar type of cat sort of category of
problem - that is task parallelism where
I might be wanting not just to do one
thing that fills my whole GPU which is
often difficult that the GPU is these
days are a teraflop of performance it
might it might be much easier to have
half a dozen or a dozen things we
running a my GPU at a time and so it's
each if each of these can autonomously
make forward progress it's much easier
to manage them if they're just managing
themselves instead of having my CPU now
juggle twelve different things instead
of just one and finally there's the
there's the type of a new type of
algorithm that you can approach you can
approach a recursive types of algorithm
things where the categories don't either
divide and conquer algorithm where the
way you take all of them would work that
you need to do and you conquer it by
subdividing and sub dividing and
subdividing repeatedly and a typical
example would be quicksort for example a
well nerd a well known problem where you
take your your data that you want to
sort and recursively you progress
through the data until you end up with
it with a final sort of data set so one
of the demos you guys showed when you
launched this was
in body simulation a interstellar a
bunch of stars moving around being
attracted to each other with gravity and
so with dynamic parallelism you were
able to write that in a way that you
hadn't written it before so how did that
come about like what was the cool thing
you could do with it
I mentioned octree spatial partitioning
plus now and that is a key component of
an n body simulation so the way that you
approach an n body simulation with a
very large number of bodies is instead
of doing an all-to-all comparison where
you just calculate the gravity between
all the bodies so for n bodies that's an
N squared problem you can cut down the
complexity of the problem to an N log N
or an order of an n problem by
partitioning things into space doing a
local interaction of gravity between
your closed neighbors and doing an
approximation to a center of mass at a
more distant neighbor to do this you
build an octree partitioning your bodies
up into small octants little cubes
everyone inside your cube you do you do
the N squared problem and for all of the
other cubes you have you have you then
only have an order of an N expansion for
so what we did with dynamic parallelism
in an N body in the same body problem
was we optimized the tree build which is
approximately half of the time that it
takes M ulation we optimized the tree
built by date by using this recursive
property by using this this irregular
parallelism ability where the tree might
in the case of a galaxy of stars for
example might be very very dense in some
regions and very very sparse and others
too much more efficiently build the tree
so instead of building the tree level by
level by level so that you're wasting
work were building the tree for areas of
where the bodies are sparse you focus
the compute performance on the area
where you need it could you have done
this with our dynamic parallelism yes I
guess you could but the overhead of
moving backwards and forwards between
the CPU and GPU would probably have
negated the performance gain that we go</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>