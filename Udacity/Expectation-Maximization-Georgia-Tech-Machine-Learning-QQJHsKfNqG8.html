<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Expectation Maximization - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Expectation Maximization - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Expectation Maximization - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QQJHsKfNqG8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this is going to lead us to the
concept of expectation maximization so
expectation maximization is actually at
an algorithmic level it's surprisingly
similar to k-means so what we're going
to do is we're going to tick-tock back
and forth between two different
probabilistic calculations so you see
that I'd kind of drew it like the other
one mm-hmm the names of the two phases
are expectation and maximization sort of
you know our name is our algorithm I
like them so what they're going to do is
we're going to move back and forth
between a soft clustering and computing
the means from the soft clusters so the
soft clustering goes like this this
probabilistic indicator variable z IJ
represents the likelihood that data
element I comes from cluster J and so
the way we're going to do that since
we're the maximum likelihood setting is
to use Bayes rule and say well that's
going to be proportional to the
probability that data element I was
produced by cluster J and then we have a
normalization factor normally we'd also
have the prior in there so why is the
prior gone Charles well because you said
it was a maximum likelihood scenario
yeah right we talked about how that just
meant that it was uniform and that
allowed us to just leave that component
out it's not going to have any impact on
the normalization right so that's what
the Z step is if we had the clusters if
we knew where the means were then we
could compute how likely it is that the
data would come from the means and
that's just this calculation here so
that's computing the expectation
defining the Z variables from the Mew
the Centers we're going to pass that
information that clustering information
Z over to the maximization step what the
maximization step is going to say is
okay well if that's the clustering we
can compute the means from those
clusters all we have to do is just take
the average variable value right so the
average of the x i's within each cluster
J what's the likelihood it came from
cluster J and then again we have to
normalize if you think of this as being
a zero one indicator variable then
really it is just the average of the
things we assigned to that cluster but
here we actually are kind of soft
assigning so we could have half of one
of the data points in there and only
counts half towards the average and we
could have a tenth and one another place
and a whole value in another place and
so we're just doing this weighted
average of the data points so cash your
question Michael yeah so this makes
sense to me and I even get that for the
Gaussian case the
Zi very will always be nonzero in the
end because there's always some
probability that it comes from some
Gaussian because they have infinite
extent so this all makes sense to me is
there a way to take exactly this
algorithm and turn it into k-means I'm
staring at it and it feels like if all
your probabilities were ones and zeroes
you would end up with exactly k-means I
think I don't know I never really
thought about that let's think about
that for a moment certainly the case if
all the Z variables were 0 1 then the
maximization step would be the means
which is what k-means does then what
would happen we send these means back
and what we do in k-means is we say each
data point belongs to its closest center
which is very similar actually to what
this does except that here we then make
it proportional so I guess it would be
exactly that if we made these clustering
assignments push them to 0 or 1
depending on which was the most likely
cluster right so if you made it so that
the probability we be into a cluster
actually depends upon all the clusters
and you always got a 1 or 0 basically
did this was like a hidden Arg max kind
of a thing or hidden max or something
then you would end up with exactly
gaming's I think you're right yeah I
never thought about that ok so it really
does end up being an awful lot like the
k-means algorithm which is improving in
the error metric this squared error
metric this is actually going to be
improving in a probabilistic metric
right that the data is going to be more
more likely over time that makes sense</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>