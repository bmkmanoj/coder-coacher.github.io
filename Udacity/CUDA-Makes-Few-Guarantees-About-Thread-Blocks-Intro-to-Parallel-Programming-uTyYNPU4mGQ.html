<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CUDA Makes Few Guarantees About Thread Blocks - Intro to Parallel Programming | Coder Coacher - Coaching Coders</title><meta content="CUDA Makes Few Guarantees About Thread Blocks - Intro to Parallel Programming - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CUDA Makes Few Guarantees About Thread Blocks - Intro to Parallel Programming</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uTyYNPU4mGQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so in short kuda makes few guarantees
about when and where thread blocks will
run and this is actually a huge
advantage for kuda this is one of the
big reasons why GPU programs can go so
fast why is that so among the advantages
that the GPU gains from this programming
model is that the hardware can run
things really efficiently because it has
so much flexibility for example if one
thread block completes quickly the SM
can immediately schedule another thread
block without waiting for any others to
complete but the biggest advantage is
scalability okay because you've made no
guarantees about where the thread blocks
will run or how many thread blocks might
be running at a time that means that you
can scale all the way down to a GPU that
would be running with a single SM
something that you might find in a
tablet or a cell phone all the way up to
the massive GPUs that are used in
supercomputers importantly the
scalability also applies to future GPUs
so you can be sure that GPUs will get
more and more SMS as Moore's law gives
us more and more transistors on a chip
and by writing the code in such a way
that it can run on an arbitrary number
of SMS and doesn't depend on a certain
number of SMS a certain number of thread
blocks being resident at a time you can
be sure that your CUDA code will scale
forward to larger and larger GPUs so
this scalability applies from cell
phones to supercomputers from current to
future GPUs and that's a really huge
advantage and there are also
consequences to this programming model
which of the things we've been talking
about you can make no assumptions about
what blocks will run on what SM and you
can't have any explicit communication
between blocks for example if block X is
waiting for block Y to give us some
result before it can proceed but block Y
has already run to completion and exited
then you're going to be in a bad place
that by the way is an example of
something called deadlock in parallel
computing this really means that threads
and blocks must complete okay you can't
simply have a thread that hangs around
forever processing input or doing
something because that thread must
complete in order for the block that
it's in to complete so that other
threads and blocks can get scheduled
onto that SM</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>