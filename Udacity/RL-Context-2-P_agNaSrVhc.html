<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>RL Context - 2 | Coder Coacher - Coaching Coders</title><meta content="RL Context - 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>RL Context - 2</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/P_agNaSrVhc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so the way I like to think about it is
there's kind of three main families of
reinforcement learning algorithms so the
one that's most like what we've talked
about so far is is what we could call
model-based reinforcement learning
algorithm so model-based reinforcement
learning algorithm takes the the state
action reward tuples that it gets and
sends them to a model learner which
learns the transitions and rewards those
transition rewards once you've learned
them you could put through an MDP solver
like we talked about which could be used
to spit out a cue star a an optimal
value function and once you have the
optimal value function you can use just
by taking the Arg max of the state that
you're in you can choose which action
you should take in any given state and
that gives you the policy so it's still
mapping the state action reward sequence
to policies but it's doing it by
creating all these sort of intermediate
values in between cool and let me just
add one more thing which is that the the
model learner takes the history that is
seeing but it also takes its current
estimate of the transition to rewards to
produce the new estimate of the
transition reward so this is how I want
to kind of represent the learning piece
of the of the sequence does that make
sense all right so this is one type now
let me show you another type so the
second kind of class of reinforcement
learning algorithm that's important to
think about they're referred to as value
function based or actually sometimes
model free so the beginning of the end
of it are still the same we're taking
sequences of state action rewards and
producing a policy and we even have this
Q star in between that we generate the
policy from using the arc max but now
instead of feeding back the transitions
and rewards we're actually feeding back
Q star and we have a direct value update
equation that takes the state action
reward that it just experienced the
current estimate of the Q of Q star
actually it's kind of more acute than Q
star and and uses that to update and
generate a new Q which is then used to
generate a policy yeah so instead of
instead of explicitly building a model
and using it it just somehow directly
learns Q values from values sorry from
state actions and rewards and then a
third class of reinforcement learning
algorithms comes from the idea that you
could sometimes actually take the policy
itself and fee
that back to a policy update that
directly modifies the policy based on
the state action rewards that you
receive so in some sense this is much
much more direct but the learning
problem is very difficult because the
kind of feedback that you're getting
about the policy isn't really very
useful for directly modifying the policy
so here's these three models all
together so you can compare and contrast
and you can see in some sense they're
getting kind of simpler as we go down to
policy search but you could also sort of
say that the learning is more direct and
as we go up this way the learning
problems become more supervised in the
sense that you can you can imagine
learning to predict next dates and next
rewards from previous dates and previous
rewards to learn TNR pretty directly
these are it's a basically a supervised
learning problem you get to see what the
outputs supposed to be here you don't
quite get to see what the outputs
supposed to be but you do get values
that you could imagine kind of
propagating backwards and here you get
very little useful feedback in terms of
how to change your policy to make it
better wait so the way you put that
suggest that you should always do one
well so yeah but look at all the extra
stuff that you're doing in between so in
fact different problems have different
trade-offs as to whether or not these
which of these is sort of the best thing
to do a lot of the research has been
focused on number two and that's that's
what we're going to be diving into
because in some ways it strikes a nice
balance between keeping the computations
relatively simple and keeping the
learning updates relatively simple but
there are plenty of cases where you'd
rather do one and where you rather do
three I look forward do you convince me
that that's true</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>