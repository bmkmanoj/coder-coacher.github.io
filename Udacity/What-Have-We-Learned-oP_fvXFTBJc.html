<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What Have We Learned | Coder Coacher - Coaching Coders</title><meta content="What Have We Learned - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What Have We Learned</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oP_fvXFTBJc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so what do we learn about exploring
exploration Charles well we actually
learned a lot but really I think you can
summarize in just a couple of things we
learned about bandits okay which is
really all about stochasticity and
randomness and in particular we learned
about how we can estimate what we know
using huffing bounds and maybe some
Union bounds so yeah in particular we
employed huffing down Union bound to
convince ourselves that we have a
sufficiently accurate estimate that we
can get near optimal reward in these
kind of non-sequential but stochastic
decision problems right and that's
really important because a lot of the
time you don't know what to do because
you're not certain or how certain you
really are about what you know and what
the huffing and union bounds tell us is
how certain we really are and so we can
know when we're certain enough good all
right so we have a stochastic world we
want to learn how it works so that we
can get near optimal reward so we can
optimize right so that was the first
thing and then the second thing is we
learned well we learned about our max
really and how our max works
specifically with deterministic MVPs
right and so that was about optimism in
the face of uncertainty and how it would
cause us to explore door discover new
stuff about the world kind of by
planning ahead and trying to get to
states where new information could be
gained right well what it really says
it's really we believe the grass is
always greener until we get there and
learn otherwise all right great this let
us deal with sequential decision making
this let us deal with stochastic
decision making and then we combine them
right where we used the Bandit idea to
estimate the noisy parameters this to
the transition probabilities and so
forth and we use the R max idea to make
sure that we visited things enough so
that we could get those accurate
estimates right so rmx so basically
remained are Max and the Bandit stuff
helped us with the transition
probabilities so that we knew when we
actually believed them so they were kind
of I'm a time like this isn't really
true but they were kind of deterministic
enough so we knew enough to know what
the transition model is unless you know
this model is you can just kind of go
from there because then you can just
estimate the rewards and do the right
thing now this this notion of this the
way that learning happens in the Bandit
here where we actually kind of
distinguish between known and unknown
can actually be generalized e
and even beyond the kind of general MVPs
that we're talking about here and I
don't think we're going to get a chance
to talk about this but the idea of quick
learning is learning transition
probabilities using methods that know
what they know and if it can distinguish
between known and unknown it can
associate optimism with the unknown and
then we can make guarantees on how
equally and efficiently near optimal
behavior can be learned no so this is a
this is kind of a learning framework
that my students and I have been
studying to try to generalize beyond
learning in in tabular MDPs to be able
to generalize between transition
probabilities in different parts of the
MVP well that'll make sense to me so
then the only question is how does this
help us do practical reinforcement learn
all right so that pretty much ends this
well so okay so so just briefly I find
that our max is actually a really
effective algorithm to use in practice
we don't set C we don't set that
parameter of how often we need to see
things to be what the theory says we
should set it to we usually set it to
something much much smaller than that
and we try to by hand strike a trade-off
between making it too small in which
case it might actually not learn near
optimal behavior and making it too big
in which case the the learner actually
has to sniff around and visit many many
many many state action pairs over and
over again so r max itself does seem to
be a pretty effective algorithm it makes
very good use of the data just we can't
quite use it in the theoretically
specified way ok that's good enough for
many things pretty practical ok so we're
done so are we done with the class
what's next we have more stuff we have
what's next next oh cool well I look
forward to that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>