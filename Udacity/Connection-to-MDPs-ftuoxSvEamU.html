<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Connection to MDPs | Coder Coacher - Coaching Coders</title><meta content="Connection to MDPs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Connection to MDPs</b></h2><h5 class="post__date">2015-07-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ftuoxSvEamU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let's think about what these things
imply so the value of a state well under
the bellman equation is this and you
know nothing scary here right this is
all stuff that you know you know so now
the next thing we're going to do is
we're going to say well the value of the
state that we end up in since we're in
average Earl and we can write that as
the sum over all the anchor points of
how related this this state s prime is
or what the weight is between s Prime
and that basis point and then whatever
value we set for the basis point sure
all right so here it is in the context
of the whole bellman equation so this is
cool too right I just um I just changed
the order of summation and kind of
regrouped a little bit mm-hmm so now
we've got max over actions the reward of
that action for the state that we
started in now we're summing now instead
of overall next States we're stepping
over just the basis points sure sure
we're multiplying some quantity times
the value of the of the basis point what
can we say about this value here if we
write it let's say like this so we can
write this as some function T of the
state the action and the basis point and
the sum over next States has kind of
already built in here right yeah I gave
this a name i called it t.t prime
because i wanted you to see a connection
between this and transition functions so
how is this like a transition function
well it says well this unit state you
take an action what basis functions will
you go to next right what you don't I
mean the actual mdp you don't go to the
basis functions but the point is that
this acts like a transition function in
that it's not negative right and when
you sum up over all SBS over all the
anchor points it sums up to one right
why is it sum up to 1 because this is a
convex combination and this is a convex
combination and so when we combine them
we get a combat combination we could do
the algebra for that but I don't think
we need to and so it acts just like a
transition function so we can actually
fold the function approximator itself
into the transitions and all we get at
the end of it is an MDP so why does it
actually do the right thing because mdps
have a well-defined unique value
function and that you can find it using
things like q-learning and value
duration
beautiful isn't it though so so what we
do is we end up with something that is
actually a reasonable function
approximator it's taking the value of
some state s Prime and writing it as a
combination of other states and then we
just we turn it into over a series of
steps into a new Markov decision process
over a smaller set of states
specifically over just the basis States
and at the end of the day we know that
this is going to be well-behaved we know
that it's going to approximate things
well we don't know this can approximate
things reasonably but at least without
blowing up in a convergent sort of way
because we know a lot of stuff about
MVPs that's kind of insane it's very
it's very clever no no it's very very
clever so if I were if I were going to
you know make too fine a point on it
what would I end up with a kind of
interpretation here I'm like saying the
NDP you're moving over is sort of from
basis States to basis States yeah I
think that's a good question so it's as
if we're going from a state to some next
state which is s Prime and then from
there we're making an additional mini
transition to the basis States the sb's
and the weights on these these
probabilities are the combination
weights are they the the convex
combination weights convex combination
weights look a lot like probabilities
right so it's as if each time we make a
transition to a next state s prime we
make an additional little transition to
each of the basis States and from there
we continue and so we can actually solve
out what the value function is over
everything by just solving the MDP over
the basis States right and then we can
extrapolate that to air interpolate that
to anything that we want I love it that
makes some sense because the basis
States sort of represent a set of state
so really what you're talking about
you're averaging over the set of states
that kind of get alias through the basis
function I think that's beautiful yeah
exactly so so yeah we fold our function
approximator into the MDP itself done
very cool so the good news is here's
here's a value function approximator
that is very well behaved the bad news
is it's hard to do some things with it
because value functions aren't always
very well represented as as average or
type things there's often you know kind
of weird cliffs and so forth in the
value function that are not well
captured by just you know smoothing
everything out but still this is this is
a really nice this is a really nice idea
beautiful</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>