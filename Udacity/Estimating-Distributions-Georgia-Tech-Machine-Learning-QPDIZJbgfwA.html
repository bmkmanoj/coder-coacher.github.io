<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Estimating Distributions - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Estimating Distributions - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Estimating Distributions - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QPDIZJbgfwA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Michael so let me see if I can
convince you that we have some way of
actually estimating these distributions
so all I've written up here is the chain
rule version of a joint probability
distribution okay but just to be clear
what these subscripts mean here every X
that we have is made up of a set of
features so there's let's just say
there's n of these features and so
really X is a vector
there's feature 1 there's feature 2
there's feature 3 dot dot dot all the
way up to feature in okay sure what I
really want to know from my P sub theta
and I'm going to drop the Thetas here
for the purpose of just describing this
generic distribution is I want to say
well the probability of means seeing all
of the features let's yeah all of the
features of some particular example is
just the Joint Distribution over all of
those features now of course we could
just estimate this but that's going to
be hard why is it going to be hard
because well the first distribution is
conditioned on a lot of things so it's
an exponential sized conditional
probability table exactly so this is
exponential table if its exponential
table then we need to have an enormous
amount of data in order to estimate it
well and this is sort of the fundamental
problem but we've addressed this in
earlier lectures earlier lessons Michael
in the inference lecture maybe yes in
the inference lecture um where we can
try to estimate this incredibly painful
joint distribution by making some
assumptions about conditional
independence and in particular I'm going
to make one kind of assumption and that
is that we only care about what's called
dependency trees okay so what's a
dependency tree Michael do you remember
no I have absolutely no idea I don't
think I've ever heard of such a thing ah
so the dependency tree is a special case
of a Bayesian network where the network
itself is a tree that is every node
every variable in the tree has exactly
one parent I see so sometimes in
Bayesian networks they talk about poly
trees Paula trees just mean that if you
ignore the direction of the edges you
have a tree but you're actually talking
about the the edges have to go in a
particular direction everybody's got one
parent right exactly right and then so
you should see these as a directed graph
although I almost never draw them that
way because it's sort of obvious by
looking at it that this is the root of
the tree so all we have here now is a
tree every node has one parent and what
does that mean that means that every
node every random variable depends on
exactly one other random variable yeah
so we can rewrite this Joint
Distribution here now as a product over
each of the features depending upon only
its parent I see so the first capital PI
there means product and the second
lowercase PI there means parent exactly
so when I compare this representation of
a distribution dependency tree
representation versus the full joint
what nice properties do I get from doing
it this way versus doing it that way
well that I guess the main positive is
that you're only ever conditioned on
it's got to be at most one right not
exactly one right at most one so so in
fact you picked up on a nice notational
thing here the parent of the tree the
root of the tree doesn't actually have a
parent so just assume that in this case
PI returns itself or something and so
it's the unconditional distribution and
that can happen at any time
gotcha all right yeah and so like if
nobody has any parents then that's the
same as naive Bayes but here it can have
at most one parents who know naive Bayes
has one exactly what everyone has one
parent it's exactly the same one right
good point so if everyone has no parents
then you're saying that all of the
features are in fact independent of one
another
gotcha okay all right but but that's not
what you asked you asked comparing that
probability distribution to the full
joint the main thing is that you're no
one's ever conditioned on more than one
other feature therefore the conditional
probability tables stay very very small
right in fact you how small to this day
well depends how big this other binary
features that we're talking about now
let's say there Barner so then it's like
it's two numbers right it's like your
probability when your parent is true and
your probability when your parent is
false
well that's each table but what about
representing the entire thing so it's
going to be linear in the size of the
number of variables
all right features right and the number
the amount of data that you need is
going to be fundamentally in order to
get this state in order to get this tree
it's going to turn out that you only
need to keep track of quadratic set of
numbers quadratic in the number of
features we're not going to be given
that tree we're going
to figure it out well remember one of
the steps of the algorithm is you have
to estimate the distribution so we're
going to have to figure out of all the
trees that I might have what's the best
tree that doesn't exactly follow from
the algorithm sketch because you could
also say well then you need to figure
out that a tree is the right thing at
all instead of something else like a box
that's fair but what we're going to do
is we're just going to assume that we're
going to do dependency trees now why are
we doing this we're actually doing for a
couple of reasons Michael your points
pretty uh your points well-taken
dependency trees have this nice feature
that they actually let you represent
relationships the point is that you're
actually able to represent relationships
between variables which in this case are
sort of features in our space but you
don't have to worry about too many of
them and the only question here then is
how many relationships do you want what
kind which of all the possible
relationships you could have where you
only are related to one other thing do
you want to have well in some sense a
dependency tree since you can depend on
it most only one other thing is the
simplest set of relationships you could
keep track of the next simplest would be
as you point out not having any parents
at all in which case you would be
estimating simply this yeah I wouldn't
say that's the next simplest that's even
simpler but it doesn't doesn't allow any
of the interrelationship any of the
covariance essentially information to be
captured no that's fair that's a fair
point so we could have started with
something like this except here you must
you you're forced to ignore all
relationships because you're treating
everything is independent and we don't
believe that things are independent or
at least we think there's a possibility
there's some dependence and so by
allowing it most that you're connected
to one other parent that's sort of the
least committed to the idea you could
still be while still allowing you to
capture these relationships so that's
the basic idea now I want to be I want
to be strict here that the mimic
algorithm or just this whole notion of
representing probability distributions
does not depend upon dependency trees
we're going to use dependency trees here
because you have to make some decision
about how to represent the probability
distributions and this is kind of the
easiest thing to do that still allows
you to capture relationships I buy that
okay and one of the things worth noting
is you OPC this in a couple of slides if
you don't see it immediately is that at
the very least this will allow us to
capture the same kind of relationships
that we get from crossover and genetic
algorithms
huh and that was a bit of the
inspiration for this that crossover is
representing structure in this case
structure that's measured by locality
and this is kind of a general form of
that so you're saying if there is some
locality then whatever however we're
going to learn the dependency tree from
the samples is going to be able to
capture that and therefore it will kind
of wrap its head around the same kind of
information that that crossover is
exploiting that's exactly right and in
fact as we'll see in one of the examples
that I'll give you in a moment that it
can do better than that because it
doesn't actually require locality the
way that crossover does there's one
other thing that's worth mentioning here
about this distribution and why it's
nice it captures relationships but the
second thing is something that you you
sort of alluded to earlier is that it's
very easy to sample from right so given
a dependency tree where each one of
these features each one of these nodes
represents a feature it is very simple
very easy to generate samples consistent
with it right you just start at the root
generate a sample unconditional sample
according to whatever the distribution
is and then you go through the parents
and you do the same thing so it's
exactly a topological sort and in trees
topological sorting is very easy and
this is in fact linear and the number of
features right I get that and it is
exactly an instance of what we talked
about when we did Bayesian inference the
thing that is new is how do we figure
out a dependency tree from the sample
exactly that's what we're going to do
next and that's going to involve math</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>