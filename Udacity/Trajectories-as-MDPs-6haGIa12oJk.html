<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Trajectories as MDPs | Coder Coacher - Coaching Coders</title><meta content="Trajectories as MDPs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Trajectories as MDPs</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6haGIa12oJk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so we can take all that and just turn it
into an MVP and it's I think it's pretty
straightforward to think how you turn
this kind of story management system
into a Markov decision process right so
what are states well we said that last
time states are just partial
trajectories or partial plot sequences
actions well they're the story actions
that the system can take your model is
still the same model that we're using
before except it's a player model that
means you know you've seen a sequence of
states or we've seen these trajectories
some action is being taken and I want to
know what plot state were likely to be
in given that the player is doing
something so it still looks just like a
transition model and then the rewards
mean exactly what they meant before
except here they're actually standing in
for some notion of what some writer of a
story thinks of as a good story and you
can somehow formalize that so that you
can plan yeah well you could but that
gets us to a whole nother discussion
about where rewards come from which is
probably a discussion that we should
have some day so maybe we'll do that
towards the end of the class but just
imagine that right now you have the
reward function the same way that you
would with any other MVP and you don't
really worry about where it comes from
it's just there okay and and be given
that these trajectories are sorry that
the states are now these full
trajectories it seems like that makes
some things harder because now there
could be a lot of them but on the other
hand you can never go back to to state
you've been in before so it's a cyclic
right so you can actually solve it more
efficiently because you don't have to
run I mean basically one value duration
and you're done well that's actually
kind of a neat that you said that so
because I was about to say that there
are two problems with this the first
problem is well now that you don't have
states but you have sequences of states
that's a lot of new states how big do
you think that is I'm going to say very
it is very very big in fact we can be
even more quantitative than that it's
actually hyper exponential it's like to
do the 2 to the N we're in terms of the
length of the sequence yes of all
possible sequences you can have the
number of states even ok yes yes ok all
right because the number of states is
sort of in factorial and all the
different kind of that we can have but
we don't all the states don't have to
actually be there and so at the end of
the day it ends up looking something
like two of the to the end it could be
really really really big so the space
that we're looking over of all possible
trajectories where things might be there
things might not be there kids
really really big very quickly so that's
one point that you said but another hand
it turns out that there's structure
there that we're going to be able to
take advantage of and in fact that's
what we do and why we're bothering to
introduce this formalism that I
mentioned earlier so let's do that
actually before we do that let me
mention one other problem with thinking
about things as Markov decision
processes and it's going to be a problem
with the strength of a Markov decision
process so what is the goal of
reinforcement learn what's the goal of
solving an MDP next might reward you
maximize reward so in fact it turns out
that if you take this sort of notion of
storytelling and how I can get someone
to do a choose your own adventure and
I've got rewards I've got some
evaluation of what best stories are
what's going to happen what does the
system going to learn how to do make the
author happy make the author happy
that's exactly right but not necessarily
make the player happen so what ends up
happening with Markov decision processes
using sort of modeling this is Markov
decision processes in the obvious way is
that you force the player to experience
the best story no matter what the player
does no matter how tries you know all
whatever choices you're trying to make
nope the system is going to make certain
that you're going to enjoy yourself damn
it and enjoying yourself means whatever
the author thinks enjoying yourself me
so like grad school it's just like grad
school no no no that's that's not right
so so are you saying that the problem
why is that a problem because the the
author I mean in a book that is
certainly what happens the author gets
to choose what you're going to
experience and that's a good argument
but if we're bothering with this whole
kind of interactive entertainment choose
your own adventure story then you if you
don't really have any choice in what you
do then it's not really choose your own
adventure story got it the basic idea
here is because there is a reward and
because all the algorithms we've ever
thought about with Markov decision
processes are about maximizing long-term
reward that it's finding an answer it's
going to find an answer is going to
force you down a particular path but it
turns out we can relax that need to find
the best answer and actually end up
solving the HyperX and there's a problem
at the same time so let's do that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>