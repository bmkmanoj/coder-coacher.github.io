<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Iterations Quiz Solution - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Iterations Quiz Solution - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Iterations Quiz Solution - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/doxTNCH7oHc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right Michael what's the answer I
think I've already made a mistake okay
what's the mistake suggesting that we do
a quiz I'm pretty sure that's always
true unless Michael by doing a quiz now
in suffering pain you in the future are
a better person in which case you have
made the right long-term decision I see
your this is an MDP meta point you're
making mm-hmm all right so let's but
let's just let's just buckle down and do
this thing so okay all right so at that
state X we have to consider according to
the equation we're gonna do u sub 1 at X
and that's going to be the reward at X
which is minus 0.04 mm-hmm
plus gamma which is 1/2 yeah feel free
to write these things down okay so let's
see it's gonna be minus 0.04 plus 1/2
times all right so now we need to do
four different actions right so I know I
would make like a bracing thing at this
point
mm-hmm not a bracket but a brace oh I
could do a bracket because you're gonna
notice immediately that it's obvious
what the right action is okay all right
well we know that the the right action
is going to be to go to the right yeah
but even then you know you don't have to
do the rest of the computation because
my first guess at all the utilities is
that there's zero which means you're
always going to want to take the action
that gets you two plus one with the
highest chance right so there's nice
okay fair enough thank you for the
shortcut so so we only have to do the
one action which is to the right and so
if we go to the right there's three
possible next States we could go in one
is back to X which has a value of zero
mm-hmm one is to the thing underneath of
X which has a value of zero and then the
last one with probability 0.8 is to go
to the plus one which is 0.8 times plus
one which is 0.8 0.8 okay and that is
okay so 0.8 times 1/2 is 0.4 minus 0.04
is point or 0.36 yep and that is correct
okay so to do the same thing for you to
we we're gonna need the u1 values for a
bunch of other states
it seems to me maybe so let's write that
down so we know that for now the the
utility here is 0.36 right yeah and
you're saying that in order to do you to
I'm now gonna have you know I was able
to avoid doing some of the math before
because these were all zeros and so I
was just easy to do but when I went
right I either stayed where I was went
to the plus one or I ended up going down
well I think by the same argument that
allowed us to cheat our way out of to
cheat our way out before okay it's still
going to be best to go to the right so
yeah I know that but the value itself is
going to depend on the value in several
other states yeah well how many other
states oh just that one just this one so
what was you one of this state I see so
presumably we want to avoid falling into
the pit so the best thing we can do is
bash our head against the wall which
would get us a minus 0.04 for that state
right okay all right so maybe this isn't
so so bad so now for our you two values
we need minus 0.04 plus and a half times
your point point one times 0.36 plus
mm-hmm
point one times negative 0.04 so that's
minus 0.004 plus 0.8 times one oh i
which is 0.8 again mm-hm just like it
was last time yep and I get 0.376
which is what I get by also getting out
your calculator okay so 0.376 and you
can imagine how we would do that on and
on I want to point something out Michael
which is that you decided to figure out
what the the true utility was for the
state under X by bashing her head into
the wall but you know that based on the
discussion we had earlier that actually
the optimal policy would involve going
up instead of bashing your head into the
wall but what you did at that point was
in fact right because everything else
the initial utilities are all 0 the best
thing you could do is avoid getting
avoid ever falling into minus 1
so the policy and the very first of
bashing your head into the wall is in
fact the right thing to do at that point
but what you'll notice is that the next
time around the utility for the ex state
is bigger than zero
in fact we'll keep getting bigger and
bigger than zero as you can see it went
from point three six two point three
seven six which means at some point it's
going to be worthwhile to try to go up
instead of bashing your head into the
wall I see so this so this is kind of
cool that it works but it does seem like
a really roundabout way of getting there
I mean is there some way that we could
some I don't know maybe take advantage
of the fact that there's not that many
policies yeah
so you actually said something fairly
subtle there so let's see if we can
unpack it so let me point out two things
which I think we'll get as to what
you're answering the first is do you
realize that the reason that the value
iteration works is because eventually
value propagates out from its neighbors
right the first time we could calculate
the point three six without really
worrying about anything around it
because the utilities are all zero and
in fact based on the initial utilities
even for this state here we do the wrong
thing but after some time this state
becomes a true representation of its
utility and in fact gets higher and
higher the right thing to do here we'll
go up you'll also notice that after
another time step I'm going to need to
start looking at the value of this state
as well right so eventually I'm going to
have to figure out the value the
utilities or the values of all of these
states and this plus one is going to
propagate out towards the other states
where this minus one will propagate out
less because you're going to try to
avoid falling in there so that makes
sense right but what's propagating out
Michael what's propagating out is the
true utilities the true values of these
states but what's a policy a policy is a
function from what to what state to
actions right a policy is a mapping from
state to action it is not a mapping from
States to utilities that's what you is
that's as you are so so if we have you
we can figure out PI but U is actually
much
more information than we need to figure
out pi if we have au that is not the
correct utility but say has the ordering
of the actions correct then we're
actually doing pretty well right it
doesn't matter whether we have the wrong
utilities I see you'll remember we did
this in the first third of the class as
well when we noticed that we were
computing in the Bayesian learning case
actual probabilities but we don't really
care about actual probabilities we just
care that the labels are right there's a
very similar argument here we don't care
about having the correct utilities even
though by having the correct utilities
we have the right policy all we actually
care about is getting the right policy
and order matters there rather than
absolutely by let me sense yeah that's
interesting it's almost kind of like pi
is more of like a classifier right it's
mapping inputs to discrete classes and
the user kind of like more like
regression where it's mapping these
states to continuous values right and
given one we can given the utilities we
can find pi I given PI there's an
infinite number of utilities that are
consistent with it so what you end up
wanting to do is get a utility that's
good enough to get you to your PI which
is one reason why you don't have to
worry about getting the absolute
convergence and value iteration but it
gives us a hint of something we might do
that's a little bit better with the
policies that might go faster in
practice so I'm just going to take three
seconds to give you an example of that
okay awesome okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>