<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Speed - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Speed - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Speed - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zKxzFl9Vl8Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so let's unpack a little bit what you
said Michael you're exactly right and
everything that you said and we can sort
of write down these things I think more
generally by simply noting a few of the
pluses and the minuses the pros and the
cons so in the filtering case one of the
things that you didn't say explicitly
but I think was built into what you said
is a question of speed so filtering is
faster than wrapping because you don't
have to worry about what the learner
wants you don't have to worry about
paying the cost of what the learner is
going to do you can basically just apply
any fast algorithm you might imagine
that takes a look at the features and
does some sort of filtering it is of
course as we pointed out before an
exponential problem but you can do some
kind of approximation in here and you
can make it basically arbitrarily as
fast as you want it to be which I think
is what you were thinking about
originally when we started the quiz so
you do get speed and the price that
you're paying for that speed is that you
tend to look at the features in
isolation so what do I mean by that well
you may look at a feature and say this
feature is unimportant for whatever
reason but maybe that feature is
important if you combined it with
another feature so maybe we're in a kind
of a parody problem again and so some
feature might look unimportant but
actually in fact it is important and the
only way you're going to get the kind of
speed of that you need is typically by
looking at features in isolation either
adding and removing them and of course
the reason it gets the speed is that it
ignores the learning problem itself at
least the learner itself is not the
learning problem by contrast wrapping
actually takes into account model bias
it takes into account whatever the
learning bias of the learning algorithm
is in order to determine what the best
sub features are which means it's
actually worried about the problem of
learning itself which I don't know how
to spell apparently but in the process
it's much much slower because every time
it tries to look at a particular subset
a candidate subset it has to run the
learning algorithm imagine this learning
algorithm is something like neural
network learning this could take
thousands and thousands of iterations
you might have to do cross-validation
there's all kinds of things you might
have to do before you even get to look
at the next thing and get a score and so
wrapping makes a lot more sense because
it takes into account the learning
problem and then more importantly the
learning bias or the inductive bias of
the learning algorithm
but it's gonna take a very very long
time to do it while filtering tends to
go pretty well you buy that yeah that
makes sense okay let me ask you a
question before we sort of move on can
you think of any algorithms that we've
looked at in the last mini course on
supervised learning that basically was
already doing filtering so it's not
quite the same because that's supervised
it now we're talking about unsupervised
but we did look at some algorithms that
explicitly decided which features to
include and which ones not I'm thinking
of decision trees maybe and I think
boosting kind of has a little bit of
this depending on what the weak learner
said is right that's true I mean we the
the problem with boosting is that it's
sort of not it's not picking the
features so much as it's picking
examples but certainly with decision
trees you're exactly right decision
trees are in its own way a kind of
filtering algorithm so this might not be
obvious and maybe I could have been
clear about it but you get these
features and I think you actually asked
me this question but you get these
features but you do know what the labels
are if there's a supervised learner
built-in if you're gonna have a learner
at the end of the day it's not
considered it's considered cheating in
the filtering case to look inside the
learner but it's not considered cheating
to understand what the labels aren't so
you can take advantage of the actual
labels of your examples in order to do
your filtering so in fact if you take
the the main part of decision trees
which is information gained that's
actually a way of doing filtering on the
features so the criterion is information
gain find me the features that provide
you with the most information given the
class label and that gets exactly the
subset cool now I don't I don't quite
see how to iterate that though so I can
see how you can get the first feature
that way then are we talking after that
we're gonna select features that add
information above and beyond what we've
already gotten out of the other features
we've selected sure so in fact imagine
that my search algorithm here is
actually decision trees so I take a
bunch of features I take a bunch of
label data I run my decision tree
algorithm it picks a subset of the
features and maybe I do pruning cross
about whatever it's just an arbitrary
search algorithm I just drew a box here
and then what I do is I take all the
features that were used by the decision
tree algorithm and I pass that to my
learner
now if the learner is a decision tree
learner I presume ibly get back the same
decision tree but maybe that's a way of
picking a subset of features that are
useful for for example a neural network
or a perceptron or canon in which case I
don't quite see how to do the feet the
filtering well you run a decision tree
algorithm here that gives you the set of
features that the decision tree thinks
is important and you return that set of
features basically the entire not the
tree but you flatten the tree and then
just turn that into a set of features so
imagine for example that we actually put
inside this box a decision tree learner
that uses the set of data as input with
all of its features and all of the
labels that it knows about it builds a
decision tree which gives you typically
a subset well in fact by definition
gives you a subset of all the features
and then the features that come out of
the decision tree are passed on to
another learner like for example a
perceptron or a neural network or even
something like came in hmm it always
comes back to cane and everything comes
back to kN
so let me make sure I understand so so
we run a decision tree learner but we
don't want to use what it actually
determined instead we say let's just
look at all the things that got split on
and the union of all those now become
features to hand off to some other
learning algorithm right but why would
we want to do that if we already trained
up the decision tree because maybe that
we were worried that the decision tree
doesn't do as good a job as you would
like on noisy data for example or you
just going to run the decision tree to
some depth or even what you want to do
is you're going to run the decision tree
until it actually over fits and now you
know all the features that you could use
and you pass that on to some other
learner which has a different bias so
basically use the inductive bias of the
decision tree to choose features but
then use the inductive bias of your
other learner in order to do learning
right and if you think about the big
difference between for example cannon
and decision trees we know that KN
suffers from the curse of dimensionality
because it doesn't know which features
are important so this but decision trees
are very good at figuring out which
features are important at least for
predicting the label and so you could
ask your decision tree to give some hint
to K n which has other nice things going
for it
which features it should actually pay
attention to cool all right so there you
go</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>