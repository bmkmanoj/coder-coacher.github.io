<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What Makes RL Hard | Coder Coacher - Coaching Coders</title><meta content="What Makes RL Hard - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What Makes RL Hard</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ouzkWSozURY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so in order to elaborate on all of that
Michael um I want to ask you a simple
question I mean here it is what makes
reinforcement learning hard I'm gonna
say living on the streets makes
reinforcement learning hard yeah I'd sir
hardens it let me uh let me ask that
question slightly differently oh okay
from a computational point of view from
an algorithmic point of view what makes
me enforcement learning hard so well
there's a lot of things we've been
talking about there's the notion of
delayed reward is a very tricky thing oh
I like that one you know I like that one
enough I'm gonna write it down you're
not gonna wait no I'm not gonna wait I'm
just going to write it down right now
because it's ironic that way and related
issues well there's two pieces of that
there's the delay in this reward the
reward part is like unlike say
supervised learning reinforcement
learning is trying to do its job based
on very weak feedback that it's only
told how it's doing and not necessarily
what it should be doing right which
makes it very different from normal
function approximation in the way that
we think about in supervised learning
okay what else if we do try to use
something like supervised learning one
thing that makes the function
approximation hard is the idea that it's
kind of a moving target we tend to to
base our estimates on our current
estimates as opposed to anything kind of
legitimately valid in the world right so
that's doing estimates based on
estimates as well like averaging
averages ah okay i want to say
bootstrapping is the issue um yeah I
agree with that bootstrapping is sort of
a problem i think i would probably
summarize that as the problem with the
need to do good exploration and that's a
little different from the normal
supervised learning setup where you've
got a bunch of training examples here
because you only have estimates in the
reinforcement learning context you have
to do a bunch of extra things to
actually figure out how to get good
estimates so that you can actually do
good learning so one more thing I think
sort of worth mentioning that makes this
sort of hard it's kind of very basic
kind of computational problems right you
know you know when we describe
computation complexity we talk about it
in terms of Big O and the question is
big-oh of what mmm so what's the what
here what's the what's the in in our big
oh that makes our
or well there's a lot of them like what
well I mean if we're thinking about sort
of mdp style reinforcement learning then
the size of the state space is it is it
big o the the size of the temporal
window in some sense like how how
delayed is the reward there's issues on
what could be the actions as well
there's lots of things that could scale
but I guess I guess what makes state
super hard is that you can very easily
specify a problem that has a state space
with an exponential number of states
that is still relatively simple but
algorithms that don't generalize between
states are not going to be able to learn
in them very effectively I think all of
what you said is true some of it i think
is kind of captured that I so explicitly
in the first two bullet points but I'd
like to focus on states and actions that
the problem is that the learning that we
need to do grows super linearly in
general we aren't very very careful in
smart in the number of states and the
number of actions and so we really need
to have kind of clever ways of thinking
about learning across multiple states
and perhaps multiple actions and in fact
when you talked about function
approximation and generalization I would
claim that at least in sort of this
discussion we could talk about that as a
way of dealing with generalizing over
states that you did function
approximation over value functions right
so here you try one way of dealing with
the problem of scale when it comes to
states is well if I learned something
about one state I want to somehow have
that teach me something about a bunch of
other states and then I won't have to
visit every single state you know an
infinite number of times and take every
action an infinite number of times and
that's really good that's really
important right and the trick here is I
can see many many states by just seeing
a single state and that's what function
approximation over the value functions
actually gives me agreed yeah I don't
know that I would say quite that way but
that we learn about many states when we
learn about other states or something
like that but yeah no yeah yeah that's
absolutely the way I was thinking about
function approximation is helping us to
kind of fill in the gaps for states that
we haven't visited nearly as often so in
fact I could be aggressive and ask you
the question why did you do all this
function approximation over States but
you didn't do any function approximation
over actions
Michael that would be aggressive and
they ended I don't have a wonderful
answer that I mean that's it's you know
tradition in the sense that a lot of the
MDP is that people have been working
with really do try to keep the action
space relatively small and expand in the
state space but that's not really that
good of an answer I think action large
action spaces are really important to we
just don't have as good ways to deal
with them for one thing the computing
the max ends up being hard in the in
q-learning right so in fact what I'm
going to try to talk about in the rest
of the lesson is trying to do
generalizations of the sort of
generalization that you were talking
about and think about how we can learn
about actions and how we can abstract /
actions in the same way that you were
abstracting over States oh that seems
great and a little bit meta a little bit
meta and so that's why the lessons
generalizing over generalizing and the
whole point in the end I'm going to
claim other than just sort of it's
interesting in and of itself is that
it's going to allow us to scale so all
of these tricks ultimately have a kind
of practical use including function
approximation of allowing us to scale</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>