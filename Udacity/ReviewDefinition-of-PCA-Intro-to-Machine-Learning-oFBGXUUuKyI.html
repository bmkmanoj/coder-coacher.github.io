<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Review/Definition of PCA - Intro to Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Review/Definition of PCA - Intro to Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Review/Definition of PCA - Intro to Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oFBGXUUuKyI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">If you're still with me at this point, great job.
I think PCA is one of the trickiest topics in machine learning, so
if you're still sort of struggling to wrap your head around it, don't worry.
That's normal.
So let me take this opportunity though to just review PCA briefly at a high
level and give you kind of a working definition that you can use in the future.
PCA is a systematized way to transform input features into their
principal components.
Then those principal components are available to you
to use instead of your original input features.
So you use them as new features in your regression or classification task.
The principal components are defined as the directions in the data that maximize
the variance, which has the effect of minimizing the information loss when you
perform a projection or a compression down onto those principal components.
You can also rank order the principal components.
The more variance you have of the data along a given principal component,
the higher that principal component is ranked.
So the one that has the most variance will be the first principal component,
second will be the second principal component, and so on.
Another thing that we should point out is that the principal components are all
perpendicular to each other in a sense, so
the second principal component is mathematically guaranteed to not overlap at
all with the first principal component.
And the third will not overlap with the first through the second, and so on.
So you can treat them as independent features in a sense.
And last, there's a maximum number of principal components you can find.
It's equal to the number of input features that you had in your dataset.
Usually, you'll only use the first handful of principal components, but
you could go all the way out and use the maximum number.
In that case though, you're not really gaining anything.
You're just representing your features in a different way.
So the PCA won't give you the wrong answer, but it doesn't give you any
advantages over just using the original input features if you're using all of
the principal components together in a regression or classification task.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>