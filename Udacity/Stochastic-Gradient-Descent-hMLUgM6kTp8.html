<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Stochastic Gradient Descent | Coder Coacher - Coaching Coders</title><meta content="Stochastic Gradient Descent - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Stochastic Gradient Descent</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hMLUgM6kTp8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the problem with scaling gradient
descent is simple you need to compute
this gradient here is another rule of
thumb if computing your loss takes n
floating point operations computing its
gradient takes about three times that
compute as we saw earlier this loss
function is huge
it depends on every single element in
your training set that can be a lot of
compute if your data set is big and we
want to be able to Train on lots of data
because in practice on real problems
you'll always get more gains the more
data you use and because gradient
descent is iterative you have to do that
for many steps that means going through
your data tens or hundreds of times
that's not good so instead we're going
to cheat instead of computing the loss
we're going to compute an estimate of it
a very bad estimate a terrible estimate
in fact that estimate is going to be so
bad you might wonder why it works at all
and you would be right because we're
going to also have to spend some time
making it less terrible the estimate
we're going to use is simply computing
the average loss for a very small random
fraction of the training data think
between one and a thousand training
samples each time I say random because
it's very important if the way you pick
your samples isn't random enough it no
longer works at all so we're going to
take a very small sliver of the training
data compute the loss for that sample
compute the derivative for that sample
and pretend that that derivative is the
right direction to use to do gradient
descent it is not at all the right
direction and in fact at times it might
increase the real loss not reduce it but
we're going to compensate by doing this
many many times taking very very small
steps each time so each step is a lot
cheaper to compute but we pay a price we
have to take many more smaller steps
instead of one large step unbalanced
though we win by a lot in fact as you'll
see in the assignments doing this is
vastly more efficient than doing
gradient descent this technique is
called stochastic gradient descent and
is at the core of deep learning
that's because stochastic gradient
descent scales well with both data and
model size and we want both big data and
big models
stochastic gradient descent SGD for
short is nice and scalable but because
it's fundamentally a pretty bad
optimizer that happens to be the only
one that's fast enough it comes with a
lot of issues in practice</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>