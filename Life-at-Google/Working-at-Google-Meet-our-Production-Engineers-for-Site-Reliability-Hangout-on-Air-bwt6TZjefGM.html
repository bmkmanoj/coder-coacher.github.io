<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Working at Google: Meet our Production Engineers for Site Reliability Hangout on Air | Coder Coacher - Coaching Coders</title><meta content="Working at Google: Meet our Production Engineers for Site Reliability Hangout on Air - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Life-at-Google/">Life at Google</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Working at Google: Meet our Production Engineers for Site Reliability Hangout on Air</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bwt6TZjefGM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey everyone my name is Andrew Widdowson
and I'd like to welcome you to the
Google Hangouts on air for site
reliability engineering or SRA I'm
joined today by a panel of experienced s
eries including Daniel Dez Melissa and
Joel and we'll do introductions in just
a moment some things we're going to talk
about today include what exactly is site
reliability engineering a day in the
life of sre comments on project work and
relations with developers and Google as
a whole and we'll also talk a little bit
about the new book that we just
published with the O'Reilly but let's
first start off with some introductions
let's start with Melissa either I'm
Melissa bindi
I spent a decade at Amazon and then did
Nordstrom and a bunch of startups many
of which failed and I joined Google
about a year and a half ago I'm the
director of storage oh sorry
so we do databases file systems backups
basically if it involves stateful bits
at rest we're handling it cool it does
why don't you say hello hi I'm Dez King
I work in the advertising cycle
liability engineering team here at
Google I've been here for just over a
decade originally I studied English
literature taught myself computers
worked at a university and small Irish
company before moving to Google
cool let's have Joel say hello hi my
name is Joel Becker I started my career
at IBM doing a large-scale system
administration at the time of mine UNIX
systems I then spent eleven years at
Oracle as a Linux kernel engineer
working on file systems in storage and
for the past five years I've been at
Google working as an SRE in the web
search team this is the serving of
google.com which hopefully all of you
are using to get your search results and
last but not least Daniel hi I'm Daniel
I'm three years at Google I currently
work on the SAE team for Google's
corporate infrastructure before that I
worked for several years and then
several different IT roles at the United
Nations cool and I forgot to introduce
myself as a nine year member of Google I
work with Joel on web search and I also
work on the uh sorry
edu project which is the educational
group that ushers folks through the
weirding ways of sres in their first
several months and indeed their whole
careers and site reliability so speaking
of site reliability why don't we get
things kicked off with an understanding
of just what exactly is this thing well
Melissa what's your take on what site
reliability engineering means for you
sure thing
I am so I come from a deaf background
and what struck me most when I got here
was that there were an awful lot of
people like me and essary we were all
devs but were the devs who sat there and
went really there's got to be a better
way to do this or seriously like you
guys haven't automated that yet or the
people who care about reproducible
builds or who look at the design and say
that's neat but the second we hit 10x
traffic it's just going to all fall over
we're a very small fraction of
engineering here there's tons more devs
in there s eries that makes us kind of a
special precious commodity that we apply
to the hardest problems that we have
people sometimes characterize SRA work
as firefighting but what I've seen in my
time here is that it's actually a lot
more methodical where yes we need to get
the service back but then so much of our
work is actually figuring out why it
happened what the root cause was and
what steps do we need to put in place so
next time we detect it sooner and it
actually ideally doesn't happen again
absolutely we're
we're pragmatists were realists and and
we're also a scarce scarce resource
so with that scarcity comes a lot of
clarity around how to make a system
better and I'd love to hear also Joel
your thoughts on what site reliability
engineering means to you in general site
reliability engineering is a peer and
engineer group whose expertise is
different meaning your your product dev
is trying their very best to write the
best algorithm of the best feature and
the best product and that's their focus
and some of them have expertise in how
production systems work instead of them
don't our goal is that most of them
don't have to worry about it too much
because that's where we come in our
expertise is in the engineering of
reliable production systems so we might
write software for it you might read
manage it we might emergency debug it
but at the same time where it were peers
with the devil in ization and we
combined to create the reliable and
awesome products you use I think we have
cooperative incentives in a way we all
want to make the product the best it can
be but it comes out in different ways
whether it's performance reliability
scalability are some of the tenants of
SRE
right in addition to feature full
products wonderful experiences from our
developers and those things aren't
actually in conflict if you've set up
the systems right I think it's it's
actually quite fun to think of a world
where a very real world here at Google
where site reliability engineers will
often say to our developers let's take
on more risk let's try more things
because we have safeguards or
instrumentation or platforms in play
that allow us to take on that risk but
perhaps I'm getting a little bit ahead
of myself thanks for your definitions of
site reliability engineering Joel and
Melissa let's put some more concrete
hooks into this I'd like to hear about
challenging yet typical SOE projects
that some of our panelists here have
worked on and how they went let's start
off with Daniel can you tell us about a
challenging project you worked on sure
at Google we provide a virtual
workstation service for engineers and we
were faced with the challenge that the
infrastructure was not as efficient as
it could be so we looked at the problem
and we designed
really next generation type virtual
workstation service with that it was it
was a challenging project because it the
the kind of scope ranged from kernel
level software engineering to network
design hardware evaluation and selection
and the the actual logistics of getting
the hardware deployed and all of the
locations where we needed security
reviews around the whole thing and so
that were like about a dozen different
teams involved and many more actual
individual engineers and getting that
all coordinated and deploy it that was a
challenging project but in the end we've
been successful we are able to improve
the efficiency by at least a factor
seven so we are providing a better
service for engineers and safe Google a
lot of money and I think it's really
cool Daniel you work in a space that's
somewhat internally facing but your
customers are other other Googlers and
together you helped make many developers
and other Googlers experiences more
efficient but it's the same principles
at play that we have for external
products as we do for internal products
and I think for a concrete example of a
externally facing product that that we
work on that has similar s3 principles
at play does you want to tell us a bit
about some of your project working ads
yeah sure I think continuing the theme
of efficiency over the last number of
years we've got a lot of different ads
products a lot of users around the world
when this products need to keep scaling
efficiently to both the growth in users
and the kind of cool features that we
want to ship and make available and one
of the things that we were finding is
that the the amount of you know machine
resources CPUs that we need to keep
scaling to all of the different product
challenges we're kind of being organized
a little bit of a sort of siloed nature
each product looking after their their
own growth needs and so I was able to
work with our compute infrastructure
team who did a lot of interesting
statistical modeling on how we could
reclaim resources that we thought were
mitad and used and we could actually use
them for for for other other teams to
use and it was a project that that was
both working with the core
infrastructure statistical modeling the
compute scheduling system working within
the SV team developing software to do
capacity forecasting modeling risk
management of how we could trade off
previously 100% guaranteed resources to
maybe 99 98 percent guaranteed resources
now put them together into a run time
production service that we could run
24/7 that we could build Google's
advertising products to build on this
piece of machine management and just a
really cool kind of cross-functional
product that kind of cuts across all the
diverse sort of skill sets that site
reliability engineers bring to bear I
think it's pretty amazing how there's
this tremendous breadth of experience
and knowledge that one will end up
tapping into and working on projects in
sre we aren't necessarily typical coders
if there is such a thing at Google right
we're if you ask people within your work
group how many people know about
statistics or how many people know about
capacity planning or forecasting or
algorithmic runtime analysis or this or
that right you'll get different people
raising their hands at different times
and together I think we make a very
interesting team who can make our
products better I'll share a story as
well about a challenging project that I
worked on for Google for search so one
of the teams that I have led in recent
years works on fighting abuse of the
Google web search stack we're talking
about people who are trying or robots as
the case may be trying to copy wholesale
our index or running interesting
side-by-side experiments trying to
reverse engineer and game the system to
perhaps the detriment if done if the
externals do this successfully the
quality of search results for our users
so together with a band of SRO we wrote
a analysis pipeline that looks for
anomalous robotic traffic rather than
users in order to challenge and dissuade
this traffic from affecting our quality
results impacting our capacity plan or
causing us to spend money on things that
don't benefit real users
so here I was able to work on an
analytical project that was more
pipeline oriented in nature even though
my day job is about a real-time serving
stack I thought that was a pretty neat
way to to solve some of our user woes
and make things better
so these are all examples of ways that
we've worked for the reliability the
stability and the quality experience of
our stacks and domains at Google Joel as
an individual contributor at Google can
you tell me a little bit about how much
time you spend on coding because I have
this latent fear or perhaps some of our
audience might that's we don't get to
spend much time on project encoding can
you can you talk me out of this and tell
me why you do a lot of coding yeah
absolutely
a fundamental tenant actually of sre the
big e engineering in the name is that we
expect to send spend at least 50% of our
time on project work that is the toil of
fighting fires or rolling out new
versions of software or any of this
stuff should be less than 50% hopefully
much less than 50% we all want to have
the fun of writing new things and so
often our projects are tasked with
reducing that toil and creating more
project time for me I spend at least 70%
of my time in development projects an
older example we used to generate
service reports that would take
statistics about how well we serve their
results to users and come up with
reports so we could say hey we met our
requirements or not and so on and I
wrote a an analytical pipeline that did
statistical analysis and data collection
to produce these reports automatically
which meant that instead of spending you
know half a day or a day of recorder
writing it myself
some software did in the background and
that saved me time I could work on other
projects and today my main project is a
visualization system for monitoring for
data that we see about the runtime of
our systems and as technically to that
project I'm focused on that almost
exclusively outside of my day-to-day
responsibilities for search team well
thank you Joel
I just like to point out while we're
here in the midst of this car
that there is a question and answer
feature built into hangouts on air and
many of you are already starting to post
and meta moderate questions please go
ahead throw some questions up there for
us we'll take a few while we're doing
this chat and feel free to upload and
download ones as you see fit more from
that question pile in just a moment so
while we're talking about project work
and what exactly is sre and what sort of
mental model can I put around this to
understand how it may be different from
other experiences we've had in the
industry a question that I know many
people have on their minds is how is
this different this site reliability
engineering thing how is it different
from DevOps and Joel I'd like to throw
this back to you to get your first take
I think there are many takes on this so
yeah I'm happy to answer that as I
mentioned before we try to write
software to do things that we would be
doing repetitively the catch phrase is
automating ourselves out of a job right
if there's a piece of work we can make a
computing write software to perform that
gives us time to work on more
interesting and bigger and more
challenging problems that's awesome and
a lot of people are picking that up and
a lot of DevOps is really about that you
know it started way back when when sis
admin's would do scripting of repetitive
tasks but now it's turnkey systems that
do the work for us behind our back
without without a human hand in the
process this is excellent
but this isn't the entirety of what we
do we take this beyond first off by
thinking how we can reframe our systems
so that they become more better
candidates for this kind of automation
also as I mentioned earlier in this
conversation we bring this expertise and
the structure to our product developers
we can say hey this approach you're
going to build is going to take too many
resources or it's not going to be easy
to automate as we're part of the
architectural discussion early on and
system system design we're not reacting
we're not taking pieces of someone
else's software we're actually a
co-authors in that sense and this
actually goes sometimes to the extreme
that some asourian software engineers
write code directly in our products
right there is code in Google web search
in Google ads in Gmail the
written by an SRE who saw a scalability
problem or even a feature and because
we're pure engineers we just wrote the
code made the change passed along and
our friends over in product said oh
that's an excellent change and so we're
part of that conversation too Thank You
Joel now we have a question from our
audience today from Daniel Shaw who asks
the following which I'd like to pose to
Daniel from Daniel - Daniel the question
from Daniel is I understand that you are
capped at 50% direct support to ops
is that correct if it is how do you set
up the workload to prevent running into
that constraint and what happens if an
unplanned event forces you right up
against that threshold right so we we
look at that 50% number as a not on a
daily time scale right so we we measure
that usually on a quarterly basis so a
one-time event may bring us close to
that but for every large-scale incidents
we do something what we call a
post-mortem where we really strongly
look at the root cause and take action
to prevent this particular class of
incidents from reoccurring and also
extracting all the lessons that can be
learned and specifically in a blameless
sort of way so everybody can contribute
it so that's one important thing that
helps us to not having reoccurring on
incidents like that to consume all of
that operational budget that we have the
other factor that we have is that we
watch and and as Joel said earlier right
so the 50% is kind of the upper limit so
if there is a team which consistently
over a certain period of time as close
to that limit that already signals
something is not right here and then we
take a detailed look at the situation of
the team why might it be so close to
that threshold and what can we do about
it and measures that that we can do to
combat this is to have a conversation
with the product engineering team to see
ok maybe this service is not stable
enough there are too many incidents of
different kinds that cannot or all the
root-cause mitigation takes too long to
take effect so maybe we need to reduce
the production level of that service or
loosen the SLO or anything like that
to prevent the or or prevent that high
operational work load on the SOE team
and then the product engineering team
needs to pitch in and help us that so
these are some of the measures that we
can do to prevent it from happening I
think we're very self aware of the the
inevitable creep of ops load and what
can be done impetus to against it now
Melissa I know you have some thoughts in
the space - yeah I do the the actual
ultimate escape valve for us is that the
dev step in and take burden if the SRA
has too much so I'm you know it's
important to remember we have entire
devil orgs dedicated to these products
and there are not SRA teams without dev
teams so when you do have too much of an
ops load or too much of a pager load
that's where the dev team steps in picks
up tickets picks up pages so while
you're doing the long-term fix they're
also helping with short-term tactical
fix so you don't have a team that's just
drowning an ops workload and can never
get itself out absolutely
I'm going to take another question here
from the audience before we continue on
so the question comes from Steven
trombetti who asks are all SRS on call
and what is an on call incident like and
I'm going to start off with my favorite
answer to this and then we'll see who
else wants to jump in many but not all s
eries are on call it there are a couple
of horizontal functions within essary
where we have a few folks who may not be
on call but are in a launch gating
launch vetting and consultation sort of
role but let's let's go ahead and say
that of the systems and software
engineers we have an SRE that 99% of
them go on call for me when I think
about an on call incident I get a page
and I'm responding to perhaps an outage
that impacts users high latency
connectivity issues machine
misconfigurations fleet-wide issues what
do we roll back what do we do in order
to contain a problem the hyperbole laced
phrase I like to use for
site reliability engineers is that where
scientific we're stewards of the
scientific method in a pressure cooker
so you know time is money
right time is user loyalty and we need
to be able to mitigate issues we need to
be able to contain damage and route
around things luckily in the era of the
cloud you can have intelligent
provisioning you can have the ability to
do failover you can drain things you can
you can cache things you can do many
different things to segment off a part
of maybe your physical infrastructure
that's having a problem or through
release management fast roll backs and
having the the faith in a system where
you can roll back just as fast as you
can roll forward earning thus your right
to roll forward with your developers you
can put your users to safety as soon as
possible and then still have the luxury
of being able to reproduce a test case
of the outage I come from a real-time
user-facing serving stack sort of world
and my brothers and sisters in pipelines
and infrastructure infrastructure stacks
would have perhaps different stories to
tell but ultimately for my situation I
look at it as a little bit of an
adrenaline rush sure but that fades very
quickly and the most important thing to
me is the analysis of an open problem
finding symptoms working backwards to
root causes and convincing myself and
the stack and the development team that
the solution we have architected to
prevent this from happening again is the
right fix proportionately to save the
day or the week or the release its it
can be quite fun once you get used to
thinking statistically and
differentially about your fleet and your
stack so thanks for that question did
what anyone else like to jump in on that
otherwise I think we'll go on to another
question here so so we've talked a bit
about some differences between the
DevOps philosophy and sre but
realistically people are coming from
many different backgrounds and so I'd
love to ask some of our panelists about
the differences they've seen between
essary at Google and just other roles
that they've had previous previously
does why don't we have you tell us a
story about some differences yeah I
think looking back at some of the roles
I had prior to Google not uncommon to
many as reviews I work in and
some ISPs over the years I'm thinking
about the differences I found coming to
Google was you know the scale of
investment you know Joel talked about
you know developing kind of long-term
production infrastructure to tackle some
of these problems and that was really a
hugely welcome change from working in
aspies where everybody's instincts you
know were to do that but the kind of
time investment even even the machines
to run sort of production the
infrastructure automation software it
just it just wasn't there there was
always another project another launch
and so you know we felt we were
constantly trying to kind of script her
way around just to keep the lights on
and and move forwards and I think the
other thing I found that that a number
of the other folks have talked about is
that that sort of sense of one large
team with without such strict
organizational boundaries so so thinking
back to my times in some previous roles
where the team organizational boundaries
were a little bit more static that that
you know a different team controlled the
design they didn't sort of necessarily
want to participate in kind of
collaborative design or discussion
especially for example if you've got a
commercial vendor solution where there's
a very rigid difference between sort of
how you can influence that software
design you know make contributions as
Joel was saying directly to improve the
design and the quality I'm not that bad
I fans are so liberating at Google that
people were so open to the
collaborations and wanted to invest in
sort of the quality of the production
experience of the users and also of
Google engineers ability to kind of
sisty in building these products that
wasn't held up by sort of the the blood
of human toil that's actually a really
interesting thing we passed around here
the idea of not feeding blood to the
machines the fact is that as probably
everyone watching knows computers can do
many things much faster than humans can
do and if you have a process this gated
by a human typing a command reading a
script clicking a button not only are
you opening yourselves up to human
failure but just going a lot slower
then you could be and so when we talk
about automating ourselves out of a job
which I used before this is another way
it benefits us absolutely I've also
heard it said as no heroism if we have
if we have a human operator working on a
process that is laden with toil and they
say oh it's okay I'll take this on I
mind it less than some of my co-workers
or I want to be the hero that makes this
go away by throwing myself at it that's
not sustainable s3's are the kind of
folks who want to as you said automate
themselves out of a job or more
importantly lift their co-workers up
around them so that we can go on to
doing more interesting future projects
and and Melissa now also you have a lot
of industry perspective how does Google
essary vary from other experiences
you've had yeah sort of two main ways
the first one is control so because like
I'm very used to other environments
where we get just handed something at
the end and we're supposed to figure out
how to make it work and here we actually
have a lot more control over our destiny
because we work with the devs early on
we also report to a different management
chain than the devs do which may mean
less to those at the start of their
career but folks who are used to working
the industry longer knows that actually
reporting relationships matter a fair
bit so having a different VP means we
actually have like our own so that we
can set our own goals we can set our own
priorities and we can make sure that
reliability is a top one and not just be
at the sort of whims of product there's
also a key Google principle that helps
keep managers like me honest which is
that anyone can transfer away at any
time for any reason so if I'm not making
good and this includes from essary to
dev and back so if I'm not keeping good
environments and my teams I will
literally lose all of my s Ari's and
this also helps keep Deb's skin in the
game the devs have a deeply vested
interest in making sure the software is
something that the s-series want to
support because otherwise the s-series
will transfer away and I've had a devil
director come to me and go you know so
someone's so transferred what happens if
more s-series transfer I said that's a
great question maybe we should talk
about the quality of the software a
little bit so I love having those
conversations I love having them care
um just as much as I care about how
happy my team is and finally most places
the the sort of operations and the
production of the business is a cost
center you want to minimize it as much
as possible because they're not actually
producing more new product they're not
actually getting you new customers here
because our focus is unreliability it's
not just about reducing downtime it's
also improving performance and
efficiency so we literally are saving
like massive amounts for Google when we
go and make those changes make those bug
fixes and it allows the team to actually
be really a part of things and not just
a cost center that we want to have the
minimum amount of good stuff right I
think all this helps put site
reliability engineering in perspective
now we're coming up on about 26 minutes
indoor chat and I'd like to make the
overall format of the main talk be about
a half-hour in length but because we
have such great questions and we
promised that we'd handle those as part
of this chats we'll make sure that we
attack those onto the app so stay tuned
if you have one of these top questions
we're going to answer them right after
this last little bit but first some
parting shots so I told you earlier that
Google is very pleased to announce that
we've published a book with O'Reilly
about how Google runs production systems
you can find more information about it
on our website about SRE which is at
google.com slash sre so what I'd like to
do is I'd like to ask a couple of folks
around this panel here what are your
favorite chapters from the book and why
so I'll start I wrote a chapter so
obviously that would be my favorite but
skipping over that there's a great
chapter about managing risk and this is
a really cool one
because we talk about as some of our
members of our audience are also asking
about things like failure budgets
consider a service with a number of
nines right if we say a service is four
9s it's supposed to be up 99.99% of the
time should that's what we're shooting
for that's what we're measuring that's
what we're maybe contractually talking
about with other people with financial
penalties about right but here's the
thing what happens with the a hundred
percent minus ninety nine point nine
nine percent of the time you can you can
budget for failure to some extent but
here's the thing if you're tracking how
you're doing and your
exceeding that consider that the
downtime is a risk budget that you can
take on you could say you know what
we're being too stable
we're being perhaps too conservative
with our stack and this is where things
get fun and I make the crazy face into
the camera of can you imagine that folks
who liaised erect Li with development
teams might be the ones insisting that
we take on more risk that as a principal
fact can be site reliability engineering
so I'd encourage you to take a look at
that chapter in the book Melissa what's
your favorite chapter from the essary
book my favorite is absolutely the one
on blameless post-mortems I've seen so
many again I mean I've worked about 20
years in the industry and I've seen so
many times where you don't have the
information you need because someone's
hiding it because they're afraid of
getting in trouble and it sounds real
easy oh yeah oh just right a blameless
postmortem even essary so we've been
here for 10 years sometimes struggle to
write a properly blameless postmortem
but the the commitment to that where you
review it you make sure it's actually
blameless you focus on getting like
getting the the technology fixed and not
blaming the people for technology
problems just so key to a place that I
want to show up at work every day very
cool definitely take a look at that
chapter - Daniel what's your favorite
chapter in the book so for me it's
eliminating toil it's a relatively short
chapter but it's really to the point in
my opinion so it starts off with giving
a very comprehensive definition of what
we mean by toil and I think that
captures very well all the aspects so
specifically if you're a manager in sre
you want to make sure your team is
healthy and every individual is really
comfortable with the way how they're
working and having just like this kind
of gut feeling of year there are
repetitive operational talks sometimes
not always captures it so that's one
thing that helped me at least a lot to
be mindful about what could be
considered toil by the team and not
where can we sort of improve by trying
to eliminate that that chapter also goes
into quite detail about what are the
risks associated with doing wholesome
tasks and and even though there are some
benefits to
tall work and in in all with all things
the healthy makes is the right thing but
as we talked about earlier right so the
50% threshold is really the upper limit
and most healthy SRU teams are a
significantly lower than that and for me
that's that chapter Chester call great
and I know that both Dez and Joel were
arguing over what their favorite chapter
is and I will bestow upon Dez the honor
of describing their favorite chapter so
collectively after managing risk which
was a very close close runner-up
I think handling overload has been
something that really has has featured a
lot in my site reliability career here
we've you know had had the great
experience of working with the core
infrastructure teams improving the load
balancing systems working across site
reliability engineering on common
software infrastructure to both manage
throttling of overload situations
handled client-side logic of triggering
that's that our essary teams integrate
directly into the most critical serving
systems and processing systems and can
do it at a scalable way across the
organization and I think when you're
trying to build really high school
systems that are subject to you know
many many populations of users outside
your control doing all kinds of exciting
things with your product that maybe you
didn't think about the ability to be
able to degrade user experience
gracefully rather than have an abrupt
pour experience at a certain point of
load or or incoming requests really is a
critical part of building robust
distributed systems well so we have many
favorite chapters amongst us again if
you're curious about reading these
chapters take a look at the book titled
site reliability engineering how Google
runs production systems and with that
it's been about a half hour into our
video and we'd like to thank everyone
for sticking it out and watching through
this portion of the talk we're going to
go into questions from our audience our
virtual cohort of folks
so we'll do our best if you are watching
this video after the live broadcast to
try to put some time offsets in the
commentary so that you can skip around
to questions that are of most interest
to you I'll start off by answering a
question from Chris
Danine and perhaps my panelists can take
a look fish around through the questions
and find their favorites that they'd
like to answer as well but from here on
out it's a bit more freeform and
interactive through the questions which
are through the side panel of the
hangouts on air so the question from
Chris which I'm enthusiastic the answer
is what is the SRA onboarding process
like well thanks Chris your checks in
the mail I run along with the cast of
fun educational facilitating characters
a program called sre edu and really it's
our job to make sure that new
assessories of the company hit the
ground running with as much information
as they need and no more to get through
their first several months at Google and
then we circle back with a series and
other production interested Googlers to
give them additional bread classes depth
classes training about best on-call
principals just before they go on call
we go through several different pillars
of Google's production technology we
have hands-on live fire drill types of
examples don't worry it's a safe
environment in which you can cause
things to explode and then diagnose and
understand how to make things better
we have conversations with developers
about how the best on board new features
coming up soon we have in-depth talks
about analyses and how to divine fact
from fiction in the production
environment in order to hasten the
recovery of a production stack all of
these things are available on top of
Google's already I think quite good
orientation program we have for our
general engineering environment so we
try to custom tailor this as much to
essary and we're very excited to bring
in each graduating class of new s eries
through our program so for more
information about this and kind of
Google's principles on onboarding new
site reliability engineers you can take
a look at my talk from the using
accessory kahn last year where i give a
talk called from zero to hero
recommended practices for training you
our new site reliability engineers and
now that I've stalled with enough time
thank you Chris with that question I'd
like to open it up to some of our
panelists to answer some
their favorite questions from our
question list would anyone like to jump
on a question here and answer it for our
audience Joel you unmute it first
sure so Chris Danine asked the question
coming from a systems engineering
background what level of programming do
s eries need and the answer is as always
it varies we actually care a lot that we
have some people who are really good at
systems engineering and we have some
people who are really good at software
engineering and we don't believe nor do
we ever see that you can get everybody
to be really good at both and that's ok
we are one of our initial founders of F
sorry Ben trainer has something he calls
the trainer curve which I think we
described in the book which basically
says that on one axis the vertical axis
you have skills and essences edge and on
the horizontal axis you have people who
have sweet skills that is software
engineering and if you drew a curve
between the good enough points on both
axes you would find an area where
someone's plenty good enough as a
systems engineer but just so-so as a
software engineer and vice versa and we
actually find the mix of projects we
have makes these people really really
powerful so if you've got a really
strong systems engineering background
come join us
maybe even learn some more software
engineering and if I really want to do
some software engineering on large-scale
systems come join us
we'll probably teach you some systems
engineering along the way ultimately
Joel I would even go so far as to not
label it as so so in one axis or the
other we're really looking for people
with an open mind even if their industry
experience is strong in one of those
axes versus the other we're looking for
people who can think their way through
and learn with the heavy support and
encouragement of their fellow teammates
how to become a broader engineer in both
buckets if I came at you certainly with
the software engineering experience
rather than the systems engineering
experience and I feel like I've gained a
lot out of this experience
yes to be clear I wasn't speaking of
inherent talent it's just some people
have worked hard well you know spent
more time in one aspect and have more
and expertise cool I think what Andrew
just mentioned that we're looking for
people with an open mind also helps to
answer the question from Stephen
trombetti he asks what's the breadth of
knowledge of an S series since the role
spans many level from hardware and
software suited for a jack-of-all-trades
but also may require great depths that
require specialization and as we just
talked about what we are really looking
forward Google are people with an open
mind who can learn and yes the kind of
landscape of systems and software and
technology that we use at Google is far
more that any individual could be an
in-depth expert in in their lifetime so
when when people join us we want them to
be able to have a strong foundation
maybe focus on software engineering or
system engineering or in the with a
healthy mix of that but then through the
onboarding process and when once they
start to be assigned with a team
which should be a team that suits their
personal preferences and their skill set
then they will get all the opportunity
to learn all those necessary with their
team and even if that is in one
particular technology area if at one
point in their career they want to try
something else
that's perfectly possible and we give
them all the opportunity to learn and
start from scratch not entirely from
scratch but sort of getting to do to
know the new environment for them so
maybe I could continue that on with
Edwards question asking to a series of
much control over their own project
assignments you saw we've heard we need
a diverse set of skills to make up the
the the types of experience that you
need to bring to different types of s3
projects many people from you know
diverse learning backgrounds diverse
thought processes and the diverse
backgrounds outside there they're kind
of learning sort of experiences anything
when you take that to the all of the
different types of projects that an
asteroid team is typically responsible
for
both team members from from all of those
different viewpoints are observing what
goes wrong with the system what the
product is evolving into and what it
might need us to be focusing on and
identifying their own projects for
trying to persuade their teammates that
that's that's the project's that that we
should be working on in the coming
months and when we try and match those
projects up with the type of skills or
interests or challenges that that that
particular project will bring we need
that pool of people from all of those
different kinds of backgrounds and
skills to match up to the diversity of
the challenges that we have and so I
think you know if you look at our
process it's around team members taking
those experiences running the product
identifying projects that they they
think are the most important you know
and convincing people to take to come on
board and work on that good stuff I'm
gonna jump in and answer a question here
from Morgan Reese who who posits the
following working across teams essary
seems to be in a great position to
disseminate or unseal Oh useful
information and knowledge for example
Team X should leverage some tooling the
team Y already built for themselves how
do s eries spread information amongst
themselves I think this is exactly at
the core of a lot of what s3 does so we
are ultimately very well connected with
in and around and with the development
communities at Google and the different
products case in point within search
where Joel and I work depending on what
you count as a demon or a binary or a
piece of componentry you could easily
argue that there are fifteen to thirty
different components that are major and
complicated and have their own
development team this is the this is the
reality of working on a product such as
this and what this means is that we will
often have information that we will
route between the teams because of our
experience because of the operational
reality of having to react to a system
or having helped to code design each and
every single one of those components in
their current iteration it's not just
tools it also can be techniques and it
can also be ways of reacting to things
as well so here are a couple of ways
that we will help spread and pass that
information within SRA let alone between
and amongst development teams that we
work with one through constant vigilance
through practicing
disaster scenarios and understanding the
woes and the failures of other teams
right so we talked about our postmortem
culture so we have the ability to read
through each other's post forms and
understand how can I screw up less the
next exciting thing that might be
similar of a failure if left untouched
to another team so for example I might
read one that does is post mortems and
go hey I've learned something from this
right certainly from that also Google
having had its origins in search
we have excellent internal engineering
tools for searching through our code
bases so if you wonder I think I need to
go write the wheel dot CC I wonder if
I'm reinventing the wheel oftentimes by
looking through similar API calls to a
tool that you might want to write
yourself by searching through for
certain keywords about the tooling
that's available you can often in many
cases find either tools that are built
related to your need or that could be
adapted to your need and also with a
necessary as well as Google as a
cultural whole we're very happy to share
our design documents and musings about
how we would like to future design
documents so if any of us have this
feeling that maybe we're about to embark
on writing a tool set that could be
mutually beneficial to many parts of
essary or Google as a whole we will
loudly and broadly announce a design
proposal and we'll ask for feedback from
many different parts of Google including
sre and this is one of the most valuable
things that we can do because in many
cases other teams they don't have the
time to write the tooling have an
experience that would make your first
crack at the tool even better so I look
at sres as information mesh networks
passing information between developers
and necessaries around fooling around
on-call best practices around the
weirding ways of a production
environment I think it's a big part of
what essary is about so I think we're
just about coming up on the rest of the
time that we have I'd like us to maybe
answer let's say two more questions
before we go and thanks for everyone
who's been hanging out with us so why
don't we go ahead and have Daniel answer
another favorite question it has all
right yes so there's a question from
Mark Harris about whether there is much
movement of people from product
development groups into s3 or vice versa
either through temporary loans or
people permanently changing their job so
I don't know the actual statistics or
anything like that but we have actually
a program that allows engineers in other
areas to take an SME role for a decent
amount of time at no cost to their
product group etc and learn to become a
necessary sort of try before you buy
kind of thing and this this even I think
we have a success rate of more than 50%
of those engineers who will then stay in
an SRE role but even if they decide to
go back to the product software
engineering team it's a benefit for all
of Google because everything they learn
and pick up on the principles and
practices and sre they can then apply
with the home team and sort of help
their team better which may not have the
benefits of a series supporting their
services to sort of having learn
firsthand how to how to do things the s3
way and therefore become helping their
team to become operationally more
efficient and run their services in a
better way so I think the success of
this program sort of speaks for itself
that s3 is a highly attractive chopped
role at Google and also as Melissa
mentioned earlier in this talk that it's
it's a scarce resource and therefore
also has a certain attractiveness to it
oh and so I think we're down to our
final question last but not least Balaji
asks us being a software engineer he's
wondering what kind of coding do typical
sre do well
Balaji here's the here's the long and
short of it there is no such thing as a
typical essary but that being said I
think what would be a great thing to do
here is to have our panelists go around
and give different examples and
hopefully maximally different examples
of the sorts of coding that s-series do
to give you a sense for the
opportunities so I'll go ahead and start
off with an example monitoring and
analysis software you might think oh
geez I don't want to write yet one more
thing that visualizes some time series
and that's all that I do all day
that's a part of what we do we need to
be able to make faster rapid decisions
drill-down and defy divine what's
changed or what's going on and so I'm
happy to work at a company where we have
incredibly good instrumentation and
monitoring systems as a part of
everything we do including every single
last RPC that we could send between two
binaries so a lot of what I will do is
I'll get to stand on the shoulders of
these great infrastructure systems and
figure out trends and code up solutions
to do differential analysis on what
could be going wrong in the time domain
in the machines domain in the global
domains so that to me is an interesting
statistical and analytical part of the
sorts of software projects that s-series
could work on who would like to take a
crack at a different sort of space of
things that s-series work on Melissa I
can so one of the things so again I'm
from storage we deal with bits at rest
we essentially have for one of the
products you can think of it as a load
balancer but 4-bit storage so this is
actually a very complicated problem
because bits move very slowly relative
to traffic which is point in time and it
actually has significant essary
contributions including much of the code
because it was a really gnarly algorithm
intensely performance and efficiency and
reliability focused problem and the
s-series turned out to be the best
suited ones to work on it so that's a
team that split almost half and half
between Devon essary in terms of the
product contributions deep embedded in
the code but for those who love analysis
and algorithm work
it's a heaven on earth good stuff does
you have an example you want to share
with us yeah to take a kind of different
end of the spectrum one of the things
that I worked on earlier on my career
was working on in Google in account
provisioning automation for that they
handed me a book that said welcome to
Ruby way way way back in the day and
said learn this by Monday morning that's
what you're going to be working on in
your next project and it was it was a
logical integration project where we had
to develop api's for many many different
pieces of infrastructure at Google
bill the kind of process workflow
between them that automated all of the
accounts configuration actions to sort
of build the backbone of identification
within this is with internal systems at
Google back then glad to say that my
code I believe is all deleted by now has
finally finally been overtaken by by a
totally new system but it's the totally
different kind of challenge in terms of
some of the coding projects that we work
on yes yes indeed and since some people
may be wondering what languages are most
used at Google not that that should
define your interest or career mind you
because we're looking for people who can
adapt but the popular languages at
Google are C++ Java Python for some
scripting stuff and definitely go for
its performance in the scripting space
so as an SRE we'd imagine that you'd be
working in at least one of those
languages and we can train you on the
spot so good stuff let's see here Daniel
what's your example of coding for essary
that might be different from some that
we've talked about so in the context of
the project that I talked earlier about
like we have a series that - so there's
a for our virtualization orchestration
which is an open source platform called
Kennedy which is mainly developed by
Google so the s-series are actually
doing the product development and
product engineering for that platform
which involves kernel level software
engineering as well as the orchestration
and administration lay around that and
then to deploy it successfully within
Google we have to interface with all of
the sort of provisioning account
authorization systems that we use to
make it work within our Google
environments so that's another example
of not necessarily monitoring or
otherwise related project engineering
and and it's and it's actually so the
program languages use their range
between C and Python and go great and
Joe for the final word on software
engineering in SOA well slightly a
different layer of the stack for those
of you who deal with handling request
based systems
with a load balancing the idea that if
you have a lot of computers that can do
the same work you want to make sure
they're each doing equal work or some
fraction on their of and we have systems
that we do to do this and we a while
back it was determined that for one of
our large products the way it was
handing out the requests wasn't as
efficient as it could be and so essary
took on the task of determining the best
way to hand out requests architecting it
writing into the software system
ourselves we did the development getting
a pool from the team that owned the
software system and submitting it and
launching it and this is a you can
probably guess that there's a lot of
requests coming to Google every second
so this is a critical piece of our
system an SRE defined owned implemented
and launched this feature good stuff
bits bandwidth systems analysis what
what don't we end up eventually putting
our fingers into right there's a lot of
different contributions s-series have
made across the Google codebase and it's
an expectation you pitch in where it's
needed I think that's a really great
part about site reliability engineering
at Google and well I'm sad to say this
brings our our discussion today to a
close thanks to every every one of my
panelists for coming out today and
talking about oh sorry thanks to all of
our live visitors who've been asking
such great questions and to you perhaps
the future viewer of this video for more
information on site reliability
engineering I encourage you to visit the
site I mentioned before google.com slash
sre where we have a feature vignette
about our newly published book with
O'Reilly site reliability engineering
how Google runs production systems we
also have a interview with our VP of
Google's 24 by 7 division of which s3 is
a part that's been sloths talking about
what exactly sre is and additional
information in quotes tidbits from some
more colleagues just like the ones
you've met today so thanks everyone for
coming and we hope to see you a future
time at Google take</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>