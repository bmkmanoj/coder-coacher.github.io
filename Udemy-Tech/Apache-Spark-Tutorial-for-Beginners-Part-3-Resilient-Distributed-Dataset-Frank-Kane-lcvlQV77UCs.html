<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Apache Spark Tutorial for Beginners Part 3 - Resilient Distributed Dataset - Frank Kane | Coder Coacher - Coaching Coders</title><meta content="Apache Spark Tutorial for Beginners Part 3 - Resilient Distributed Dataset - Frank Kane - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udemy-Tech/">Udemy Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Apache Spark Tutorial for Beginners Part 3 - Resilient Distributed Dataset - Frank Kane</b></h2><h5 class="post__date">2018-03-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lcvlQV77UCs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let's talk about rdd's in spark and more
depth that's kind of the core of Apache
spark so understanding that is really
important
Rd D stands for a resilient to
distributed to data set and just like it
sounds it's an abstraction across all
the nastiness that happens under the
hood in spark to actually make sure your
job is evenly distributed across your
cluster that it can handle failures in a
resilient manner and at the end of the
day it just looks like a data set to you
so these are DD objects are just sort of
a way of storing keys and value
information or just any information in
general in an object that can
automatically do the right thing on a
cluster so from a programming standpoint
an RDD is just a data set to you but
under the hood it's resilient and
distributed and you don't have to think
about that very much how do you make
rdd's well your driver programs gonna
create something called a spark context
and that's basically the environment
that you're running your rdd's within
and it will create your rdd's for you
and from those you can do stuff with
them so there's actually a spark shell
you can run interactively that creates
an object called
SC for you or you can explicitly create
a spark context from a standalone script
that you can run you know on a schedule
or what have you so the spark context is
sort of the environment that your driver
program runs within and in spark and it
is what creates rdd's that you start
with creating an RDD can be done in many
different ways so if you're just fooling
around you can actually explicitly
create an RDD from a given list of
values so if I just want to make an an
RDD called nones
I could call parallel eyes with the
array one two three four and that will
create an RDD that contains the values
one two three and four and I could then
do stuff without RDD and it would in
theory you know distributed across an
entire cluster although in reality for
something that's small it wouldn't now
that's really not very useful in a
practical sense because if I can fit all
of my memory in my script then I didn't
really need to distribute it in the
first place right and we're not talking
about big data if I can just stick it in
a call like that so good for fooling
around but not real real useful in
practice well you'll do it more often
it's actually load up an RDD from a
given text file
and in this example it's just sitting on
my hard drive but you can also refer to
that from an HDFS file system so you can
actually load up a huge file that's
broken up into blocks distributed across
an entire cluster on HDFS if you're
using Amazon Web Services you can also
use s3n URLs to actually load data from
Amazon's s3 service and so on and so
forth and that will load up all of that
data into a new RDD that's returned by
text file and that will contain a line
of input text on every row of the RDD
once you have that you can also create
one from a high of context so we haven't
talked about hive too much we looked at
it briefly at the beginning of this
course but you can also create a RDD
from hive and then you can actually just
do sequel queries on the hive context
from within spark so sort of an
alternative way to using hive and you
can also create rdd's from any database
that's connected to JDBC from Cassandra
databases from HBase databases from
elasticsearch from from actual
structured data files like JSON or CSV
or sequence files or object files and
all the different compressed formats
that Hadoop supports so basically you
can create an RDD from Big Data from
pretty much any data source that you can
dream up sparks very extensible in that
way once you have an RDD what do you do
with it well if you think back to
MapReduce you can think of it in terms
of mapping and reducing for one thing
you can call a map on your RTD and that
will apply some function to every input
row of your RTD and create a new RTD
that is transformed in some way using
map now map is used when you have a
one-to-one relationship when every input
row of your RTD maps to an output row of
some new RTD but sometimes you want a
different number of rows in the output
maybe you want to split out each input
line into multiple rows or maybe you
want to discard some of the input lines
if they're invalid flatmap does that for
you so the difference between map and
flatmap map has a one-to-one
relationship between the input and
output flatten map can have any any
relationship where your input lines may
or may not result in one or more output
lines in the resulting RTD this will
make more sense when we look at an
example okay so I'm just kind of putting
these ideas in your head right now
filter can be used to take stuff out an
hour out of an RTD so you can provide
that
some function that determines whether or
not a row in your
RDD survives and the resulting are DD
distinct to us what it sounds like just
gives you back the distinct unique
values in an RDD you can also sample
them randomly and you can combine rdd's
together using operators like union or
intersection or subtract or Cartesian
product so very a lot of lot of stuff
very similar to what you were doing in
pig but spark again is a little bit more
powerful let's look at some example code
here so let's say that we want to just
do a simple example of taking an RDD of
integers and squaring them so all you
would do is once you have your SC object
for your spark context you can call
parallel eyes 1 2 3 4 to create a simple
RDD that just contains the values 1 2 3
4 so at this point we have an RDD where
each row think of it as a little
miniature database contains 1 column 1 2
3 &amp;amp; 4 we call map on that RDD with what
we call a lambda function and this is
just a way to apply a given function in
one little compact line to your RTD so
this is saying take each input row from
RTD call it X and then return the value
x times X into the resulting RTD called
squared RTD so what's going to happen
here is it's going to go through each
row of RTD start with a 1 we're going to
call X 1 and it's going to return 1 x 1
so the output and the squared RDD will
be 1 but then the second row comes in 2
we assign two to the value variable X
and return 2 times 2 which returns 4
inches square at RTD let's do this one
more time let's say we go to the third
row 3 will then be assigned to X x times
X is 3 times 3 that gives us back the
number 9 now if you're new to
programming that might take a little bit
of wrapping your head around but if
you've done any scripting or programming
before then this is pretty trivial stuff
the main concept here that might be new
though is this functional programming
here where we're actually defining a
function inline you know this is
basically a function that says take an
input value X and return the value x
times X so we're had this little compact
representation of saying here's a simple
function I'm actually punt I'm actually
specifying this function
as a parameter to the map function so at
high level that's all functional
programming is you know it's pretty easy
to understand when you look at an
example like this but big fancy term
functional programming that's what we're
doing here let's talk about that a
little bit more depth so in Python the
syntax for this functional programming
paradigm is the lambda function so we
can say lambda X x times X and that is
exactly the same thing as saying
defining an actual function called
squareit that takes an input of X and
returns X times X so in Python this is
what a function looks like and we could
just pass in that function name to our
map function and it would do the same
thing okay so if you have sort of a
one-liner function you can use this
lambda notation to do it without
actually writing another function but if
you want to do something more
complicated usually you're gonna
actually write a function within your
script that transforms that RDD and pass
in the name of that function to your map
call on the RDD okay and that's that's
all functional programming is it just
means you're defining functions that can
transform things and specifying those
functions to things like map not that
hard more things you can do an RDD so we
were talking about transformations and
are devious ways you could actually take
one RDD and turn it into another and
that's you know analogous to the map
function of a MapReduce job but what if
you want to reduce what if you actually
want to crunch the numbers down well
that's what the actions do so collect
will actually just take the input all
the results of an RDD and suck them down
to the driver script so make sure that
at that point you've reduced your
results down to something that can fit
in memory collector will basically take
whatever's in your RDD and return a
Python object that you can then do
whatever you want with from your Python
Driver script print it out save it to a
text file what have you count will
actually give you back a count of how
many rows are in the RDD count by value
will actually give you a tally by each
unique value in your RDD and how many
times each value occurs take can be used
just to take you know for example the
top 10 results or whatever top same idea
just returns the the top few rows of an
RDD so if you just want to be these are
useful for debugging if you just want a
little quick view
what's in an RDD at a given point taken
top or a good way of just taking a
little sample from those rdd's so you
can confirm that it contains what you
expect and reduce does what you would
expect you know it allows you to define
a function to actually combine all the
values associated with each unique key
just like you would do in MapReduce and
more and the important thing is that in
SPARC nothing actually happens in your
driver program until one of these
actions are called so nothing will
actually kick off within your driver
script until you call one of these
functions on an RDD and at that point
you're basically telling SPARC ok this
is the result that I want to get out of
this RTD
and that's when SPARC can say I'm gonna
work backwards from that result and
figure out the fastest way to achieve
that result that you want so basically
as you're going through this script and
transforming your rdd's until you hit an
action all that's doing is building up
this graph this chain of dependencies
within your driver script and only when
that action is called does it actually
figure out the quickest path through
those dependencies and it's at that
point that it actually kicks off the job
on your cluster now this can be
confusing when you're debugging things
because you know things don't really
happen until you actually call one of
those actions you need to keep that in
mind but that is one of the main secrets
to spark speed so with that let's talk
about an a real example of using an RDD
to solve a real problem enough talk
let's get some action and actually write
some code we'll do that in our next
lecture</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>