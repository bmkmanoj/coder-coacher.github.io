<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hadoop Ecosystem Explained in 20 min! | Coder Coacher - Coaching Coders</title><meta content="Hadoop Ecosystem Explained in 20 min! - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udemy-Tech/">Udemy Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hadoop Ecosystem Explained in 20 min!</b></h2><h5 class="post__date">2018-03-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DCaiZq3aBSc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi I'm Frank Kane and I spent nine years
at amazon.com and imdb.com as a senior
engineer and as a senior manager my job
there included making sense of their
massive data sets and I'm here to share
some of my knowledge about wrangling big
data with you so let's talk a little bit
about the basics of a Duke where it came
from what it's for what it's all about
let's dive in alright let's dive right
in and talk about the Hadoop ecosystem
the mascot of Hadoop is this yellow
elephant here and we'll talk about why
in a minute again my name is Frank
Haines and I'll be your instructor as we
make sense of this wild world of Hadoop
what's the dupe all about so what is
Hadoop anyway well the definition from
Hortonworks which is one of the major
vendors of Hadoop platforms is an open
source software a platform for
distributed storage and distributed
processing a very large data sets on a
computer cluster it's built from karate
Hardware wow that's a mouthful let's
break that down a little bit so an open
source that means it's free that's a
good thing software platform is just a
bunch of software that runs on a cluster
of computers so as opposed to just
running on a single PC Hadoop is meant
to be run on an entire cluster of PCs
that run inside a rack and some data
center somewhere so it leverages the
power of multiple pcs multiple computers
to actually handle big data for
distributed storage so distributed
storage is one of the main things that
hoodoos provides the idea is that you
don't want to be limited by a single
hard drive if you're dealing with big
data you might be getting terabytes of
information every day or even more where
you got to put all that well the nice
thing about distributed storage is that
you can just keep on adding more and
more computers to your cluster and their
hard drives will just become part of
your data storage for your data and what
Hadoop gives you is a way of viewing all
of the data distributed across all of
the hard drives in your cluster as one
single file system and not only that
it's also very redundant so if one of
those computers happens to burst into
flames in your data set and melt into
this gooey puddle of silicon Hadoop can
handle that because it's going to keep
backup copies of all your data on other
places in your cluster and it can
automatically recover and make that data
very resilient resilient and very
reliable
the other thing it gives you is
distributed processing so not only can
it store vast amounts of data across an
entire cluster of computers it can
distribute the processing of that data
as well so whether you need to transform
all that data into some other format or
to some other system or aggregate that
data in some way Hadoop provides the
means for actually doing that all in a
parallel manner so we can actually set
all the CPU cores on your entire cluster
chugging away on that problem in
parallel and that way you can actually
get through all that data in a very very
quickly so you know you don't have to
sit there with a single CPU chugging
away on a petabyte of information it can
divide and conquer that problem across
all the PCs in a cluster and of course
it's meant for very large data sets that
can't be managed by a single PC all
computer cluster is built from commodity
hardware and by commodity hardware we
don't mean cheap we just mean readily
available stuff you can rent from say
Amazon Web Services or Google or any of
the other vendors out there that sell
cloud services so you can just rent
off-the-shelf hardware throw it into a
Hadoop cluster and go for it you know
just go and keep on adding as many
computers as you need to handle the
amount of data you have what's the
history of Hadoop talk about that
briefly
so Hadoop wasn't the first solution to
this problem quite honestly Google is
the the grandfather of all this stuff so
back in 2003 in 2004 Google published a
couple of papers one was about the
Google file system GFS and that was
basically the foundation of the ideas
behind how Hadoop does its distributed
storage so GFS kind of informed what
Hadoop storage system turned into
MapReduce is also Hadoop's similar to
well it is Hadoop solution for
distributed storage of information so
GFS is basically what inspired hoops
distributed data storage and MapReduce
is what inspired Hadoop's distributed
processing and in fact they didn't even
bother changing the name it still called
MapReduce so Hadoop was developed
originally by Yahoo they were building
something called nuch which was an
open-source web search engine at the
time they saw these papers published by
Google and said hey Google thanks for
telling us all your trade secrets we'll
take that from you thank you very much
and primarily a couple of guys
Doug cutting in Tom white in 2006
started putting Hadoop together and of
course they had a bigger team than that
but they were the guys who take the
credit story goes that Hadoop was
actually the name of Doug cuttings kids
toy elephant and apparently this is the
actual yellow stuffed elephant named
Hadoop that the project was named after
so now you know why Hadoop's mascot is a
yellow elephant it's actually named
after this stuffed elephant here and
padieu peasant really mean anything and
unfortunately you got to find that a lot
of the technologies and hoodoos don't
really mean anything and it can be hard
to keep track of what does what but
that's why we're here we're gonna help
you through all that and hey you know
the rest is history as they say ever
since 2006 the doops continued to evolve
and not only has Hadoop of all but the
ecosystem surrounding Hadoop has
continued to grow and more and more
applications and systems surrounding
Hadoop have come out we're going to talk
about that shortly why Hadoop let's talk
about that again so again data is just
too darn big these days I mean maybe you
have I don't know DNA information sensor
information web logs trading information
from the stock market who knows whatever
it is in this day and age and the size
of the companies that we have today one
PC isn't gonna cut it anymore right
so you can't just go back to the old
days of having a giant Oracle database
so you just keep throwing more bigger
CPUs on or more CPU cores or bigger raid
arrays it you come to a limit where that
just can't scale any further and not
only that you're dealing with things
like disk seek time so even if you did
have like some massive petabyte hard
drive you still have to wait for that
disk head to move around right so there
are still advantages using a cluster of
computers with many many many hard
drives where they can all be seeking in
parallel with all their independent
independent disk heads the other thing
is you don't want a single point of
failure so that giant Oracle database
what happens when it goes down well bad
things you have to deal with replication
and all that nonsense
I do panels that problem for you so we
can actually keep track of what's
available on your cluster and failover
automatically to backup copies if need
be processing times are almost also
better if you have an entire cluster you
have that many more CPUs at your
disposal and Hadoop offers a means of
parallel processing that can take
advantage of all those CPUs so again
remember horizontal scaling like we do
with a Hadoop cluster is linear if you
need to handle more data or need to
process it faster just to add more pcs
to the mix add more computers to your
cluster and it will be that much faster
and that's not the case with vertical
scaling you know you start to hit these
limits like for example with your disk
see times where linear scaling is not
possible when you're doing vertical
scaling
now Hadoop originally was just made for
batch processing and the idea was well
you know if you need to run some sort of
analysis that you can wait a few minutes
for the answer who do it might be for
you and let's be honest you know if
you've ever dealt with a giant database
on a Oracle instance in a data warehouse
those don't come back instantly either
you know that can take several minutes
as well so you know we're not really
doing any worse than we were back in the
the Oracle days but there are things
that have been built on top of Hadoop
that actually do make it appropriate for
interactive queries as well and we'll
talk that about that as well there are
systems that can also expose the data
from Hadoop and a means that can be
consumed by web applications or whatever
you want at very high transaction rates
so it's not just for batch processing
anymore so that's sort of the back story
of Hadoop what it's all about what it's
for what it's used for why it exists and
why it's so powerful so up next let's
talk about the actual technologies that
make up the Hadoop ecosystem so you can
talk about it and know just enough to be
dangerous see in the next lecture all
right we're going to talk about the
Hadoop ecosystem at a very high level
now and it's got to be a little bit
overwhelming I gotta warn you there's a
lot of technologies here but bear with
me you know all I really want you to do
is to get some exposure to these terms
and these technologies so that they
don't seem as foreign when we dive into
them in more depth and you know this can
be a very valuable lecture in and of
itself so if you watch this a couple of
times it might be a good idea because
this is all you really need to really
understand what all these cryptic names
in the Hadoop ecosystem really mean and
what everything is for at a very high
level so let's dive in and actually
uncover the secrets of Hadoop all right
so let's go into some detail about the
major components in Hadoop I just want
to kind of briefly touch on all these
different technologies and we're going
to go into a lot more depth that's what
the rest of this course is all about
just deep
diving into each one of these and giving
you some examples of using them so I've
split things up here into three general
areas here we have what I call the core
Hadoop ecosystem which is just things
built on the Hadoop platform directly
and then we have some ancillary systems
that we'll talk about as well there's a
lot of different ways of organizing
these systems and this is just what
makes sense to me there are a lot of
complex inter dependencies between these
systems so it's there's really no right
way to represent these relationships but
what I'm trying to do here is show you
graphically the things that build on top
of each other within Hadoop
now the pink things here are the things
that are part of Hadoop itself
everything else is sort of add-on
projects that have come out over time
that integrate with Hadoop and solve
specific problems so let's start at the
base of it all which is HDFS that stands
for the Hadoop distributed file system
so remember we talked about GFS HDFS is
the Hadoop version of that and that is
the system that allows us to distribute
the storage of big data across our
cluster of computer so it makes all of
the hard drives on our cluster look like
one giant file system and not only that
it actually maintains redundant copies
of that data so if one of your computers
happens to randomly burst into flames
and melts into a puddle of silicon hey
it happens
it can actually recover from that and it
will back itself up to a backup copy
that it had of that data automatically
it's like you'll never even know
anything happen so that's the power of
HDFS that is the data storage the
distributed data storage piece of Hadoop
now sitting on top of HDFS we have yarn
and that stands for yet another resource
negotiator so we talked about the data
storage part of Hadoop and there's also
the data processing part of Hadoop yarn
is where the data processing starts to
come into play so yarn is basically the
system that manages the resources on
your computing cluster it's what decides
what gets to run tasks when what nodes
are available for extra work which nodes
are not which ones are available which
ones are not available so it's kind of
the the heartbeat that keeps your
cluster going now given that we have
this resource negotiator we can build
interesting applications on top of that
and one of them is MapReduce which again
is a piece of Hadoop proper and
MapReduce and a very high level is
just a programming metaphor or
programming model that allows you to
process your data across an entire
cluster and let's break that down so we
it consists of mappers and reducers
these are both different scripts that
you might write or different functions
if you will when you're writing a
MapReduce program mappers have the
ability to transform your data in
parallel across your entire computing
cluster in a very efficient manner and
reducers are well aggregate that data
together and it may sound like a very
simple model and it is but it's actually
very versatile and we'll see later on
that there are some very creative ways
you can put mappers and reducers
together to solve very complex problems
now originally a map reducing yarn we're
kind of the same thing in Hadoop they
got split out recently and that's
enabled other applications to be built
on top of yarn that solved the same
problem as MapReduce but in a more
efficient manner we'll talk about that
in a bit and then sitting on top of
MapReduce we have technologies such as
Pig so if you don't want to write Java
or Python MapReduce code and you're more
familiar with a scripting language that
has sort of a sequel style syntax Pig is
for you
so Pig is a very high level programming
API that allows you to write simple
scripts that look a lot like sequel in
some cases that allow you to chain
together queries and get complex answers
but without actually writing Python or
Java code in the process so Pig will
actually transform that script into
something that will run on MapReduce
which in turn goes through yarn and HDFS
to actually process and get the data
that it needs to get the answer you want
that's Pig just a high-level scripting
language that sits on top of MapReduce
let's zoom out a little bit here and
remind ourselves where everything fits
together we'll talk about high if next
which also sits on top of MapReduce and
it solves a similar problem to Pig but
it's really more directly looks like a
sequel database so hive is a way of
actually taking sequel queries and
making this distributed data that's just
really sitting on your file system
somewhere look like a sequel database so
for all intents and purposes it's just
like a database you can even connect to
it through a shell client and ODBC or
what have you and actually execute
sequel queries on the data that's stored
on your Hadoop cluster even though it's
not really a
relational database under the hood so if
you're familiar with sequel hive might
be a very useful API useful interface
for you to use zoom out a little bit
here and again get our bearings here
we'll talk next about and Bari and
Apache anbari it's basically this thing
that sits on top of everything and it
just gives you a view of your cluster
unless you visualize what's running on
your cluster what systems are using how
much resources and also has some views
in it that allow you to actually do
things like execute hive queries or
import databases into hive or execute
paid queries and things like that so and
Bari is what sits on top of all this and
lets you have a view into the actual
state of your cluster and the
applications that are running on it now
there are other technologies that do
this for you and Baris what Hortonworks
uses there are competing distributions
of Hadoop stacks out there Hortonworks
being one of them are the ones included
cloud era and map are but for
Hortonworks they use Ambari alright so
let's go over here
meso so meso s-- isn't really part of
Hadoop proper but I'm including it here
because it's basically an alternative to
yarn so it it too is a resource
negotiator remember yarn is yet another
resource negotiator meso is another one
they basically solve the same problems
in different ways there are of course
pros and cons to using each one that
we'll talk about later on but they so
this is another potential way of
managing the resources on your cluster
and there are ways of getting meso Singh
yarn to work together if you need to as
well and we bring up mesas because we're
going to talk about spark which i think
is one of the most exciting technologies
in the Hadoop ecosystem this is sitting
at the same level of MapReduce in that
it sits on top of yarn or meso so it can
go either way to actually run queries on
your data and like MapReduce it requires
some programming you need to actually
write your spark scripts using either
Python or Java or the scallop
programming language Scala being
preferred but SPARC is kind of where
it's at right now it is extremely fast
it's under a lot of active development
right now so it sparks a very exciting
technology right now in a very powerful
technology so if you need to very
quickly and efficiently and reliably
process data on your Hadoop cluster
SPARC is a really good choice for that
and it's also very versatile it can do
things like handle sequel queries they
can do machine learning across an entire
cluster of information it can actually
handle streaming data in real time and
all sorts of other cool stuff so I'm
very excited to teach you more about
SPARC later in this course moving on
Tezz similar to spark in that it also
uses some of the same techniques as
spark notably with something that's
called a directed acyclic graph and this
gives us the legs up on on what
MapReduce does because it can produce
more optimal plans for actually
executing your queries tez is usually
used in conjunction with hive to
accelerate it so we remember we looked
at hive earlier that kind of sat on top
of MapReduce but it can also sit on top
of tez so you have an option their hive
through tez can often be faster than
hive through MapReduce or both both
different means of optimising queries to
get a efficient answer from your cluster
let's talk about HBase so HBase kind of
sits off to the side and it's a way of
exposing the data on your cluster to
transactional platforms so HBase is what
we call a no sequel database it is a
columnar data store and you might have
heard that term before it's basically a
really really fast database meant for
very large transaction rates so it's
appropriate for example for hitting from
a web application hitting from a website
doing oil TP types of transactions so
HBase can actually expose the data
that's stored on your cluster and maybe
that data was transformed in some way by
spark or MapReduce or something else and
it provides a very fast way of exposing
those results to other systems and what
else can we talk about let's go over
here and talk about Apache storm storm
is basically a way of processing
streaming data so if you have streaming
data from say sensors or web loss you
can actually process that in real time
using storm and spark streaming solves
the same problem a storm just does it in
a slightly different way so Apache storm
made for processing streaming data
quickly in real time so it doesn't have
to be a batch thing anymore
you can actually update your machine
learning models or transform data into a
database all in real time
it comes in pretty cool stuff let's go
over here
talk about Uzi Guzzi is just a way of
scheduling jobs on your cluster so if
you have a task that needs to happen on
your Hadoop cluster that involves many
different steps and maybe many different
systems lucy is a way of scheduling all
these things together into jobs that can
be run on some sort of schedule so when
you have more complicated operations
that require loading data into hive and
then integrating that with Pig and maybe
querying it with spark and then
transforming the results into HBase Luci
can manage that all for you and make
sure that it runs reliably on a
consistent basis moving over here a bit
zookeeper also sits alongside all these
technologies it's basically a technology
for coordinating everything on your
cluster so it's it's the technology that
can be used for keeping track of which
nodes are up which nodes are down it's a
very reliable way of just kind of
keeping track of shared States and
across your cluster that different
applications can use and many many of
these applications we've talked about
rely on zookeeper to actually maintain
reliable and consistent performance
across a cluster even when a node
randomly goes down so zookeeper can be
used for example for keeping track of
who the current master node is or
keeping track of who's up who's down
what-have-you and it's really more more
extensible than that even but we'll talk
about that later
over here there's also systems that are
just focused on the problem of data
ingestion so how do you actually get
data into your cluster and onto HDFS
from external sources scoop for example
is a way of actually tying your Hadoop
database into a relational database
anything that can talk to OLTP or or I'm
sorry ODBC or JDBC can be transformed by
scoop into your HDFS file system so
scoop is basically a connector between
Hadoop and your legacy databases flume
it's a way of actually transporting web
logs at a very large scale and very
reliably to your cluster so let's say
you have a fleet of web servers flume
can actually listen to the web logs
coming in from those web servers in real
time and publish them into your cluster
in real time for processing by
something like storm or sparks streaming
Kafka solves a similar problem although
it's a little bit more general-purpose
it can basically collect data of any
sort from a cluster of pcs from a
cluster of web servers or whatever it is
and broadcast that into your Hadoop
cluster as well so those are all three
technologies that solve the problem of
data ingestion alright moving on now
your data might be exposed or stored in
other places too so let's talk about
those as well
HBase would also fit into this category
but since HBase is really part of the
Hadoop stack itself I left it off of
this little collection here my sequel of
course or any sequel database is
something you might be integrating with
your Hadoop cluster you can not only
import data from scoop into your cluster
you can also export it to my sequel as
well so a lot of these technologies like
spark have the ability to write to any
JDBC or ODBC database and you can store
and retrieve your results from a sequel
database if you're so inclined
Cassandra like HBase and also MongoDB
are both also columnar data stores and
there are also good choices for exposing
your data for real-time usage to say a
web application so you definitely want
some sort of layer like this like
Cassandra Oh Mongo DB sitting between
real-time applications and your cluster
we'll talk about those in a lot more
depth both are very popular choices for
vending simple key value data stores at
very large transaction rates my sequel
Cassandra MongoDB all external databases
that might integrate with your cluster
and there are also several query engines
that sit on top of your Hadoop cluster
so if you want to actually interactively
enter sequel queries or whatever you can
do that using these technologies again
things don't always fit neatly into
different circles here if you remember
hi if that actually is a similar thing
as well but again since hive is more
tightly integrated into Hadoop I chose
to leave it out of this particular
circle but it too is a way of querying
your data Apache drill pretty cool stuff
it actually allows you to write sequel
queries that will work across a wide
range of no sequel databases potentially
so it can actually talk to your HBase
database and maybe your Cassandra
your MongoDB database as well and tie
those results all together and allow you
to write queries across all those
disparate data stores and bring them all
back together when you're done Hugh also
a way of interactively creating queries
it works well with hive and HBase and
actually for cloud era it kind of takes
the role of Ambari for sort of the thing
that sits on top of everything and lets
you visualize and execute queries on the
hadoop cluster as a whole apache phoenix
kind of similar to drill it lets you do
sequel style queries across the entire
range of data storage technologies you
might have but it takes it one step
further it actually gives you acid
guarantees and oltp so you can actually
make your not not sequel Hadoop
datastore look a lot like a relational
data store in a relational database with
all the guarantees that come with that
and finally presto yet another way to
execute queries across your entire
cluster these all solve you know kind of
the same problems Epling is just another
angle on it that takes more of a
notebook type approach to the UI and how
you actually interact with the cluster
but at the end of the day those are all
ways of actually executing queries and
extracting meaning from your cluster
without necessarily writing programs to
do it so that's the world of Hadoop in a
nutshell a lot of stuff there but hey
that's all there is to a rough spend the
rest of this course diving into each one
of these one at a time and doing some
actual exercises and activities with
them so you get to understand them more
in more depth but for now those are the
buzzwords and hopefully when you hear
these terms and these technologies they
won't seem quite so foreign to you
anymore so welcome to the world of loop
let's move on and dive into some more
detail all right that was a lot of stuff
right there and you know I had to Sanofi
need to go back and watch it again
there's a lot of information there but
don't worry we're gonna dive into each
and every one of these technologies so a
lot more depth later on in the course I
just wanted to kind of hit you with all
this right up front so you could know
just enough to be dangerous and know
enough about these technologies to
actually know what people are talking
about when they're talking about Hadoop
I mean that really that's half of the
battle here none of these things are
really that complicated
be honest you know all these things are
actually fairly easy to use and
understand Hadoop is only really hard to
grasp because there are so many
different technologies and a lot of them
do the same thing but if you can
understand what they're all for and how
to choose between them and how to put
them together to actually solve real
business problems well that's what they
pay people the big bucks for thanks for
watching
if you'd like to learn more check out my
ultimate hands-on Hadoop course on udemy
in it you'll learn over 25 different
technologies in the Hadoop ecosystem and
build up a powerful arsenal against your
big data just use the coupon code in the
description below and you'll get a large
and special discount see you there</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>