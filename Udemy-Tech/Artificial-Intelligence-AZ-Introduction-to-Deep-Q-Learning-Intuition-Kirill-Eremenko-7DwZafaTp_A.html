<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Artificial Intelligence A-Z: Introduction to Deep Q Learning Intuition - Kirill Eremenko | Coder Coacher - Coaching Coders</title><meta content="Artificial Intelligence A-Z: Introduction to Deep Q Learning Intuition - Kirill Eremenko - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udemy-Tech/">Udemy Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Artificial Intelligence A-Z: Introduction to Deep Q Learning Intuition - Kirill Eremenko</b></h2><h5 class="post__date">2018-03-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7DwZafaTp_A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey did enthusiasm my name is Carol and
I'm part of the super data science team
where we teach dozens of data related
courses which have been taken by over
400,000 students globally and this is a
quick preview of our best-selling
artificial intelligence course which has
been taken by over 45,000 students and
where you can learn all of the ins and
outs of this amazing field I hope you
enjoy and happy analyzing hello and
welcome back to the course on artificial
intelligence in today's section we're
tackling the topic of a deep Q learning
so let's see how we're going to attack
this in this section we will learn a
deep key learning intuition the learning
side of things so we're going to
separate deep you're learning the
intuition behind it into two parts the
learning and the acting and we're going
to have two tutorials on that so first
we'll understand how the neural networks
actually learn and how they update their
weights based on what we are feeding
them in and how the whole concept of Q
learning works so how we're going to
take the temporal difference concepts
that we discuss in simple Q learning
we're going to apply them into typical
learning and then we're going to talk
about how deep key learning algorithms
actually decide what action to take in
what state now then we're going to talk
about experience replay a very important
addition on top of deep Q learning which
actually enables deep you learn to work
properly and you'll see why it's
important from that tutorial and then
we're going to talk about action
selection policies we're going to talk
about how deep your learning agents are
able to combine exploration with
exploitation so once they found
something a good approach they can use
that approach but also they need to
explore so that they don't get stuck in
a local maximum and one more thing I
wanted to mention about this section is
it is highly beneficial if you have a
look at annex number one artificial and
neural networks so if you go and explore
all those topics we've got some very
powerful intuition tutorials prepared
for you there if you haven't done of
course if you haven't done the deep
learning course if you've done the
deployment course then you already know
all of these things and you can proceed
with the section
but if you want to get that additional
knowledge about neural networks before
you proceed with this part of the course
this is highly advisable because it will
help you understand exactly how neural
networks work and why they're so
powerful why we're leveraging them in
this deep Q learning algorithm and once
you've refreshed your knowledge or
gained that knowledge on a neural
networks from that to annex then come
back here and we will proceed with the
typical area if you're pretty
comfortable of neural networks then
let's get straight into it let's start
talking about deep Q learning intuition
all right so let's have a look
previously we spoke about key learning
and what it's all about and we learned
about the agent environment and how the
agent will look at the state here or she
is in take an action get a reward enter
into a new state and based on that
feedback loop they will continue taking
actions and they will learn from that
understand what are the better actions
to take and so we looked at this basic
example of a maze we understood that as
the agent explosions environment
understands what the values of the
states are then we moved on from dealing
with the values of the states to dealing
with the values of the actions or the Q
values and then based on that we
understood how plans in a non stochastic
environments work and how policies work
in stochastic environments and this is
an example of policy so that's a quick
recap of everything we discussed in the
basic Q learning and now let's have a
look at how this can be taken to the
next level through deep learning through
adding deep learning ok so this is our
environment and what we're going to do
now is we're going to add instead of
just doing basic calculations in this
matrix that we have which is queries
pretty simple what we're going to do is
we're going to add two axes I'm going to
add an X and Y axis or we'll call them X
1 and X 2 just to make things even more
general and here we've got two will
number the row the columns 1 2 3 4 here
we'll number the rows 1 2 3 and so now
every single state can be described by a
pair of two values X 1 and X 2 so any
one of these squares in which the agent
can possibly be in can be described by X
1 X 2 so for instance
now he's in the square with X 1 equal to
1 and X 2 equal to 2 and therefore
that's in a sum that same way we can
escape in your square meaning we can
describe any state then of course this
is a very simplified version of an
environment of describing states but
nevertheless it works in this case and
that means that now we can feed this
these states into a neural network and
by the way here I would just like to
mention that at the end of the course
we've got annexes we've got annex number
1 and an X number 2 in order to proceed
successfully with this section it's
highly advisable that you check out
annex number 1 which is on artificial
neural network so you understand how
they work so that we can we don't have
to delve into that here and we can just
use the benefits of the knowledge of how
artificial neural networks work and so
we feed in this information on the state
into a neural network and then it will
process this information so X 1 and X 2
depending on the structure then your own
network it might have multiple hidden
layers and so on so that's something
that you'll figure out in their
practical tutorials but at the end we
will structure in such way that it spits
out 4 values and these 4 values are
actually going to be our Q values so the
values which dictate which action we
need to take and further down in this
tutorial will see exactly how these Q
values are used to decide which action
is taken but the main point here is that
we no longer look at just this maze from
a key learning perspective we're now
taking the state of the maze and we're
feeding them into a deep neural network
in order to get these Q values and and
at the end of the day we're still going
to come up with an action we're still
going to understand how what action we
need to take and we'll discuss all this
in more detail but the question right
now is why why are we doing all of this
why we complete why we're making things
so much more complicated when that
initial approach of Q learning was
working already well the reason for that
is the Q learning was working in this
very simplistic environment and we're
continuing to deal for now with this
very simplistic environment in order to
better understand the concepts but at
the same time that simple Q learning
will no longer work
more complex environments and we're
talking about for instance the
self-driving cars we shall be creating
or playing doom when the other
artificial intelligence is playing doom
or other Atari games like breakout or
even self-driving cars and more advanced
reinforcement learning things such as
like robots walking around and
performing actions in all those cases
basic you learning is insufficient is
not strong is not powerful enough to be
able to master those challenges and just
like bro we've seen in the deep learning
course if you've been in our deep
learning course or if you've done the
annex sections on X number 1 and X 2 you
will where you know that deep learning
is by far superior to any type of
machine learning let alone a simple Q
learning and that's why we're leveraging
the power of deep learning here so we're
feeding in the information about the
environment as a vector of values for in
this case just two values into a deep
neural network and then we're using that
to perform the actions that we want to
decide which actions that agents going
to take so that's kind of like a
high-level overview of why we're doing
this and now let's have a look at in a
bit more detail what happens to the
concepts of Q learning when we transfer
when we make this transformation from
transition from simple Q learning into
deep Q learning so as you saw in the
previous intuition tutorials we had a
slide like this which is the foundation
of temporal difference learning this is
the formula for temporal difference and
basically so let's go through this so
basically we had an agent who was in
this state over here which was indicated
by blue arrow and we were understanding
how temporal difference works for this Q
value of for instance going up and so
what we saw here was before this is in
the simple key learning not the deep key
learning is in the simple key learning
what we saw was before the agent had a
certain quick hue value that he had
learned about this action of going up
and so then he decides to take deception
to go up and right after he takes his
action he gets a reward for taking this
action in this state and that is there
that reward plus now he can eval
the value of the current state he's in
which is the maximum of all of the new Q
values of all of the Q values of the new
actions he can take a prime in the new
state s prime and read multiplied by the
decay factor of gamma so that E is
essentially the Q the new Q value or
kind of like that the empirical Q value
that he has just received for taking
that action and ideally these two should
be the same so the actually the Q value
that he had in his memory about this
action in this state it should equate to
the actual reward plus the gamma times
the value of the state that he ended up
in and therefore that's how we calculate
the temporal difference we take what he
got after - what he got and what he had
in in mind what he was expecting you'd
subtract one from the other and that's
your temporal difference and then you
use your learning rate alpha to adjust
your Q value your your new Q value by
the temporal difference but with a
coefficient of alpha so that is the
essence of the simple Q learning now
let's have a look at how it changes in
deep Q learning and so we're still going
to work with this slide but we're going
to just see exactly what's happening so
in deep Q learning the neural network
will predict for us as we saw in the
previous slide and as you'll see further
down this tutorial the neural network
will predict four values or it might
predict more values if there's more
possible actions in a given state but in
this case we know that there's only four
actions up right left down and so the
neural network will predict four of
these values so there will be no in in a
deep key learning situation it's
important to send these know before or
after and this is how we'll get to know
this a bit better so the neural network
will predict four of these values and it
will compare not to what will happen
after but the neural network will
compared to this exact value but it was
the this value which was calculated in
the previous step so in the previous
time when the agent was any in this
exact square so let's say I don't know
some time ago and the agent was again
was in this exact square as well and
it's calculated this value previously so
in the previous time a long time ago the
agent calculated this value then the
agent stored this value for the future
and now the future has come so now he's
in the square again and now he's got
these q values which is predicted and
one of them is for the before going up
so now what he's gonna do he's gonna
compare the predicted value of Q to this
value which he had recorded from the
previous step and we'll understand
exactly why this is important right now
so just important to understand here is
there's no before an officer in this
specific Square this specific time we're
taking the Q value that he's predicted
using the neural network this time and
we're comparing it to this value which
he had from the previous time from the
previous time he was in this square
assessing all of the situation and you
know likely other previous time he
actually performed this action so there
we go now let's have a look at how this
all works out in the neural network and
why why is it like that I know it sounds
a bit complicated right now but we'll
break it down into simple terms right
just in a second so this our neural
network we're feeding in the states of
the environment into the neural network
is going through the hidden layers then
it's coming out with these outputs q1 q2
q3 q4 in that specific state these are
the Q values that the neural network is
predicting for the possible actions
those are the Q bells so then we're
comparing to target and these targets is
exactly so if we go back here this is
the target so this is the value that was
predicted and then but also we know that
we have a target from the last time we
were in the square we have a target for
this same action which is up for
instance so here we've got a target and
we're going to compare so we're
comparing q1 versus that target we're
comparing q2 versus that target the
target that we had from previously q3
versus the target q4 versus the target
and so this is the part where the neural
network or the agent is now learning
through deep learning how to better go
through this mess and the key point here
is that we're still applying Q learning
but the concepts in CID
in simple q learning you learn through
temporal differences which are pretty
straightforward which we've already
discussed and we know quite well by now
but at the same time in deep learning
how do not networks learn well neural
networks learn through our adjusting
their weights so we have to adapt the
concept of rain for the concept of
simple key learning to the way neural
networks actually work and that is
through updating their weights and so
this is what we're trying to figure out
here how do we adapt that concept of
temporal difference to a neural network
so that we can leverage the full power
of neuro networks and so far we've
gotten this so we enter our environment
state here as a vector goes through a
neural network we get predictions of Q
values and then from the previous time
the agent was in that state we have
these cute targets to target one two
three and four for each of these
respective actions and so now we're up
to okay let's compare each one with each
one and from here it's it becomes pretty
straightforward if you're up to speed
with neural networks
once again that's only a annex number
one we're going to calculate a loss
which is L here and we're going to be Q
target this one minus Q minus this one
we're going to square that so the
squared difference of the of each one of
these and we're gonna sum them so we're
going to take the sum of the squared
differences of these Q values and their
targets and we're gonna send them up and
that's gonna be our loss and so ideally
just as we had Intel in the temporal
difference learning so if we go back for
a second remember we said ideally we
want this to be equal to this so we want
the temporal difference to be zero so
that's that means basically the agent is
predicting exactly correctly what you
know the Q value is that the agent is
predicting are exactly or that he has a
memory are exactly a descriptive of the
environment and therefore the agent can
navigate navigate the environment well
right there's no surprises there's no
there's no if as long as it's temporal
difference is Paul highly positive or
highly negative then then we got some
surprises but if the temporal difference
is zero then he knows environment so
well that he can predict what's going on
and he can and therefore his policy
is going to be very good and he's going
to be able to navigate it
so here same thing so we want this loss
to be as close to zero suppose as small
as possible
and that's why now we're going to this
is the part where we are going to
leverage the real true power of neural
network so we're going to take this loss
and we're going to use back propagation
or stochastic gradient descent to take
this loss and pass it through the
network pass it back or back propagated
through a network and through stochastic
gradient decent update the weights of
these synapses in the network so that
next time we go through this network the
way it's already a bit better
descriptive of the environment and
that's exactly how it works so here you
have if we go back this is calculated
losses calculate and it gets bright
prove propagated for the network the
weights are updated then the next time
we get here this happens again and we
get here this happens again and so on
and so and it keeps happening and that's
how this agent learns or basically now
it's a neural network which is the brain
of the agent is learning is becoming
more and more descriptive of the
environment and therefore the agent is
able to navigate the environment when we
say descriptive environment basically
means that when we put in the states of
the environment that this agent is in we
are more likely to get closer and closer
to the actual Q values and that happens
because the Q values that we want be
good and to find the right action and
that happens because these cue targets
are actually empirically derived so he
every how does he find these cute
targets that's because that's actually
this so he actually observes okay so
once I do take this step what's the
reward I get and then what's the values
of this state so same thing as we saw
previously in Q learning in the simple Q
learning intuition so he learns this
through trial and error and then he
constructs his network or updates the
wait saves his network in such a way
that the predicted Q values are closer
and closer approximating the target Q
value so very similar to the concept we
discussed here in this simple temporal
difference learning of the simple key
learning algorithm
so there we go that's that's how the
agent learns so we're up to here that's
the learning part and now we're going to
move on to the actual acting part this
so this in there's two parts two
distinct parts that we have to remember
so that's the learning part but now he
actually he's done all of this that's
beautiful
now he actually has to take an action he
has to decide what is he gonna do he's
gonna do action one two three or four
and so how does he do that well the way
he does it is now given those same cue
values so the Q values don't change
after we've we have these Q values if
compared them with calculate the laws we
back propagated error we've updated the
weights but the Q values don't change in
that whole process so I'll so we've got
the Q values they're they're fixed we
know what they are sold this happens the
networks updated and now using those
same Q values that we had what we're
going to do is we're going to pass them
through a soft max function and again a
soft max is described in I think in
annex two and we'll talk a bit more
about soft max further down in or we'll
talk about this action selection policy
further down in the rest of this section
so just in a few tutorials but for now
we're just going to say we're passing it
to the softmax function basically what
it does is it allows it helps select the
best one it selects the best action
possible and there's a small caveat to
that it's not just the best one possible
we'll talk about that in the action
selection policy tutorial but for now
let's just say it selects the best
action from here it says okay so q1 you
know the likelihood basically we know
the Q values so it's predicted the Q
values so it's it can look at them and
say okay so the highest Q value out of
these just as we did in the simple Q
learning algorithm it'll just look at
all these for say the highest Q values
this one and I'm gonna select that
action we're gonna take those and that's
pretty much it that's how it shows us
which action take takes and it takes the
action and then all of this process
happens again for for the next state the
add agent ends up in our case in the
next square of the maze but generally
speaking it's the next state so there we
go that's how we feed in a reinforcement
learning problem into a neural network
through a vector describing the state
that we're in and once we feed it in
there's two parts of the process that
happened part one is the learning
so remember that part where we compare
each of the queue values with the
targets and then we back propagate the
loss through in the network to update
the weights so that our network is
learning as we go through this maze or
through this environment and also the
second part is of course we have to act
we have to select an action and that is
where we pass the Q values through the
softmax function and or basically an
action selection policy which we'll talk
about further down and then we simply
select the action that we want to take
and we perform that action and then this
whole process starts again and then
maybe the agent gets then maybe the
agent doesn't pass the game in any case
the game ends and then once again the
whole whole process repeats the agent
plays the whole game again and then that
stops so basically that's that's another
epoch every time the agent you know the
every time the game ends with a
favorably on favorite that's the end of
an epoch and then he starts again and
then he starts again and then he starts
again and so on so that happens and this
process happens for every single time
the agent is a new in a new state so the
state is encoded here so that's that's
important so not just for every single
game that he plays but for every single
state so he's in a state it goes through
his process updates and so on and
happens every single time and so the
learning happens and then the acting
happens as well so that is deep Q
learning in no the intuition behind deep
Q learning we've got lots more to cover
off and then of course we've got the
practical and in the meantime if you'd
like to get some additional information
on tip Q learning we've got a
recommended reading so we've already
spoken about Arthur Giuliani's series of
blog posts if you look at simple
reinforcement learning with tensorflow
part 4 you will find the part that's
relevant to what we discussed today note
that here he talks about convolutions we
are not covering revolutions in this
section we're going to be talking about
them in the next section of the course
so the difference here is that so just
kind of skip the convolutions part for
now and we'll talk about them in the
next part of the course but the
difference is in convolutions you're
like looking their agent is looking
the image and and therefore he has to
process an image an additional
complication for now where we're slowly
gradually building up to that
for now we're encoding our environment
through so if you look here we're
encoding our environment or maybe like
look at this one probably yeah encoding
our environment as a or an according a
state the agent is in as a vector so in
our case it was a very simple vector of
two values sometimes people even in that
in that simple may sometimes over as
you'll see from this blog post sometimes
people prefer the one hot encoded
version of that state so basically where
every single box of the maze has a so
you have like a vector of four in our
case would be 12 values 3 by 4 so it's
like either either 1 or 0 depending on
which elements in which box you're in in
the environment so in a whichever way
you decide to encode your environment
and the state of your environment that's
how we're encoding it so it's basically
a vector the key here is that it's not a
convolution so it's not like an image
and there's no convolution in volts so
this part will come later for us it
starts over here and that just
simplifies the process for us to
graduate understand better and of course
don't forget that this blog post is
written in tensor flow and we're using
PI torch in our tutorials so hopefully
you enjoyed this quick intro into a deep
convolutional deep notes convolutional
yet deeper key learning and on that note
I look forward to seeing you next time
and until then enjoy artificial
intelligence alright so I hope you're
enjoying the tutorial so far we're
nearly done with the intuition you're
soon very soon get to the practical side
of things we've just got a few little
things that we need to cover off all
right so previously we talked about how
we add neural networks into this whole
equation of key learning and take you
learning to the next step and turn it
into deep Q learning and today we're
going to add a an extra important
feature which you will be coding in the
practical side of things so
hadlen and i decided that it's important
for us to cover it off in the in
tradition side of things so that you're
more prepared for it when it comes in
the coding side of things so as we
discuss we've got the
network there there's two parts that
happen first of all it's the learning so
the network actually learns with every
new state it's slowly updates its
weights to get better and better and
better at dealing with this environment
and then there is the acting inside the
state so after the Q values have been
counted in the state then one Q value
selected so today we're still going to
talk about the learning part we're going
to come up with a interesting feature
that's going to well we're not going to
come up with this feature ourselves but
we'll talk about a feature that is very
important for a deep Q learning and that
a feature is called experience a replay
all right so here is our network so
we've just copied it over here we've got
that a loss that is calculated the
bottom is back propagated through our
network and let's have a look at an
example of what happens to understand
the problem that we're dealing with a
bit better so here is an example
actually from this course this is a
screen short shot exactly from this
course this is what you'll be
programming this is a self-driving car
that is driving through this sort of
along this road and it's it has to learn
how to navigate this road and so what is
as we discussed previously what is this
in more in this state and exact orce the
state is now just gonna be x1 x2 at Lund
will describe a bit in a lot more detail
what the state is it is gonna be a
couple of parameters which relate to the
angle of the car and some relative
parameters what the sensors are reading
and so on so there's gonna be more
parameters than that to describe the
state but nevertheless is going to be a
vector of values it's going to go
through a neural network and then on the
output you're going to have some Q
values again there'll be a difference
depending on this environment it can be
a different number of actions possible
actions but we're just going to for
simplicity sake leave it at four just
for us to be able to understand a bit
better what's going on here
so in this case what is the question is
so far what is this this input into this
neural network or more specifically how
often do we trigger this neural net how
often does this neural network go
through well every time the car ends up
in a new state so the car makes a move
it ends up in a new state and then
everything goes all that data all that
information from about the state goes
through the network Q values are
calculated errors but this error is
calculated based on what we discussed in
previous tutorials this there is back
propagated through a network weights are
updated then the car selects which
action was to take makes that move ends
up in a in a new state in the new state
everything starts over again and so
basically this happens every time the
car is in a new state well I'll have a
look at this example I specifically took
this screenshot because it looks it's
very well illustrates the problem that
is addressed through experience replay
and an experience replay is not just
something that we use in this course or
in this specific problem it is something
that you will see used throughout like
on and on and over and over again in
artificial intelligence algorithms
because it is so powerful and it's so
important so look at this car this car
in this problem or in this environment
its goal is to come from go from here to
here and back it's recall is to navigate
its way here here without crossing these
walls which are made of sand and so the
car start over here it went down and
like the it's reward is based on you
know how close it is to start it so the
car went from here it went down and kept
going like this like this like this like
this like along this wall along this
wall and what is going to do next is
going to turn is going to keep going
well what did we wanted to do is keep
going here but let's think about it for
a second once it got to this wall every
single time it moves forward it moves
forward it moves forward it moves
forward moves forward move forward moves
forward and so on it moves forward so
there might be like depending on the
structure environment could be like a
hundred moves here or 50 moves here and
it just keeps moving forward forward
forward forward forward and nothing
changes nothing really changes yes it
gets away further away from this started
closer to this target that's lovely
but in terms of their surrounding
environment not many things are changing
it's still that same wall if you're
sitting in the car
you've probably seen this situation when
you're driving in the the whatever
you're seeing is like the environment is
so monotonous that you're just seeing
kind of the same thing it's just passing
by but it like is that imagine you're
driving through a desert and you're just
seeing the same thing it's the same sand
it's the same sound nothing is happening
nothing is changing and so basically
every single time we're putting that
state that new state into here yes of
course something might be changing for
instance you're driving the car and your
GPS is showing you're closer to your
destination so one of these inputs is
changing but a lot of these other inputs
the sensors for instance which are on
the car they're not changing and
therefore as you're driving so in this
state put input the inputs into your
neural network here here here here here
here here and here and here all the time
the inputs are pretty much the same and
so if you keep inputting the same inputs
the same values the same vector or very
similar vectors into your network
because there is no variety the car will
learn very well one thing you'll learn
very well how to drive along this wall
which is on its right and so that's how
the network will update and it will get
rewarded will slowly start getting
rewarded for driving so well it will be
like okay so up from here will be
started learning oh I'm doing so good
I'm doing even better I'm doing it
better it'll it will have this this
false perception that it's actually
doing very well even though it only
learns how to drive along this wall and
so the neural network will become very
adapted to driving along this wall and
then all of a sudden there's this curve
and the car doesn't know what to do and
it completely doesn't fit in with this
neural network and even if it does
adjust somehow let's hypothetically say
it passes this part and then it ends up
on this wall same thing is gonna happen
it's gonna drive from here here here
okay now the neural network is
restructuring itself to adapt to this
wall and then BAM this thing happens and
then even if somehow it gets past that
it'll drive past this thing and then
same thing along these lines so
basically it's like a very vivid example
of the problem that we're what we have
is that because the way we're using than
your own
updating it with every single state once
we have lots of consecutive stares they
don't even have to be the same but there
is in environments it's normal that
consecutive states are somehow
correlated or somehow interdependent and
we don't want that interdependency to
bias our network we don't want the card
to just learn how to drive along like a
straight line or along a curved line or
like anything that you think that you
can think of in in life where an agent
would be navigating environment where we
can think of correlated or
interdependent states that come after
another that can really mess up your
neural network if you're just going to
let the agent learn from that and that's
where experience replay happens in what
what happens in experience replay is
these experiences so these states that
it's in 1 2 3 however many 50 states
here in a row they don't get put through
the network right away they're actually
saved into memory of the agent and so
for instance it save all these and saves
all these and some at some point once
that gear reaches a certain threshold
which you'll be able to code and at Lund
we'll show you how to do that once it
reaches a certain threshold then the
agent decides for itself ok it's time to
learn I have I have this batch of
experiences that I have and now I'm
going to learn from that batch and so it
randomly selects a uniformly distributed
and a uniformly is key is important here
because that's something we'll we'll
talk about on the next slide we'll
mention that but it takes a uniformly
distributed sample so basically all
experiences are considered to be equal
it takes a uniformly distributed sample
from that batch of experiences that it
has and then it goes through them and
it'll learns from them so it doesn't
take all the experience it just takes a
uniformly distribute sample so it might
take couple from here couple from here
couple from here and it and each
experience is characterized by the state
it was in the action that it took the
state it ended up in
and the reward its it achieved through
that action in that specific state so
four elements in each experience state
one action state two and reward and so
it takes all those experiences and then
it passes them through the network and
it learns and that way it it breaks the
pattern of that bias which comes from
the sequential nature of the experiences
if you were to put them through the
network one off to the other so that's
the main focus of experience replay
that's that's what the problem it
addresses and another benefit of
experience replay is that sometimes in
an environment like this you might have
very valuable rare experiences so for
instance I don't know let's say let's
look at this corner right this is a this
is a right corner right and a very sharp
1 how many sharp so it'll be coming from
here assuming it's going to be hugging
this corner so how many sharp right
corners do we have in this in this
column we only have one right corner
here and one right corner here right so
when it's coming this way that's the
right corner and then when it's going
back it's a sharp right corner here so
and this one's not sharp this from the
shop so there's only one opportunity in
the whole environment to learn from a
sharp right corner and that's a very
less important experience because it
might get really good at driving along
straight lines get really good at doing
like soft corners like that like that
but and then it will keep messing up
this sharp right corner simply because
simply because his own it doesn't have
that much opportunity to learn from it
and so therefore it'll learn everything
else pretty quickly but it'll take a
long time to learn this right corner is
it's a very simplified example is a very
simplified explanation but it
illustrates the concept that sometimes
they're rare experiences which are which
can be valuable and if you're just doing
a simple neural network where you're
putting in your values here and you know
they're going through and you know like
even if we forget about that problem of
the sequential nature of experiences and
how they can be interdependent and all
correlated if you even forget about that
for a second what happens is once you
put an experience in it goes through
networks updated then you instantly
forgive or forget about
experience you move on to the next one
that's just how the neural network works
then you move on to the next state the
next state the next state the next
experience next experience next
experience and so on so this right
corner as soon as it goes through a
network it's gone there and you don't
have any memory of that valuable
experience whereas with experience
replay because you're putting these
experiences in two batches you can
organize your batch as a rolling window
so for instance you could have like a
hundred batches so 100 experiences in
your batch so when it's coming back from
here it's a as soon as it has this
recorded this experience in its batch
then like at some point it's runs it
takes a uniform distribution from its
batch of experiences and then there's a
rolling window so it forgets these
experiences but then it keeps these
experiences and then again it learns
from once it's here it learns from this
batch and then once it's here if it gets
all the way up to here but then it has a
batch of experiences like that so
therefore now it learns from these
experiences and that way what you're
getting is that this right-hand corner
it might come up several times in its
learning process because it was in that
batch when the batch was like this
around there then it was in the batch
here in the batch here so it came up in
several batches because the batch might
be updated as a rolling window of
experience so the older experiences get
kicked out the newer experiences are
added and then again older experience
get tada
so an experience it stays in the batch
for quite some time and the car or agent
can learn from that experience several
times so that's another advantage of
experience replay and of course the
final advantage is experience replay
gives you an opportunity to learn from
more experiences then if you're just
learning from one at a time because you
have that batch and therefore an answer
rolling window and therefore even if
your environment is limited to
experience your experience replay
approach can help you learn faster and
instead of just redoing that environment
many many times you can learn faster
because you you don't have to redo it
you have those experiences saved so
those are the main advantages of
experience replay let's recap and then
we've got the we're breaking a pattern
of independence and correlation of
sequential experience
we save rare experiences which might be
important therefore we can learn from
them more often and we can learn in
environments we can learn foster
environments which are experience which
have a shortage of experiences which
don't have that many experiences that
the agent goes through and still we can
be able to learn it so that is what
experience replay is all about if you'd
like to read a bit more then this is
there's an interesting article published
by deep mind in 2016 it's called
prioritized experience replay and it
talks about why why are we using a
uniform distribution to select our
experiences from the experience batch
why don't we find a better way to select
our experiences and prioritize some of
the experiences which we feel that are
important and so it's quite an
interesting thing so in this case you
will be able to not only reinforce not
only reinforce your knowledge on
experience replay but you'll actually be
able to move with the cutting edge of
technology so this is 2016 and published
by deep mind so it's a very recent very
powerful paper so you'll be able to
actually explore the limits or the
explore even further this algorithm and
take it to the next level so I'll leave
it up to you to find out why and how we
can change the uniform distribution to a
different approach to experience replay
from this paper if you'd like and I hope
you enjoyed today's tutorial and now we
know what experience replay is and we
can confidently use it in our practical
tutorials I hope you enjoyed the course
so far and today we're talking about
action selection policies alright let's
dive straight into it previously we
talked about adding a neural network to
our simple cue learning and so far we're
getting quite into deep cue learning
we've talked about the learning part
quite a bit including adding some
elements to it and today we're talking
about this part we're talking about the
acting so let's have a look so here
we've got what we discussed about acting
that once you input the values the
parameters or the vector describing the
state the agent is currently in in that
environment
then that is after all the learning it's
done or B even before the learning is
done basically we get all the Q values
so we're not interested in the learning
right now we're interested in acting so
once we have these Q values how do we
understand which one we need to use well
if you think about it Q values are
simply these are the predictions for the
Q values so as we did in the simple Q
learning algorithm what did we do we
just selected the one with the best of
the highest Q value once we have the one
with the highest Q value we just take
that action because it just brings us
the highest Q value in look and that we
know that Q values calculate is the
immediate reward that we expect to
receive plus the decay factor times the
value of the next state and it's a
recursive calculation so why not why
wouldn't you take and the best Q value
and that's kind of the end of it but as
you can see here it's not as simple here
we're using a soft max function and this
is where we're going to talk about
action selection policies so here in
reality we don't have to have just a
soft max function we can have different
action selection policies for example
we've got epsilon greedy epsilon soft
and we've got the soft max and those are
kind of like the the most commonly used
action selection policies of course
there are others for instance the most
basic one is here's a very simple action
selection policy just select the best
the one with the highest Q value but why
doesn't that action policy fly and why
do we have different types of action
policy action selection policies well it
all boils down to exploration versus
exploitation and that is the the core of
reinforcement learning Begay because
we've already talked about this a little
bit that your agent when it's operating
in an environment it might predict
certain Q values which which might be
good and it might and it might turn out
great it might turn out that those
corrals are bad and will be forced to
explore so if we for instance in this
case predict that Q 2 is the best one
and then it takes Q 2 takes action - and
it's so from here it takes action - and
then it gets it gets a very negative
reward then their environment is forcing
the agent to go and explore because now
it's going to learn that oh actually I
thought Q 2 is gonna be very good but it
turned out very bad
so so the results are not very bad so
the network's gonna update itself so
next time he's in this state he's gonna
probably eat my soul she's cute too if
it was you know like if it was very very
favorable so you might think that's
that's like you know he might need a
couple of times a couple of penalties or
punishments in order to learn if you
choose a bad action but maybe he'll
already soon learn that okay I'm gonna
take a different action gonna take the
action because now it has the best Q
value so sometimes the environment
forces the agent to take different to
explore different actions but sometimes
the agent might get it find itself stuck
in a local maximum it might find that it
found like through through its initial
exploration it found that oh this is a
pretty cool action like I'm going to go
right here and that that's pretty
collection but the problem is that it
thinks it's the best action simply
because it hasn't explored its explored
going up is go explore going left
it's explored going right but it hasn't
explored going down from that specific
state that it's in and now that it's
kind of like biased towards this action
and thinks so good actually is gonna
keep taking is gonna keep getting he's
gonna keep taking this action is gonna
keep getting a good reward but what if
this action would have been even better
if this action would have been so much
better that if it knew about this action
it would actually switch to this section
but because it got stuck in a local
maximum and is getting these good
rewards it's just going to be reinforced
it's just going to keep reinforcing
itself that or the environments going to
reinforce it that this is a good action
to take keep doing that but really the
reality is that there's this out of the
action that it hasn't found yet or it
hasn't even explored that would have
been much better and so what we want to
do is we want to come up with an action
selection policy that allows our agent
not to get stuck in a local maximum yes
it's important to you know keep doing
the good actions that's the exploitation
part we want to exploit what we've found
but at the same time we still want to
explore we never want to stop exploring
it like in life you never want to stop
learning you stop learning you die
that's there's a saying like that that
when you're not growing you're dying or
something that so you want to keep
learning and your agent wants to keep
learning and that's where these action
selection policies
so we've got three year listed here so
the first one is epsilon greedy it's
it's a very simple one it sounds pretty
complex in the in the sense that like
it's got a cool name and usually things
called names are a comple Sun it's
actually not so basically what it does
is it will select the one with the best
Q value and and Epsilon like epsilen
greed you might hear it in other places
it's just like a selection policy so in
this case we're using it to select
select our out of out queue valleys hour
of action so you'll select the one with
the highest Q value and all the time
except for epsilonpu cent of the time so
for instance if you set epsilon to 10%
then you're going to or 0.1 then 10% of
the time the action is going to be
selected at random so 90% of the time
you're still going to be selecting the
best action based on the highest Q value
but 10% of the time is going to be
selecting a random actions uniform image
is going to be absolutely randomly
taking an action or if you set epsilon
to 0.548 0.05 that means that 95% of the
time the agent is going to be taking the
action with the highest Q value but 5%
of the time it's still going to be
selecting a random action so it's going
to be going out there and exploring so
epsilon soft is very similar now by the
way that that's kind of like why it's
called epsilon greedy because then
you're greedily selecting the action the
good action except for that's little
epsilon percent of the time so that the
lower the abseil the lower the left
Epsilon they're more greedily you're
selecting that kind of the action that
is the optimal action and the less
you're leaving the less chances you're
leaving for exploration epsilon soft is
opposite so basically you're selecting
at random you're selecting 1 minus
epsilon percent of the time so if you
have sounds like 0.1 so 10% then only
10% of the time you're taking this
action and 1 and 90% of the time you're
selecting a random action so very very
simple just inverted algorithms and soft
max is kind of like the next step from
or it's it's a more advanced version I
would say of epsilon of the epsilon
greedy algorithm
they both have merit and they both have
place we're going to be using softmax in
our coding and our practical side of
things so that's why we're going to talk
in a bit more detail about soft max so
let's have a look so let's move on to
soft max hopefully it's pretty clear
about epsilon greedy so it's a pretty
straightforward algorithm select this
one most of the time except for
sometimes go and explore and now we also
see why it's important to do that
exploration so that we don't end up in
local maximums in our in our
optimization process so now we're good
to talk a bit more about South max
there's a tutorial on soft max at the
end of the course in I think it's in
annex
number two where we talk about the
concept behind soft max I'm just going
to refresh a little bit here so they
were talking about convolutional neural
networks and by the way but we are going
to be covering convolutional knew we're
not covering collisional neural networks
in this section of the course in this
section we're still using a vector but
in the next section of the course when
we're we're creating an AI to play Doom
we are going to be using convolutional
neural network so it could be beneficial
for you to look at convolutional neural
networks and then take the softmax
function or you can learn a bit more
about self max after you take the
convolutional neural networks annex of
the course later on but here's a quick
refresher so here we've got a
convolutional neural network which
decides whether it's a dog or a cat so
here we've got the voting process
between these neurons and this one that
says that it's a it's got the features
you know the fluffy ears what the
pointed pointed face type of thing and
the kind of liquor the features that are
the types of eyes like the way the eyes
look all these features that belong to
dogs so it's a 95% chance that it's dog
and the 5% chance that it can but the
question is how did we get and in that
tutorial we're talking about how did we
get these values to add up to 1 well
whatever the convolutional or our whole
neural networks are the convolutional
neural network plus the fully connected
layers whatever it's spat out whatever
the values it's bad out we applied a
softmax function over here and this is
where we introduce them and foul
formula for the softmax function this is
what it looks like and then we got these
values and so basically that's a quick
refresher this is the formula for the
softmax it's what it does is it takes
however many outputs you have doesn't
matter it will take them and it will
squash them all into values between 0 &amp;amp;
1
regardless of how big they are just by
looking at this for me you can see that
there's a total sum at the bottom so
these values are gonna be 0 between 0 &amp;amp;
1 and also the all these values are
going to add up to 1 always and so
that's that's very beneficial for us
because when we're using the softmax
function what happens is we get these Q
values we select this best q value but
in reality what happens is these Q
values that we get there they're actual
numbers right so there's some kind of
numbers they don't have to add up to 1
and don't have to be between certain
ones just some numbers but when we apply
softmax we don't just select the best
one we actually get numbers like that so
we get numbers in the range between 0
and 1 and that are also that also add up
to 1 and so what other thing do we know
that adds up to 1
well probabilities we know that
probabilities always have to add up to 1
so that is why we can say here we've got
Q values but here all of a sudden we've
got soft matter or we've got
probabilities so we can say that the
likelihood of this being the best action
is 90% this basically in the other
sections 5% 2% 3% because we know the
higher your Q value the better the
action and so if we squash them to 0 to
1 then these become probabilities and we
can deal with them as such and therefore
now is when the action is selected and
that's how we come up with Q 2 but if
you look at it closely this isn't a
strict hundred-percent and these are not
0 percents so this is a 5 percent 2
percent 3 percent so the the most
natural way to apply the softmax in
order to preserve exploration in the
algorithm is to use these exact
probabilities as how often we're going
to be taking that action so these per
actually represent the distribution of
these actions that we're taking so
basically softmax makes it very easy for
us to come up with a way to combine
exploitation and exploration so the
better the best action will always have
the highest probability because it has
the highest Q value and therefore here
we're going to be just we're going to
use these as our distribution and we're
going to say okay we're going to be
taking Q to 90% of the time but 5% of
the time we're still going to be taking
Q 1 and 2% of the time we're going to do
3 and 3% of the time we're going to be
taking Q 4 and the beauty here is also
that as these values update as then as
the agent goes to the network more and
more and more it becomes more familiar
with with the environment and therefore
these updates so this value for instance
might become like it might might
entertain that this value is actually
less or this actually is higher and so
these probabilities will also change as
an agent goes through so even though
here we've got to - nobody is to say
that sometimes 5% of the time to be more
precise we'll be selecting q1 as the
action to take and sometimes or action 1
will be taking action 1 sometimes we'll
be taking action through a 2 action 3 2
percent of the time and action 4 will be
taking about 3 percent of time so every
action has a chance to play in this
process as long as we have enough
iterations and agent goes through lots
and lots of times through these states
that they're in and that's that's how
this that's how any kind of deep
learning algorithm works that you want
to do this many many many times so that
you learn from experience and therefore
as you can see here it's a very natural
transition to we're not just randomly
like an epsilon greedy algorithm we're
not just randomly selecting the actions
we're selecting and based on their
softmax values which makes it makes it
like has some logic behind it not just
not just at random 10% of the time we're
selecting a random action but there's
some logic behind how we're doing it and
based on their the Q values that we've
explored and so that's the action
selection policy that we're going to be
using in this course you're welcome to
definitely check out the epsilon greedy
actually
suppose if you like but we're going to
be predominantly using the softmax
action section policy and I've got an
interesting reading for you so this is
called adaptive epsilon greedy
exploration in reinforcement learning
based on value differences it's a 2010
article and it's interesting because
Mike Michele I'm not sure how to
pronounce I'm Michele and we Cal talking
introduces a different type of algorithm
so and adjusted epsilon greedy algorithm
and called the V de V DBE algorithm or
epsilon greedy VDB algorithm you can see
it over here and he actually compares
compressor to the EPS ingredients
softmax and it's an Epson already
algorithm which basically the main idea
behind it is to adjust the value of
epsilon depending on the state the agent
is in so if this if the agent is very
certain about the state they're in then
epsilon should be smaller so there
should be less exploration if the agent
is uncertain epson should be higher
should be more exploration so it is a
2010 article I'm not sure if it's if
this new proposed algorithm is widely
used or is has been accepted in the
community or if artificial challenge has
moved kind of away from this this
suggestion but nevertheless it will
definitely help you reinforce your
knowledge about action selection
policies which we discussed the epsilon
greedy and the softmax will help you
it'll give you an opportunity to compare
them somebody said and also see in which
direction people actually think when
they want to improve artificial
intelligence so if you're ever planning
on creating really interesting
algorithms that are pushing the edge of
artificial intelligence and pushing the
envelope in this space then this could
be a good way for you to see in in which
direction people think sometimes when
they're trying to improve the norms of
artificial intelligence or the norms
that existed back then in 2010 so there
we go hopefully you enjoyed today's
tutorial about the action selection
policies and we learned about epsilon
greedy epsilon soft and the softmax and
now you're even more
prepared for the practical side of
things and on that note I look forward
to seeing you next time and until then
enjoy AI thanks for watching I hope you
enjoyed and if you'd like to learn some
more then check out our artificial
intelligence a to that course on udemy
where you will learn all of this and
much much more including the typical
learning algorithm and the a3c algorithm
you'll also learn how to create your own
artificial intelligence how to create
self-driving cars and how to build AI
which can play games like doomed the
link is in the description below and I
look forward to seeing you there</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>