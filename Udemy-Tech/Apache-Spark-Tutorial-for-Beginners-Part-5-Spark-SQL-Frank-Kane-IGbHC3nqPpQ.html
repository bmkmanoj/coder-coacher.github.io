<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Apache Spark Tutorial for Beginners Part 5 - Spark SQL - Frank Kane | Coder Coacher - Coaching Coders</title><meta content="Apache Spark Tutorial for Beginners Part 5 - Spark SQL - Frank Kane - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udemy-Tech/">Udemy Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Apache Spark Tutorial for Beginners Part 5 - Spark SQL - Frank Kane</b></h2><h5 class="post__date">2018-03-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IGbHC3nqPpQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so now let's talk about spark sequel and
the spark 2.0 way of doing things using
data frames and data sets so the thing
about the directions spark is going in
and said it's stealing more and more
with structured data so when we're
dealing with rdd's they're just these
rdd's of rows of stuff right it doesn't
really have any information built in
about what the types are of that stuff
explicitly so what data frames do in
spark is extend the RDD to a data frame
object and a data frame just contains
row objects where those rows can contain
structured data so you know now you can
have actual structured information in a
data frame that has actual columns of
the given type and given names and once
you have that you can actually run
sequel queries on your data frame with
inspark so again we're kind of blurring
the lines here between the different
technologies on top of a tube they all
tend to seem to converge on sequel at
the end of the day don't they and the
good thing about this approach is that
since we have an actual schema
associated with a data frame we know
what the actual types are within it we
can store that more efficiently we can
transport it across the network more
efficiently and we can also optimize
those sequel queries to make spark even
faster and even more efficient and it
also means we can read and write to
structure data formats such as JSON or
high of databases or park' files
whatever you have and we can communicate
with other databases using JDBC or even
systems like tableau but that gives you
sort of a higher level view of your data
now within Python it's pretty easy to
use the lines again are kind of blurry
because in Python everything is
dynamically typed anyway so all this
talk talk about you know typed and
structured data doesn't really mesh very
well with Python all the time but using
it's very simple basically you can just
import sequel context and row into your
script and at that point there's a
several different ways of creating a
data frame object if you are connecting
to a high of database you can just
create a high of context and read in a
data frame from that or more commonly
you'll actually read in data from a file
somewhere that might be something on
your HDFS file system you can just say
rock reaches on givin a spark object and
create an input data data frame where it
just uses the contents of the JSON file
in the structure that's inherent in it
to infer the column names and column
types of that data and once you have a
data frame you can call create or
replace temp view to actually create
what looks like a little database table
and we're going to call that table my
structured stuff in this example so once
you have that you can actually issue
sequel queries on my structured stuff
and here's an example for example of
running a sequel query on a hive context
that we imported and you could do the
same thing with any data frame just call
dot sequel and actually execute sequel
commands on your data frame you can also
do things more explicitly so instead of
actually writing sequel you can do
things more programmatically you can do
things like dot selects if you want to
select a given column filter to actually
apply some function that you will filter
your data frame by group by just like
group by and sequel it will give you
back the results group by whatever field
you want very much like a reduce
operation right and you can also do
things like dot mean to actually compute
the average of all those grouped results
mmm that might come in handy that's a
that would be a very quick way of doing
what we just did in the previous example
right of finding the average ratings of
all the movies in our data set we'll
come back to that and you can also
remember data frames are built on top of
rdd's so you can always extract the
underlying RDD if you want to and
perform operations at the RDD level as
well so if I wanted to I could call RTD
to get back the RTD of that data frame
that contains row objects and apply some
function to those row objects if I want
to go down deep to the RTD level now
let's spark 2.0 they introduce a new
concept called the data set as opposed
to a data frame the lines between the
two are very blurry and Python because
blurry saw pythons all dynamically typed
but what you need to know is that a data
frame is really a data set of row
objects and data set is a more general
term that can contain any sort of typed
information not necessarily a row like
you have in a data
right so again this is just a way for
spark to actually get more information
upfront about the structure of your data
and it can do that too actually for
example give you more compile time
errors about problems instead of waiting
until runtime that's kind of the real
power of data sets and it also
encourages you to use sequel queries
within your your scripts that actually
allow further optimizations again with
Python it doesn't really mean you have
to do anything differently necessarily
but in Scala and Java that's more of a
big deal okay a little a little footnote
here you can actually start a database
server using spark sequel and connect to
it and query it just like you would any
other database kind of a neat trick so
if you start the thrift server built
into spark you can actually connect to
it through JDBC or ODBC and just start
issuing sequel commands on a given on a
given table that you've actually loaded
up so kind of a cool thing also spark
sequel is very extensible you can
actually create user defined functions
that plug into sequel and create your
own functions that you can use within
your sequel queries here's a simple
example of creating a function called
square that squares a given value and
actually using that within a query so
very easy to do that as well so that is
the power of data frames and data sets
in spark 2.0 and spark sequel another
thing that's worth noting is that this
is sort of the unified API between the
different subsystems of spark going
forward so you'll see that in spark to
the ml machine learning library or the
spark streaming library are all now have
data set based api's that you can use so
data sets are kind of the the common
denominator between these different
systems that allow you to pass data
between them so not only do you get
performance benefits by using data sets
in spark 2 you also get easier ways of
actually using all these other
capabilities built on top of spark where
you can mix and match them in
interesting ways so but with that let's
look at a real example enough talk let's
get some action let's revisit our
example of looking at the worst movies
and this time see how we might use it
might do it using spark 2 and data
frames</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>