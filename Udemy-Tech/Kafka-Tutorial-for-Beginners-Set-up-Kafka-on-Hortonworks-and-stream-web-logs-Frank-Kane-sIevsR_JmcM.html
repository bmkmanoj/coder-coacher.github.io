<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Kafka Tutorial for Beginners - Set up Kafka on Hortonworks and stream web logs - Frank Kane | Coder Coacher - Coaching Coders</title><meta content="Kafka Tutorial for Beginners - Set up Kafka on Hortonworks and stream web logs - Frank Kane - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udemy-Tech/">Udemy Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Kafka Tutorial for Beginners - Set up Kafka on Hortonworks and stream web logs - Frank Kane</b></h2><h5 class="post__date">2018-03-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sIevsR_JmcM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi I'm Frank Kane and I spent nine years
at amazon.com and imdb.com as a senior
engineer and as a senior manager my job
there included making sense of their
massive data sets and I'm here to share
some of my knowledge about wrangling big
data with you if you're in the world of
data engineering or big data you've
probably heard of Kafka it's a really
hot technology for streaming data into a
cluster in real time some people are
going so far as to call it a Hadoop
killer but that's another story what's
important is that it's a very useful
tool in your big data arsenal that
manages the complexity of getting
continuous streams of data from point A
to point B in a reliable and resilient
manner in this video we'll just dip our
toes into cactus water we'll explain how
it works and run a couple of really
simple examples showing you how to
actually use it on a real cluster but
first we need to set up a development
environment if you do in fact want to
follow along that is we're going to
install a Hortonworks sandbox
environment on a virtual machine on your
own PC to work with now you don't have
to do this step you can skip it and just
watch instead if you want to but if you
have a PC handy that has more than 8
gigabytes of RAM installed on it I
encourage you to actually install HDB
and CAFTA yourself and go hands-on with
me and the activities later in this
video so let's start by installing
VirtualBox and the Hortonworks sandbox
and also a terminal so you can connect
to it so let's go ahead and install a
Hadoop stack on your PC it's not that
hard start by opening up a web browser
and the first thing we need to do is
install a virtual machine on your
desktop basically we're going to run a
little Linux environment on your own PC
or Mac or whatever the heck you it is
you're running so to do that open up
your favorite web browser and search for
VirtualBox and that will take you to
virtualbox.org which is free software
for actually running a virtual machine
right on your PC and what there should
be a big friendly download button maybe
it's newer than 5.1 depending on when
you're watching this and you can see
it's available for Windows OS X Linux
and Solaris so go ahead and install the
version that makes sense for you from
you that's gonna be Windows so I'm just
going to go ahead and download that
comes down pretty quickly about 100
Meg's
and then will install it and a standard
Windows install are going on here just
accept the defaults and you'll be fine
all right I stopped things through the
magic of video editing because it does
take a few minutes for that to install
if you're following along with me and I
hope you are go ahead and pause this
video until you get to this screen here
and be sure to watch out for a little
pop-up dialogues that might have come up
in the installation that might be hidden
behind this window so keep an eye out in
your taskbar down there for anything
that you might need to dismiss now we're
not actually gonna start it yet so I'm
gonna uncheck that and click finish
we now have VirtualBox in place which
will let us run a virtual Linux box on
our PC and the next thing we need is an
image for Hadoop so let's go and grab it
from Hortonworks shall we so what we're
gonna do is go to Hortonworks comm slash
sandbox just like that or just search
for Hortonworks sandbox there are
multiple vendors of Hadoop technology I
just find this to be the easiest one to
get up and running with Cloudera is also
very popular I should point out horton
works in cloud era are in competition
with each other but this stuff is all
free so it's fine by us
let's go ahead and download the sandbox
so you'll see a bunch of stuff here you
can choose from what you want to look
for is the Hortonworks sandbox on a VM
for a VirtualBox ok now let me give you
a little bit of a tip here that's gonna
save you a lot of trouble potentially
although this course works just fine on
HDB version 2.6 point three and probably
newer versions as well it's kind of take
a very long time for that version of
Hortonworks to actually boot up on your
system so if you want to make life a
little bit easier I would go down to the
archive here just scroll down to where
it says Hortonworks sandbox archive and
expand that and if you can find version
2.5 that's going to boot up a lot faster
and it will work just as well with this
course find HTTP 2.5 on Hortonworks
sandbox 4 VirtualBox click that button
and just wait for that to come down so
11 gigabytes so we'll
a little bit of time to come down go
ahead and hit pause while that gets
downloaded on your system okay time is
passed and we've downloaded 11 gigabytes
of Hortonworks sandbox goodness all we
have to do now is open up that OVA file
that we downloaded I hope you had enough
room on your hard drive for that 11 gigs
isn't that big these days and we're just
gonna go ahead and click the import
button here to import that image into
VirtualBox so we can fire it up whenever
we want to and this will take a little
bit of time to copy over after all it's
11 gigabytes so again we'll hit pause
here and wait until that's done alright
we've imported our Hortonworks docker
sandbox what is this thing that we just
download it anyway well basically it's a
pre installed Hadoop environment that
also has a bunch of associated
technologies installed as well a lot of
the stuff that we're gonna go through in
this course is going to be already set
up for you by virtue of installing the
Hortonworks sandbox so all we have to do
now is click on it and hit start and
that will basically be the same thing as
pushing the power button on a PC that
has sent OS installed so we're just
gonna sit here and wait for that to boot
up it will just do its thing
automatically it does take a little bit
of time to get up and running so let's
let that go ahead and do its thing all
right we are in business check it out so
we have a little sent os instance
running here that actually has to do up
and running now I'm using something
called putty on Windows and on Mac OS
you might just use the built in terminal
instead but if you're on Windows you'll
want to look up putty and if you go to
putty or you'll find where to download
that from and you can just download the
actual executable there and and run it
so I've actually attached that to my
let's close this so once you install
putty it will look something like this
now to connect to this virtual machine
that I'm running from my local machine
what I'm gonna do is open an SSH session
to it and what I'm gonna do is for the
hostname I'm going to type in Moorea
underscore dev at 127 dot 0 dot 0 dot 1
and this particular sandbox is exposing
telnet on port 2 2 2 2 as opposed to 22
so make sure you enter that as well
now I'm going to save this as HDPE so I
can get back to it easier next time
okay so let's hit open and I'm just
going to type in for the password Maria
underscore dev and I'm in cool so now
that we're logged in as Maria dev we
have to set up a couple of other
accounts so that we can log in with
elevated privileges when we need them in
different situations
let's start by setting up our root
account so let's type in su that stands
for super user root and the initial
password on the root account is Hadoop H
ad oo P all lowercase alright now the
first time you log in as root it will
require you to change the password so
start by entering in the current
password which is again Hadoop H ad oo P
and now type in whatever your favorite
password is one that you'll remember
type it again to make sure you didn't
fat-finger it and now we were signed in
as root as indicated by the prompt their
route at sandbox Maria underscore dev
now you don't want to stay logged in as
root generally speaking because that
gives you a lot of dangerous permission
so that hash signed there by the way on
the profit side of the dollar sign is
another cue that you're currently logged
in as root so let's type in exit to get
out of that now we also want to set up
an admin user and this is going to be
used from the web UI for Hortonworks and
to do that we need to be well logged in
as root so let's log in as root again
and make sure that that still works su
root type in your new password and
hopefully you are successfully logged in
as root now we need to type in the
following command Ambari - admin -
password - reset and you'll be prompted
to type in your favorite password for
the admin account as well whatever you
want it to be type it twice and we now
have an admin user being set up that we
can use later on in the course and that
will allow us to use web UIs for
starting services and things like that
give that a moment or two to start up
and you can see it automatically
restarted the Ambari server will talk
about Ambari later but the other thing
you need to do is type in Ambari - agent
restart just to be safe alright so now
we've got admin and routes set up we can
go ahead and exit out of the root
account to be safe and we can move on so
now we're going to talk about streaming
and this is the process of actually
publishing data from some source like
web logs or sensor data or something
like that and actually getting that in a
scalable manner published into your
cluster where you can actually do some
processing on it maybe that processing
is also done in real time so this is
called streaming and we're going to talk
about in the next section just how you
go about publishing that data from your
data sources into your cluster in real
time so that you can then process it
let's dive right in the first technology
we're going to talk about for streaming
data into your cluster is going to be
called Kafka this is a published
subscribed messaging system so what is
streaming anyway what are we talking
about well we've been talking a lot so
far about processing data on your
cluster using tools like hive and Pagan
spark but we're assuming that your data
is already on your cluster somewhere it
had to come from somewhere though right
like it didn't just magically get onto
your HDFS filesystem or it didn't
magically get into a database sometimes
you want to process new data as it's
coming in and you don't want to deal
with having to load it manually all the
time in these big chunks right so that's
where streaming comes in with streaming
technologies such as Kafka you can
actually process new data as it's
generated into your cluster maybe you're
gonna save it into HDFS maybe you'll
save it into HBase or some other
database or maybe you'll actually
process it in real time as it comes in
you can do all of that with streaming so
that might there are many applications
of this for example you might be
monitoring customer behavior data coming
from the logs on your web servers you
might want to be transforming those logs
into databases into more structured
forms you might have sensor data coming
in from some big internet-of-things
deployment right you know or you might
be dealing with stock trades coming into
real time who knows it could be anything
but usually when we're talking about Big
Data there's a big flow of it coming in
all the time and you want to be dealing
with it as it comes instead of storing
it
been dealing it with it in batches so
streaming lets you publish that data in
real-time to your cluster and you can
even process it as it comes in if you
want so there are two different problems
to this whole scenario of streaming one
is how to get the data from your data
sources into your cluster so you might
have a very widely distributed cluster
of web servers or sensors or
what-have-you and you need some
mechanism for being able to publish
those to your cluster in some scalable
and reliable manner and then the second
problem is what you do with it once it
gets there so we're gonna focus in this
section just on that first problem how
do I actually publish data from my data
sources into my cluster at scale so
Kefka is one popular solution for this
it's not just a Hadoop thing it's a more
general-purpose
published subscribed messaging system so
what does that mean well you can set up
a cluster of Kafka servers and their
entire job is just to store all incoming
messages from publishers which might be
a bunch of web servers or a bunch of
sensors or who knows for some period of
time and as it comes in it will store
them up and publish them to anyone who
wants to consume them now these messages
are associated with something called a
topic and that represents a specific
stream so for example you might have a
topic of web logs from a given
application or a topic of sensor data
from a given system right consumers
basically subscribe to one or more
topics and they will receive data as its
published and the good thing is that
Kafka because it stores it it can your
consumers can catch up from where they
last left off so it will maintain the
point where each consumer left off and
allow them to just pick up whenever they
want to so it can publish data in real
time to your consumers but if your
consumer goes offline or just wants to
catch up from some point in the past it
can do that too so it's very flexible
and how it can manage these sorts of
things that's one thing that Kefka is
especially good at that other systems
aren't so good at managing multiple
consumers that might be at different
points in the same stream and it can do
that very efficiently and again Kafka is
not just for Hadoop you can use this for
any sort of application outside of
Hadoop as well that requires some sort
of publish/subscribe mechanism that is
scalable and reliable and I can tell you
from firsthand experience that is
actually a very very hard problem
especially in the face of an unreliable
network anyway architectural e
this is basically my retooling of an
image from the caf-co website but you
can think of a caf-co cluster as being
at the center of this entire world of
streaming data here so that might
represent many processes running on many
servers that are distributing out your
cactus court storage and Kefka
processing now producers are the things
we talked about that are generating the
data so these might be individual apps
that might be listening for new loglines
they might be listening for new sensor
data but somehow they are applications
that have been written to communicate
with both the thing that's generating
your data and to kafka so these apps are
responsible for pushing data into your
calf kkuk luster now on the other end
you can have consumers that just receive
that data as it comes out so as
producers published messages to topics
these consumer apps might also be public
might be subscribing to those topics and
receiving that data as it comes in so
these consumer apps also Lincoln the
caf-co libraries to be able to read that
data as well and process it in some way
so for example there you might have a
spark streaming app that is actually
configured to talk to a specific topic
on a specific Africa cluster that
receives data in real time from these
apps up here now usually you know there
are gonna be existing apps you can use
off-the-shelf you don't always have to
write them from scratch so even though
it says app that doesn't necessarily
mean you're gonna be developing an app
in order to use Kafka often there are
ones you can just use off the shelf and
Kafka even comes with some built-in that
might serve your purposes so don't be
too scared about that you can also have
connectors in Kafka where they're just
plug-in modules for various databases
that will automatically either publish
new rows in a database as messages on a
topic to Kafka or can receive new
messages on a topic from Kafka so you
can set up a database to automatically
publish changes into Kafka or to
automatically receive changes that come
into Kafka as new rows in a database so
you can see that can be pretty powerful
stuff you know you can monitor an
existing database throw that into a CAF
kotappa canned you can have some other
application that's listening for changes
to that database and processing it as
they come in or you might have some
producer of data that's publishing data
into Kafka that you want to store more
persistently and you can have a database
just connected to Kafka that's listening
for all that new
and automatically saving that data to
new rows in some database table also one
last thing you can do with CAF is what's
called stream processors and what these
can do is transform data as it comes in
so your producer for example might be
producing maybe unstructured raw web log
lines out of a web server you might have
some stream processor that listens for
new lines new log lines from that log
data extracts the information you care
about it in a more succinct and
structured format and then republishes
that on a new topic back into the paska
so think about what might how you might
set this up let's say you have a bunch
of producers to just listen for new logs
on a web server you know every access
that your web server gets over your
entire fleet of web servers for some
giant website you can have a stream
processor that processes those log
entries in real time extracts the
information you care about which might
just be a few fields and then republish
this add on a new topic which could go
to a database connector to be stored
more persistently so that's an example
of like the power of Kafka's
architecture you can make these pretty
fancy systems that are very scalable
very fast that can transform data and
store it and do whatever you want with
it really as it comes in
how does Kafka itself scale well like we
said Kafka can be spread out on a
cluster of its own so you definitely
don't want a single point of failure
with Kafka you can actually have
multiple servers each running multiple
processes and those will distribute the
storage of the data on your topics and
the processing of all the publishers and
subscribers connected to Kafka or
consumers and producers in caf-co
terminology you can also distribute the
consumers themselves so let's say you
have a consumer group that it's all a
bunch of consumer servers that are all
subscribed to the same consumer group so
these guys are configured to have the
same consumer group name in this
situation when Kafka publishes data to
that consumer group that's subscribed to
a given topic it will distribute the
processing throughout that entire group
so in this situation you might have four
servers that are set up to process data
on some given topic and Kafka will
automatically distribute messages
throughout that entire cluster of
computers there so as new data comes in
it might send one message to see for and
another message see six and the next
message
see three and that way you can actually
scale out the processing the consumption
of that data
you can also set things up so that each
consumer has its own group and in that
case each individual group will get its
own copy of each message so that's what
this image here is trying to show I just
looked at that from the CAF qey website
but the point is you can set things up
to distribute the processing of data as
it comes in to a consumer group or you
can actually replicate that data to as
many groups as you want it's up to you
so with that background let's play
around for a bit let's kick off the
kappa service on our Hortonworks sandbox
first thing we're going to do is set up
a topic and just using some command line
tools publish some data into it and
watch it gets consumed by another
consumer that's also running from a
command line and that is a more
interesting example we're going to set
up a file connector and what it's going
to do is actually listen for changes to
a given text file on our sandbox and
publish that through Kafka and actually
write that out to another file somewhere
else and we can also listen to that from
another consumer as well so let's see if
caf-co really works and get more
familiar with it and get our hands dirty
let's dive in let's mess around with
Kafka there's a reason I save this
toward the end of the course it's
actually it can get a little bit
confusing when we're dealing with all
these different producers and consumers
running at the same time together so pay
attention it's easy to get lost here
first thing I need to do is start CAF
qey and fortunately it comes
pre-installed on Hortonworks so all I
have to do is go to Ambari go to 120 7.0
that's 0.1 8080 and let's login as admin
and now I can just click on CAF kheh
over here on the service list and go to
service actions and say start confirm
and I should start right up
cool all right so let's mess with Kefka
now that's the server is running so we
have a little kapa cluster of one going
on here if you will let's log into our
Hortonworks sandbox as we always do
through SSH marina underscore dev is our
default account and let's first of all
see where kefka lifts on HDB on
Hortonworks so let's go to CD / user / h
DP / current / kefka broker and here's
where all the caf-co binaries and
configuration live so if we go into the
bin folder we can see what's in here and
have some fun so let's first start off
by creating a calf qey topic to work
with and play around with so what we're
going to do is create a topic in calf
gay and if you remember a topic
corresponds to a specific stream of data
and then we can publish data to that
topic and consume it and see how that
all works so let's go ahead and say dot
slash Kafka - topics dot SH - - create
so that means we're going to create a
new topic
now Kafka depends on zookeeper to keep
track of what topics exists so we're
gonna say - - zookeeper and sandbox
fortune works.com which is the name of
our sandbox server here : 2181 which is
the port that's running zookeeper we're
on say replication factor 1 since we
only have one server anyway partitions 1
since we only have one server anyway and
now finally the topic name let's call it
I don't know Fred it could be anything
they get squid if you want I don't care
yeah just make sure you remember what it
is so now to make sure that it actually
got created we can say dot slash Kafka -
topics dot SH - - list and again it
needs to know we're zookeeper lives to
fulfill that request sandbox dot
Hortonworks dot-com slash 2181 and I
should give us a list of all the topics
that have been created on this instance
and in addition to some that I
set up earlier you can see that fred is
indeed in the list of active topics in
zookeeper so yay we have a stream name a
topic name of Fred let's actually
publish some data into it shall we
so for that we can use the Kafka -
console - producer SH script and this is
just a sample producer app that talks to
Kafka and publishes data on a stream to
it so you know you might write your own
app that does this or you might have an
app that uses this script actually as a
gateway so we can say - - broker - list
so this specifies the actual kefka
server itself sandbox Hortonworks com
slash six six six seven is the port
we're gonna use because I know that's
open - - topic Fred okay now what this
is gonna do is listen to standard input
and broadcast anything any line that it
sees on standard input on the Fred topic
through Kafka now since I'm not actually
redirecting a file into this or anything
it's just gonna listen to the console
for keyboard input so let's go ahead and
kick that off and now it's just
listening for data so I can type in this
is a line of data
I am sending this on the Fred topic okay
so what's happened now is we've
published two messages to the Fred topic
of Kafka the calf cos server is sitting
there and holding on to these it's
stored them just waiting for consumers
to well consume them so let's do that so
let's open up a second console window
here to our sandbox to do that I'm going
to right click on the putty icon and
click on HDPE again and we will log in
again as Maria underscore dev so we have
a second session going on here so my
other session as you recall is dedicated
to publishing messages - Kafka on the
Fred topic using the Kafka console
producer script so now I'm going to use
the kefka console consumer script to
actually get those messages back so
let's CD back into where Kafka lives -
user slash HTTP slash current / Kafka
broker - slash bin and from here I can
run dot slash Kafka console consume
sh - a bootstrap - server and this is
gonna be one of the Kafka's servers
where we can figure out how to get the
data from the topic that we want and
that will just be our one host that we
have that work with your sandbox dot
Hortonworks comm :
six six six seven same port that we
published on we also need to tell it
where zookeeper lives - a Tsuzuki /
localhost : 2181 and we can say we want
to listen to the topic Fred and we can
also optionally say I want you to give
me everything from the beginning of time
for this topic so if I didn't say - -
from - beginning I would only get new
messages published ich - Fred I would
not see the - that I already published
so let's go ahead and kick that off and
see what happens
I should see this is a line of data I am
sending on the Fred topic cool so those
are the two messages that we sent
earlier from our other session let's
actually see it working in real time now
that we have everything connected so
here in this window I have a CAF
consumer receiving messages on the Fred
topic and over in this window let's just
put them side-by-side I have a CAF cup
reducer sending messages on that topic
so I can type in anything well I want
over here and should show up over here
so let's say here is yet another line
and sure enough it showed up over on the
consumer window so it may not seem that
impressive just looking at this simple
example but when you think about how
scalable this is that's the key right
you know I wouldn't in the real world
just be sending little arbitrary lines
of data here and there this would be
massive floods of data coming in from
like massive web server fleets and
things right
it can handle that and publish it
scalable and reliably to wherever it
needs to go so that's really what it's
all about it's a scalable and reliable
mechanism for publishing and subscribing
to massive data streams so very simple
example there of it working in our next
lecture let's talk about a little bit
more of a complex example and actually
use a calf connector to do something a
little bit more fancy before we do that
though let's ctrl C on both of these
processes so we can get out of them
cool so publishing arbitrary lines of
text that we typed into a console is fun
and all but it's really not terribly
useful in a production sense right so
let's do something a little bit closer
to what you might do in reality let's
use a built in Kafka connector to
actually monitor a file and publish new
lines on that file to a given Kafka
topic that then get written out to some
other files somewhere else so this is
getting a little bit closer to what you
might be doing for say log processing
where you have a web server that might
be has some error log or something being
written out or some access log being
written out continually as new traffic
comes to your web server and you want to
publish those new lines on your logs
into Kafka where they can all be
aggregated from all of your web servers
into one topic and store it somewhere
else so to do that we're going to use
again what's called a connector and
Kafka has a file connector built in that
we can just use but we have to configure
it the way that we want so the first
thing we need to do let's pick one of
our sessions here how about this one and
we'll CD back up out of the bin folder
of Kafka and you can see there's also a
con folder and inside here we have some
sample configuration properties for some
of the connectors that come with Kafka
so you can see we have a connect
standalone dot properties file and
that's what we're gonna use to set up
the network environment for our
standalone connector server here because
we're just running on one computer
there's also a connect file sync
properties that describes the properties
of how we're going to store the contents
of that stream as it comes in so the
sink is what's going to be the consumer
of that stream and dumping the output
into a file of our choice and there's
also a connect file source top
properties file that can be set up to
listen to changes on a given file that
we specify and publish that as a
producer on a topic on a stream as well
so since these are part of Kafka itself
we don't want to directly modify these
files so let's start by making copies of
them someplace safe where we can mess
with them so I'm gonna copy these into
my home directory so let's say CP
connect
- standalone dot properties into tilde
which is just a shortcut for my home
directory for Maria dev
and we will do the same thing for
Connect - file - sync dot properties and
for connect - file - source stop
properties okay so now that we have
these in our home directory let's CD
into our home directory CD tilde and
edit them and for lack of a better
editor will just use VI so let's type in
VI connect
- standalone dot properties and the main
thing we need to change is the bootstrap
server to specify the host that we're
running on and the correct port so I'm
gonna hit I'm gonna go down there where
the arrow keys and then hit the i key to
go into insert mode in VI move to the
end of this line delete that host and
substitute my own
which is sandbox
Hortonworks comm : 6 6 6 7 ok that's all
I need to change in this one so I can
escape to get out of insert mode : WQ to
write and quit this file now we need to
also modify the connect file sync which
is the file we're going to write our
results into and what I need to do is
change the file name for one thing where
I actually want to write it so I again
to get into insert mode go to the end of
that and I will type in where I want to
store the results so let's listen and
store this stream in to slash home slash
Maria underscore dev slash
logout dot txt so output from the log
that we're listening to and we need to
specify the topic that we're going to
listen to what should we call it how
about I don't know log - test make it
meaningful right escape : WQ and let's
finally edit the file source properties
similar exercise here so I to go into
insert mode the file we're going to
listen to is going to be slash home
slash Maria underscore dev slash access
underscore log underscore small dot txt
and that's just going to be a sample of
an actual access log I have from a real
web server that I run
so kind of a real-world example here now
if it was a real web server who probably
just be called access underscore log or
something
most HTTP web service or Apache at least
will write all of its access log entries
into an access log folder inside the log
folder of the HTTP server and rotate
those logs off periodically so that
would be all you would need to do in the
real world for publishing new lines of
your access log from a web server I also
need to change the topic make that
consistent with the one that we're
listening to which was log - test escape
: WQ alright everything should be
configured so let's get that access log
we were talking about I put a copy of it
for you at W get HTTP colon slash slash
media dot sundog - soft comm slash hadoo
/ access underscore log underscore small
dot text and there you have it take a
little peek at it let's access log small
text and you can see these are our
actual log entries from my no hate news
column website check it out if you're
curious and you can see I'm getting
crawled by some robot here it seems so
actually it's a or it might be my cache
I don't know it doesn't matter it's
locked sir it's centuries from an access
log on a web server the cue to get out
of that alright so before we start
publishing data from that log file let's
set up a consumer to actually listen to
it on the side here as well so in
addition to using our connector to both
publish and consume this log data's
information and store it off to a file
I've also got to set up another consumer
over here to just kind of watch it in
real time so so let's set up our
consumer here to just print out what's
being published on that test log topic
we're in the bin folder here already
inside the calf car broker folder so I
can just say well let's remind you where
we are here dot slash Kafka - console -
consumer god SH - bootstrap server
sandbox dot Hortonworks
dot-com : six six six seven so again
we're telling it where to connect -
Kafka
- topic the topic name we're listening
to is log - test and - as zookeeper
remains running on a local host : 21 81
all right so at this point we are
listening to anything being published on
the log test topic at the same time that
we're using our connector to both listen
to changes on our access log and publish
changes to it so back to our other
console here let's actually kick off our
connector that will listen to changes on
that file and publish them out and we'll
go back to user HTTP current Africa
broker slash bin and we're going to say
dot slash Connect
- standalone which sets up our little
standalone connector and we're going to
give it as parameters the three property
files that we edited earlier so that's
going to be tilde for our home directory
slash connect
- standalone dot properties then we'll
give it the configuration for the source
the publisher which is tilde slash
connect file source stop properties and
the configuration for our consumer which
is connect file sync dot properties
all right let's kick it off this will
take a few seconds to get spun up and
check it out it's humming away and there
it goes so pretty cool so over here we
see that we received the lines of from
our log they're pretty cool and that
came across automatically from our
connector that just listened to changes
on that log file and publish them on the
log test subject so this is a little bit
closer to a real-world scenario where we
have a connector that's just listening
for changes to a log file and publishing
them on this topic through Kafka through
some system that listens for them now
let's open up a third window here I'm
going to right click on putty here and
go to create another HTTP session and
we'll log in again as Maria underscore
dev that's something I can type it
alright so let's do a few things here go
to our home directory and we should see
a log out text file let's see what's in
it
sure enough there's a copy of our log in
there so that was the connectors
consumer that was automatically
publishing data to a file in a different
location so if you remember we set up
our standalone connector to listen to
changes on our log file and then publish
them to this logout dot txt file
somewhere else
so in the real world that would probably
be in a different server maybe an HDFS
somewhere right and we're also listening
to it from this consumer over here so
we've got two consumers going on at the
same time for the same publisher and
let's just show you again that this is
really working in real time so let's add
a line to our access log dynamically
here we can say echo this is a new line
slash in quote and append that to access
log small dot text and we should see it
come through yeah sure enough right away
this is a new line popped up over on our
consumer up here so there you have it it
worked let's go ahead and exit out of
this and ctrl C out of both of these and
we can shut everything down control C
and exit exit and if we want to really
clean things up we can go back to Ambari
and shut down the Kafka service because
we are done with it for now all right
and thus ends our playing with caf-co
let's move on to our next streaming
technology thanks for watching if you'd
like to learn more check out my ultimate
hands-on hoodoos course on udemy in it
you'll learn over 25 different
technologies in the Hadoop ecosystem and
build up a powerful arsenal against your
big data just use the coupon code in the
description below and you'll get a large
and special discount see you there</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>