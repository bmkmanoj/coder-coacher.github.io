<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Apache Spark Tutorial for Beginners Part 4 - Using RDDs in Spark - Frank Kane | Coder Coacher - Coaching Coders</title><meta content="Apache Spark Tutorial for Beginners Part 4 - Using RDDs in Spark - Frank Kane - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udemy-Tech/">Udemy Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Apache Spark Tutorial for Beginners Part 4 - Using RDDs in Spark - Frank Kane</b></h2><h5 class="post__date">2018-03-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/F6xNTLjnnMw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so to make this real let's take a look
at an actual spark script written in
Python that will answer the question
what are the worst movies in the movie
lens data set now we're going to find
worst as the lowest average rating so
let's dive in and take a look at a spark
script that I wrote in Python that will
answer that question now if you were to
think about this in terms of MapReduce
that's very similar to how you would
think about things in spark one and
that's using the RDD interface we'll
take a look at the spark two way of
doing things later on but for now let's
just use the the lower-level RDD
interface of spark so how would you
actually tackle this problem from a
MapReduce standpoint well if you need to
figure out the average of something you
need a total of how many times that
thing occurred and the total of that
value so what we need to end up with is
the sum of all the ratings for each
unique movie ID and the number of times
each movie was rated so you can imagine
a world where we're just going to reduce
down these tuples of the sums of each
rating and the number one and boil it
down to an average at the end of the day
so that's the approach we're going to
take let's dive into the code here and
see how it all works so we will run this
in a moment but for now I just want you
to kind of understand what's going on
here so some you might be new to Python
basically the way that we import
libraries that we need is a syntax here
so we're saying from the PI spark
library that allows us to do spark stuff
within Python we're going to import
spark conference spark context which
will give us the hooks we need to
actually create rdd's and do stuff with
them let's skip down to the actual main
meat of the script here so like I said
we first need a spark context to work
with to create our initial RDD from and
we're going to do that by creating a
spark conf a configuration class that
just sets the name of the application to
something called worst movies so if we
were to look up this job running in our
dashboard it would be under the name
worst movies and there's other things
you can do with spark off like set
exactly how it gets distributed what
cluster it runs on and how much memory
is allocated to each executor and things
like that but let's keep it simple for
now so given that configuration we can
create our spark context and we're going
to call that spark context s
so we have a variable SC that is our
spark context the environment that we're
running within next thing we're going to
do here is not really a spark thing
we're just kind of load up a Python
dictionary that lets us quickly map
movie IDs to movie names and since I
know that's a small amount of data it
will fit in memory I don't need to
create an RDD for that I'm just gonna
keep that in a Python dictionary so I
can use that while printing out our
final results when we're done
so while load movie names does is load
up the u dot item file in our ml - 100 K
data set from the local disk okay this
is not on our cluster it's just data
that we expect to be deployed alongside
with our script go through each line of
that and creating dictionary that map's
movie IDs as integers to the movie names
and that gives us back a dictionary
called movie names that we can use later
on but back to spark so going back to
our spark context SC we're gonna call
text file on it with a path on HDFS to
the U dot data file so this is the
actual movie ratings data set where each
row remember contains a user ID a movie
ID a rating in a timestamp and it's
going to load that from HDFS so that
could be you know spread out across our
entire cluster potentially and we're
gonna load that into an RDD called lines
so already you're seeing how our DDS
abstract away a lot of complexity I
don't have to think about how you dot
data is actually spread across HDFS and
how it deals with failure if one of
those notes goes down all I care about
is I have an RDD object that I've named
lines that I can do stuff to in my
driver script makes life a lot easier so
the first thing we're going to do is
actually convert that input data into a
structure that looks like this so we're
going to create a new RTD named movie
ratings that contains a key value pair
where the key is the movie ID and the
value is this tuple of the actual rating
value and the number 1 so our strategy
again is to sum up all of those ratings
and sum up all those ones to get a count
of how many times each movie was rated
and from that we could create it we can
compute our average ratings so to do
that we've written a function called
parse input let's take a look at that
all it does is split up the input data
and returns back exactly that structure
a key value tuple where the key is the
integer cast movie ID
and that's field one because we start
counting from zero and Python and then
we extract field 2 which is actually the
third field the actual rating cast that
is a floating-point number and the value
1.0 so we have a key of the movie ID
integer and the value is a tuple
containing the actual rating value as a
floating-point number and the number 1.0
and you can see we've defined that as a
function we just pass in that function
name to our map function and that will
apply that function to every input line
of the lines our DD and output a new r
DD called movie ratings so at this point
we have a new r DD called movie ratings
that contains this structure a movie ID
rating in the number 1 okay next thing
we're gonna do is reduce that by key so
we're gonna sum up all of the ratings
and all of those number ones using this
lambda function so instead of actually
defining another in another function
like we did with parse input we're just
going to make that in line using the
lambda syntax here now when you do a
reduce operation on an RDD you get
passed into values so what happens is
that for each unique key you're going to
be passed in a pair of values that need
to be combined together in some way your
function here that you pass in to reduce
by key is responsible for combining two
values together for a given key and
we're saying the way we're gonna combine
them is to add up both that rating and
that number 1 by extracting field 0 and
field 1 from both the the first value
and the second value and reducing that
down to the sum as we go along so once
we're done with that operation what we
end up with is a new RTD called rating
totals and count that contains each
unique movie ID the grand total of the
ratings for that movie and the grand
total of how many times that movie was
rated next thing we can do is convert
that to actual averages so now we're
gonna do a map to just transform all of
those sum and total tuples into an
average and that's all this does here it
takes each value of the ratings total in
count and transforms it using this
lambda function by just dividing the sum
of ratings by the total ratings to get
back an average so now we have an
average ratings RDD that contains each
unique movie ID
and their average rating okay so this is
a lot to wrap your head around if you're
new to this stuff or if you're new to
Python so feel free to hit pause right
now and just stare at that for a little
while let it sink in and if you look at
this long enough I think it will make
sense okay so if you need to take some
time this is a good point to actually
stop and kind of like study these last
three lines of code in particular to
understand the flow of how that's all
happening okay
so at this point we have the values that
we need we have a map of we have an RDD
that contains movie IDs and the average
rating all we have to do now is sort the
results in print them out so we're going
to call sort by and that basically says
okay what value do I want to sort by and
we pass in a function that says okay I
want to extract the second field which
is the average rating remember we start
counting from 0 so 1 is actually the
second field in Python so that says I'm
going to take the average rating sort it
by that second field the average rating
so now I have a new RTD called sorted
movies that is sorted in ascending order
by the average rating so the top of that
should be our worst movies I can just
call take 10 to take the top 10 worst
movies and at this point I have a Python
list called results that I can just
iterate through and print out so this is
the point where we go from Rd DeLand to
Python land if you will so this will
actually say collect the final results
from the cluster and return them back to
my driver script so I can just print
them out one at a time and here as I'm
going I will use that movie names
dictionary that we created earlier to
print out the movie name from the movie
ID and the average rating it's just that
simple
so let's actually run it shall we
now I've already sparked up my little
virtual box here my virtual Hortonworks
Hadoop cluster so that's already running
and I'm what I'm gonna do is first
things first we need to configure spark
the way that we want so let's open up
Ambari and again you go to 1:27 dot 0
dot 0 dot 1 colon 8080 in your browser
to get to that once your Hadoop instance
is running and I'm going to sign in with
the admin account that we created
earlier in the course because I'm going
to actually change some configure
and spark and here we are so let's go to
the services here and we'll go to spark
okay and from here we're going to go to
configs so what we need to do here is
change the log level of spark by default
it's a very verbose and it's hard to see
your results when you're actually
running the script as a result so we're
gonna go down here to where it says
spark log4j properties and see here
where it says locked for J root carrot
category equals info double click that
and change it to error okay and hit save
when you're done typing a little note of
what you did change the log level for a
spark to err rorr and it will save that
but before that takes effect we need to
restart the services so go ahead and hit
this restart button here and say restart
all effective I'll confirm restart all
and it will go off and actually restart
the services for spark to make that
configuration change take effect
shouldn't take too long okay now let's
do the same thing for spark 2 while
we're in here just to get it over with
same thing we're gonna click on spark to
go down to log4j properties change the
root category to error hit save type in
a note of what we did change log level 2
error and spark 2 and again once that's
saved we need to restart spark 2 as well
and we'll let that go do its thing 2
okay while we're in here let's make sure
you have the data you need on HDFS so
let's go up to the files view here and
go into the user maria dev and make sure
you have an ml - 100k folder in here
that contains your data and you item
from the movie lines data set that you
downloaded earlier in the course if you
need to go ahead and create that at this
point you should have it from earlier
lectures but you can always create new
folders and upload data into it from
this UI if necessary but make sure those
are in place okay so you should have
user maria dev ml 100 k u dot data and
you item alright so we have everything
we need in place on the cluster let's go
ahead and actually run this bad boy so
i'm going to open up a terminal to my
cluster like we've done before all right
so i need to download both the scripts i
need to run and the data remember our
script depended on the u dot info file
to actually get those movie names so i
need to download that so take a look at
what's in my home directory here you
might already have this but if not make
der ml - one i can't type 100 k and
we'll CD into ml - 100 k and i've
uploaded a handy dandy copy of that for
you so if you need to you can type in w
get HTTP colon slash slash media dot a
sundog - soft comm / Hadoop / ml - 100 k
/ u dot item alright so now I should
have au dot item file within my ml - 100
K folder and my driver script will
actually use that to create that
dictionary of movie IDs - movie names
now I need to get the scripts itself
there's a bunch of scripts associated
with this section so let's go ahead and
get them all at once so again I've CD
DUP I'm in my Maria dev home directory
here okay should look like something
like this at this point and I'm gonna
type in W gets
HTTP colon slash slash media dot sundog
- soft calm slash Hadoop slash mark zip
capital s and that should come down here
make sure it's there and I can unzip
that to get my actual scripts here so
the one we're gonna run is lowest rated
movie spark py make sure it's there we
can do a little s lowest rated movies
bark py and see that is indeed the
script that we were looking at earlier
so let's run it to do that we're gonna
use the sparked submit commands so when
you're running spark Python scripts you
don't want to just use Python to run it
that won't work you want to use the
spark - submit shell which basically
sets up all the spark environment for
you and make sure that it's actually
running on your cluster as opposed to
just running in a single process on your
local machine here now since we only
have one machine here it doesn't really
make a difference but normally it would
so spark - submit is we're gonna type in
lowest rated a movie spark dot py just
like that and that will actually run it
so let's uh see what happens
caching and it's chugging away and
there's our results that didn't take too
long so the worst movies in the movie
lens dataset are these bad boys and a
lot of them are tied so I'm thinking you
know a lot of these movies were probably
only watched by one person and that one
person gave it a one-star rating so we
can do better we can think a little bit
more about what it really means to be a
bad movie later on in this section but
for now there is some stinkers in there
for sure the bloody child a medieval
doll house I've never even heard of most
of these and it's probably a very good
reason for that because they're probably
pretty bad movies so there you have it
spark in action and sparks emit actually
can take a bunch of parameters as well
so you can actually pass in command-line
parameters on sparks submit to specify
which cluster you want to run on or
specific things like how much memory you
want to allocate to each executor on
your cluster and stuff like that usually
though your
your cluster will be pre-configured to
actually have reasonable default values
for you so unless you're a Hadoop and
administrator you usually don't have to
think about that too much that's all you
need to do so there you have it you
actually ran your first spark script on
a real cluster on your little sandbox
here and got some real results so up
next let's talk about another way to do
it using this smart to dataset API</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>