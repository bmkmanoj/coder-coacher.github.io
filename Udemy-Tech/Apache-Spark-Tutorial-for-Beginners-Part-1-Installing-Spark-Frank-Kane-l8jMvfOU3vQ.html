<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Apache Spark Tutorial for Beginners Part 1 - Installing Spark - Frank Kane | Coder Coacher - Coaching Coders</title><meta content="Apache Spark Tutorial for Beginners Part 1 - Installing Spark - Frank Kane - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udemy-Tech/">Udemy Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Apache Spark Tutorial for Beginners Part 1 - Installing Spark - Frank Kane</b></h2><h5 class="post__date">2018-03-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/l8jMvfOU3vQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi I'm Frank Kane and I spent nine years
at amazon.com and imdb.com as a senior
engineer and as a senior manager my job
there included making sense of their
massive data sets and I'm here to share
some of my knowledge about wrangling big
data with you
Apache spark is by many accounts the
hottest technical skill to have in the
field of big data right now and with
good reason it's insanely scalable fast
and powerful with libraries that can
even handle machine learning on massive
datasets with just a few lines of code
in this series of videos I'll get you
started with the basics of Apache spark
and just gonna teach you enough to be
dangerous here now if you want to follow
along with the activities and the
exercises yourself you first need to
install a development environment you
don't have to you can just watch if you
want to but if you have a PC that has at
least 8 gigabytes of free RAM that's
free Ram not total ram watch the rest of
this video if you'd like to install
spark we'll do it by installing the
Hortonworks sandbox on a virtual machine
running on your PC and connecting to it
using a terminal just like you would in
the real world let's get started so
let's go ahead and install a Hadoop
stack on your PC it's not that hard
start by opening up a web browser and
the first thing we need to do is install
a virtual machine on your desktop
basically we're going to run a little
Linux environment on your own PC or Mac
or whatever the heck it is you're
running so to do that open up your
favorite web browser and search for
VirtualBox and that will take you to
virtualbox.org which is free software
for actually running a virtual machine
right on your PC and well there should
be a big friendly download button maybe
it's newer than 5.1 depending on when
you're watching this and you can see
it's available for Windows OS X Linux
and Solaris so go ahead and install the
version that makes sense for you from
you that's gonna be Windows so I'm just
gonna go ahead and download that comes
down pretty quickly about a hundred
Meg's and then we'll install it
and a standard Windows installer going
on here just accept the defaults and
you'll be fine all right I stopped
things through the magic of video
editing cuz it does take a few minutes
for that to install if you're following
along with me and I hope you are go
ahead and pause this video until you get
to this screen here and be sure to watch
out for a little pop-up dialogues that
might have come up in the installation
that might be hidden behind this window
so keep an eye out in your taskbar down
there for anything that you might need
to dismiss now we're not actually going
to start it yet so I'm gonna uncheck
that and click finish
we now have VirtualBox in place which
will let us run a virtual Linux box on
our PC and the next thing we need is an
image for Hadoop so let's go and grab it
from Hortonworks shall we so what we're
going to do is go to Hortonworks comm
slash sandbox just like that or just
search for a Hortonworks sandbox there
are multiple vendors of Hadoop
technology I just find this to be the
easiest one to get up and running with
Cloudera is also very popular I should
point out horton works in cloud era are
in competition with each other but this
stuff is all free so it's fine by us
let's go ahead and download the sandbox
so you'll see a bunch of stuff here you
can choose from what you want to look
for is the Hortonworks sandbox on a VM
for a VirtualBox ok now let me give you
a little bit of a tip here that's gonna
save you a lot of trouble potentially
although this course works just fine on
HDB version 2.6 point three and probably
newer versions as well it's kinda take a
very long time for that version of
Hortonworks to actually boot up on your
system so if you want to make life a
little bit easier I would go down to the
archive here just scroll down to where
it says Hortonworks sandbox archive and
expand that and if you can find version
2.5 that's going to boot up a lot faster
and it will work just as well with this
course find HTTP 2.5 on Hortonworks
sandbox 4 VirtualBox click that button
and just wait for that to come down it's
11 gigabytes so we'll take a little bit
of time to come down go ahead and hit
pause while that gets downloaded on your
system ok time is passed and we've
downloaded 11 gigabytes of Hortonworks
Vox goodness all we have to do now was
open up that OVA file that we downloaded
hope you had enough room on your
harddrive for that 11 gigs isn't that
big these days and we're just gonna go
ahead and click the import button here
to import that image into VirtualBox so
we can fire it up whenever we want to
and this will take a little bit of time
to copy over after all it's 11 gigabytes
so again we'll hit pause here and wait
until that's done alright we've imported
our Hortonworks docker sandbox what is
this thing that we just download it
anyway well basically it's a
pre-installed Hadoop environment that
also has a bunch of associated
technologies installed as well a lot of
the stuff that we're going to go through
in this course is going to be already
set up for you by virtue of installing
the Hortonworks sandbox so all we have
to do now is click on it and hit start
and that will basically be the same
thing as pushing the power button on a
PC that has sent OS installed so we're
just gonna sit here and wait for that to
boot up it will just do its thing
automatically it does take a little bit
of time to get up and running so let's
let that go ahead and do its thing
alright we are in business check it out
so we have a little scent OS instance
running here that actually has to do up
and running now I'm using something
called putty on Windows and on Mac OS
you might just use the built-in terminal
instead but if you're on Windows you'll
want to look up putty and if you go to
putty or you'll find where to download
that from and you can just download the
actual executable there and and run it
so I've actually attached that to my
let's close this so once you install
putty it will look something like this
now to connect to this virtual machine
that I'm running from my local machine
what I'm going to do is open an SSH
session to it and what I'm going to do
is for the hostname I'm going to type in
Moorea underscore dev at 127 0.01 and
this particular sandbox is exposing
telnet on port 2 2 to 2 as opposed to 22
so make sure you enter that as well now
I'm going to save this as HDB so I can
get back to it easier next time okay
so let's hit open and I'm just gonna
type in for the password Maria
underscore dev and I'm in cool so now
that we're logged in as Maria dev we
have to set up a couple of other
accounts so that we can log in with
elevated privileges when we need them in
different situations let's start by
setting up our root account so let's
type in su that stands for super user
root and the initial password on the
root account is Hadoop H ad oo P all
lowercase alright now the first time you
log in as root it will require you to
change the password so start by entering
in the current password which is again
Hadoop H a do opie and now type in
whatever your favorite password is one
that you'll remember type it again to
make sure you didn't fat-finger it and
now we were signed in as root as
indicated by the prompt their route at
sandbox Maria underscore dev now you
don't want to stay logged in as root
generally speaking because that gives
you a lot of dangerous permission so
that hash signed there by the way on the
profits Ted the dollar sign is another
cue that you're currently logged in as
root so let's type in exit to get out of
that now we also want to set up an admin
user and this is going to be used from
the web UI for Hortonworks
and to do that we need to be well logged
in as root so let's log in as root again
and make sure that that still works su
root type in your new password and
hopefully you are successfully logged in
as root now we need to type in the
following command Ambari - admin -
password - reset and you'll be prompted
to type in your favorite password for
the admin account as well whatever you
want it to be typed it twice and we now
have an admin user being set up that we
can use later on in the course and that
will allow us to use web UIs for
starting services and things like that
give that a moment or two to start up
and you can see it automatically
restarted the Ambari server will talk
about Ambari later but the other thing
you need to do is type in Ambari - agent
restart just to be safe
all right so now we've got admin and
routes set up we can go ahead and exit
out of the root account to be safe and
we can move on all right so at this
point you have installed a virtual Linux
machine on your desktop and we've
installed the Hortonworks sandbox
environment onto it which is a hadoop
distribution and again this is relevant
to spark because we're running spark on
top of Hadoop in this little video here
so the idea is that we're using Hadoop
s-- built-in resource manager called
Yarn to actually schedule the resources
in spark and we're also using Hadoop
file system called HDFS or the Hadoop
distributed file system to actually
store the data that spark is going to
manipulate so what we need to do next is
actually get some data for spark to play
with in this video series so to do that
we're going to get some data from the
movie lens data set movie lens is a
collection of real user movie ratings
out there so they have a website out
there where people submit 1 to 5 star
ratings of various movies that they've
watched and they have a small subset of
that called the ml 100k data set that
we're going to use within our little
examples here so you can go to group
lens org if you want to learn more about
where that comes from
but up next we need to install that data
set of movie ratings from movie lens
into HDFS so that our spark application
can actually retrieve it and get
interesting insights out of it so let's
take care of that next all right so
let's actually copy some data into
Hadoop just by looking at it here first
of all let's mess around with it so the
way you actually manipulate HDFS from
the command line on a linux host here is
by using the hadoop command and then FS
and then - whatever command you want to
do so for example if I want to do an LS
to list what's in the filesystem I would
do something along those lines and we
can see here the contents of my home
directory and HDFS for Maria underscore
dev so again Hadoop FS - LS so let's go
ahead and create a directory in here to
hold my movie lens ratings data so we'll
type in Hadoop FS - make der just like
that and we're going to make a directory
called what we call before ml - 100 K
right
now some of you might not be totally
comfortable with messing around on a
command prompt like this and if not
that's okay you can just watch you don't
have to follow along but you know I
think most of the students here are
going to be of such a background where
this is valuable so let's go ahead and
make sure that that's actually in their
Hadoop FS - LS again and we should see
our ml 100k directory and sure enough we
do very cool all right so let's actually
upload some information into this we're
basically doing exactly what we did
through the Ambari UI just from the
command line but in this case we're
uploading data into HDFS from the local
file system but since we're not going
through a web browser on our Windows
machine our local file system actually
means this linux host itself so you can
see if I do an LS by not using Hadoop
I'm looking at what's in the actual
Maria dev home directory here and I have
nothing on my local filesystem yet to
play with so the first thing I have to
do is actually get some data on here
that I can play around with now I've
actually uploaded a copy of the movie
lens dataset to my own server so let's
go ahead and grab a copy of that you can
just type in W get which is a linux
command that retrieves information from
the web HTTP colon slash slash media dot
a sundog - soft calm just like that make
sure you got all the spelling right
that's just a server that I own slash
Hadoop / ml - 100k / u dot data and that
should go retrieve my copy of U dot data
from my web server to here now if I type
in LS I'll see that sure enough LS - la
to see more details you dot data is
there and it is does appear to be in one
piece so let's upload this into HDFS to
do that we're gonna do Hadoop FS just
like every HDFS command
- copy from local and we're going to
start with what we're copying which is U
dot data and it's just going to do that
from my current directory and we're
gonna put that into from our home
directory in HDFS so this is going to be
a relative path ml - 100k slash u dot
data
so that has copied from my local file
system on this host into HDFS the unit
data file so let's go ahead and make
sure it's actually in there we can do
Hadoop FS - LS ml - 100k - let's see
what's inside the ml 100k folder and
sure enough there it is it got there in
one piece so we're cool so we finally
have everything installed so we can
start messing around with spark but
first let's talk about what sparks all
about and we'll dive into that in the
next video in this playlist</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>