<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Apache Spark Tutorial for Beginners Part 2 - Intoroduction to Spark - Frank Kane | Coder Coacher - Coaching Coders</title><meta content="Apache Spark Tutorial for Beginners Part 2 - Intoroduction to Spark - Frank Kane - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udemy-Tech/">Udemy Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Apache Spark Tutorial for Beginners Part 2 - Intoroduction to Spark - Frank Kane</b></h2><h5 class="post__date">2018-03-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/M_cjCGwqLnc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in our next section we're going to talk
about apache spark something near and
dear to my heart it's something that I
personally have a lot of experience with
and I'm very excited to share with you
this is a very very hot very quickly
emerging technology in the Hadoop
ecosystem for managing the processing of
massive amounts of data in a very
extensible way and it's a lot of really
cool features built on top of spark like
things for machine learning and graph
analysis and streaming data and all
sorts of cool stuff so let's dive in and
find out what spark is all about let's
talk about spark Apache spark to be
particular very exciting technology I
can't wait to tell you about it it's
something that I use a lot personally
and it's power and speed is just
something that I can never get enough of
what is spark well if you go to the
official spark website at spark Apache
org it will just say it's a fast in
general engine for large-scale data
processing that's not terribly specific
now is it I mean I could say that about
the entire Hadoop ecosystem but spark
kind of takes it up to a new level it
does give you a lot of flexibility where
you can actually write real scripts
using real programming languages like
Java or Scala or Python to do complex
manipulations and transformations and
analysis of your data and what sets it
apart from other technologies like pig
for example is that it has a rich
ecosystem on top of spark that lets you
do all sorts of complicated things like
machine learning and data mining and
graph analysis and streaming data that
we'll talk about more so it's a very
powerful framework and a very fast one -
and its scalable just like you would
have in any Hadoop based technology
spark follows the same pattern where
basically you have a driver program a
little script that controls what's going
to happen in your job and that goes
through some sort of a cluster manager
and you can use yarn that's but that's
only one option like you spark can run
on Hadoop but it doesn't have to run on
Hadoop it can actually use its own
cluster manager as well that's built in
and it can also use another cluster
manager called meso so we'll talk about
later in the course but whatever cluster
manager you choose that will distribute
your job across an entire cluster of
commodity computers which is kind of the
whole point processing all this data in
parallel where
each executor process has some sort of
cash and some sort of set of tasks that
it's responsible for now that cash is
the key to the performance here a part
of the key to performance because unlike
disk based solutions where it's always
just hitting HDFS all the time
SPARC is a memory based solution so it
tries to retain as much as it can in RAM
as it goes and that's one of the keys to
its speed another key to expedite cyclic
graphs which we'll talk about
momentarily so it is really fast if you
compare it to MapReduce it can be up to
100 times faster when it's operating
within memory or even 10 times faster
when it is limited to disk access but
either way it's gonna be faster than
MapReduce now hundred times might be a
little bit hyperbolic but in my
experiment it is indeed much faster than
MapReduce so you know given the choice
between the two you know you would
probably want to take spark over
MapReduce and again you know MapReduce
is very limited in what it can do you
know you have to think about things in
terms of mappers and reducers whereas
SPARC provides a framework much like Pig
does for removing that that level of
thought from you you can just think more
about your end results and program
toward that and think less about how to
actually distribute it across the
cluster SPARC does that for you and
remember when we used tez with pig that
was a directed acyclic graph and SPARC
has the same thing built in so it too
can optimize workflows to actually work
backwards from the end result that you
want and figure out the fastest way to
get there and that too is key to its
speed and performance which is like I
said pretty darn impressive it's also a
very hot technology I mean there's this
whole world around SPARC right now they
have their own conferences several
conferences per year in fact that are
very popular and it's in very active
development right now it's hard to keep
up on all the things that are changing
in SPARC companies like Amazon eBay NASA
Yahoo and tons of others are using SPARC
today for real-world problems on real
massive data sets so it's not only a
popular skill to have it's a very
valuable skill to have if you know SPARC
that's a dang good thing to have in your
resume and the beauty of it is that it's
not really that hard you know when you
start looking at some of the code that
we're going to look at later in this
section it's not a whole lot to it you
know a few lines of code can
you kick off some very complex analysis
on a cluster and you have your choice of
using Python Java or Scala for
programming it we'll get into the
reasons why you might choose one or the
other in a moment but at the end of the
day it's all built around one main
concept it's something called the
resilient distributed data set it's
basically just a an object that
represents a data set and there are
various functions you can call on that
RTD object to transform it or reduce it
or analyze it in whatever way you want
to produce new rdd's so usually you're
just writing a script that takes an RDD
of your input data and transforms that
in whatever way you want as you go now
in SPARC 2.0 which came out in 2016
they've built on top of our DDS to
produce something called a dataset
that's a little bit more of a sequel
focused take on an RTD but you know at
the end of the day it's still built
around the RTD like I said SPARC has a
lot of depth to it so while you could
just program at the RTD level with
inspark or there are also libraries
built on top of SPARC that are part of
SPARC itself it just comes along as part
of the package so SPARC streaming is one
of them and we'll have a whole
discussion of that later in the course
where instead of just doing batch
processing of data you can actually
input data in real time so imagine you
have a fleet of web servers that are
producing logs that log data can be
ingested as it's being produced in to
SPARC and then actually analyzed across
some window of time and you can output
the results of that analysis to a
database or some no sequel data store
whatever you need to do all with a few
lines of code and spark streaming very
exciting stuff sparks sequel very very
hot area right now it's basically a
sequel interface to SPARC so again if
you're familiar with sequel you can just
write sequel queries across against your
data using spark sequel or you sequel
like functions to transform your your
datasets and spark this is really the
direction that spark is going in right
now a lot of the optimization work is
really focused on the spark sequel
interface namely data sets on spark and
that allows you to do even more
optimizations beyond the directed
acyclic graph because they can do sequel
optimizations on the queries that you're
actually running and it's even faster so
that's kind of the way of the future
right now the way the present really if
you're using
our 2.0 ml lip very exciting stuff an
entire library of machine learning and
data mining tools that you can run on a
data set that's in Sparks so you know it
can be very very challenging to try to
think about how to break a machine
learning problem like clustering or you
know regression analysis into mappers
and reducers you don't have to though
because sparks ml lib we'll figure that
out for you and you can just create very
high level classes for extracting
meaning from the data that you have
graph X that's the graph in the terms of
graph theory so imagine for example you
have a social network graph you know a
graph of friends of friends and whatnot
and you want to analyze the properties
of that graph and see who's connected to
who and what way and what are the
shortest paths and things like that
graph X provides a very extensible way
of doing that in one of my other courses
we actually go through an example of
using that to figure out the degrees of
separation between superheroes and comic
books so you can do some pretty fun
stuff with graph X as well it's very
easy to use as well so very rich
ecosystem surrounding spark that lets
you do a wide variety of tasks on Big
Data across a cluster and that's why I
find sparks so exciting now for our
examples we're going to be using the
Python language in this course and well
because this is just an overview and we
don't really have time to teach you a
new programming language as part of this
course we're gonna stick with Python
because we're using that as some other
sections here and you're there's a good
chance you've already encountered Python
code before it's a lot simpler to use
and the real advantage to using Python
in this course is that you don't have to
bother with compiling stuff and building
jar files and worrying about
dependencies and all that nonsense
Python is just a lot easier to get up
and running with but if you do want to
end up using spark in production in the
real world
Python it's okay to start with and
prototype stuff but you're probably
gonna want to move to Scala as a
programming language ultimately Scala is
what spark itself is written in it's a
good functional programming model that
Maps very well to how SPARC works and it
also gives you much faster performance
and more reliable performance because
you don't have all the overhead of
Python going on there Scala compiles
compiles right down to Java bytecode and
like we've talked about before all of
Hadoop ultimately runs in Java so that
lets you actually run in the most
efficient way possible
Python is in fact a lot slower and
Harrison if you were to run the same
jobs in spark on Python versus Scala
skala will almost always be noticeably
faster and use less resources which
means it's going to run more reliably on
a cluster and require smaller clusters
and you might need for the equivalent
Python code but again Python very easy
to use so we're gonna stick with that
for now and the good news is that if you
do move from Python to Scala later on it
looks very similar when you're dealing
with SPARC scripts so for example here's
a very simple real piece of Python SPARC
code that just creates an RDD a
resilient distributed data set of the
numbers 1 2 3 &amp;amp; 4 and then it squares
them using this little function that
takes in each input value and multiplies
it by itself and returns two results
into a new RTD actually it's actually
collecting it to a Python object called
squared and if you compare that to the
scallion of that code you'll see it
looks very familiar so you know the
syntax is a little bit different you
know instead of using brackets you're
gonna construct a list here instead of
using lambda you just have this
different notation here with a little
funny arrow but at the end of the day
it's not very hard to move from Python
to Scalla you'll pick it up quite easily
even those Scalla might be an entirely
new language to you so with that let's
talk more about SPARC in depth and how
it works and the resilient distributed
data sets</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>