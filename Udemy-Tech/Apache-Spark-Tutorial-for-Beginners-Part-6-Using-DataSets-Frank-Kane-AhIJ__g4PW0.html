<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Apache Spark Tutorial for Beginners Part 6 - Using DataSets - Frank Kane | Coder Coacher - Coaching Coders</title><meta content="Apache Spark Tutorial for Beginners Part 6 - Using DataSets - Frank Kane - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udemy-Tech/">Udemy Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Apache Spark Tutorial for Beginners Part 6 - Using DataSets - Frank Kane</b></h2><h5 class="post__date">2018-03-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/AhIJ__g4PW0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let's solve that same problem of finding
the worst rated movies this time using
spark 2.0 style dataframes so let's dive
in and dissect this script that we're
going to run on our cluster as well or
cluster of one virtual machine at least
so unlike last time we're not gonna
import a spark context what we're gonna
do instead is import something called a
Sparks session and this is a new thing
in spark 2.0 that encompasses both a
spark context and a sequel context so
the new paradigm if you will is that in
spark 2 you create a sparks session
object and leave that session running
through your script and you can execute
sequel queries on it and whatever you
want and when you're done you stop it
and it's over so we're also gonna import
the row object here that's gonna let us
create data frames of row objects and
some sequel functions so we can do
things like compute the average of the
ratings as we go move it on load movie
names same function as before
it's basically returning that Python
dictionary that lets us map movie IDs to
movie names when we produce our final
output and let's again skip ahead to the
main body of our script here so unlike
before where we built a spark conf in a
spark context we're going to build a
sparks session this time because that's
the spark 2.0 way of doing things
so we're going to call spark session
builder dot app name to give it a name
that's gonna show up in our console dot
get or create and they look returned a
spark session object that we're just
gonna call spark now get or create is
worth talking about one neat thing is
that these spark scripts can actually
recover from themselves so if you were
to actually have some sort of a failure
get or create means I will either create
a new spark session or if I didn't
successfully stop from the last time
create from a saved snapshot of that
session and pick up from where I left
off so kind of a neat little feature of
spark sessions will create our movie ID
- movie name dictionary for later and
then we'll dive into the meat of using
spark so this time we can see that our
sparks session contains a spark context
and we can still call text file and
anything else we could do on spark
context with that
and we'll do it that way for now so
we'll use the same old text file
interface to load up our au dot data
file that contains all of our movie
ratings from HDFS into a new RDD called
lines okay so same as before just a
little bit at slightly different context
here and sparks session also has some
built-in ways for reading text files
directly without going through a spark
context but I don't want to get into too
much depth now just like before we need
to convert that into something else
and this time what we're going to do is
create an RDD of row objects but that we
can later convert to a data frame so we
want all we really care about are the
movie IDs and the ratings right so we're
going to extract the movie IDs and
ratings and create an RDD of rows that
contain movie IDs and ratings and to do
that we're going to use our parse input
function which is going to work a little
bit different this time so as before
we're going to split the input fields
coming in from the u data file one line
at a time and now we're going to extract
the movie ID cast it to an integer and
extract the rating and cast it to a
floating-point number like we did before
but this time instead of just returning
a tuple we're going to return a row
object that gives these fields names so
we have a row that contains a movie ID
column or field that contains the movie
ID and a rating field that contains the
actual rating so at this point we have
an RDD of row objects where each row
contains a movie ID enter rating and now
we can call spark create data frame to
convert that RDD into a data frame and
it has all the information it needs
built in about the types and column
names to do that so now that we have a
data frame we can do cool stuff with it
so for example let's figure out the
average rating for each movie we can do
that with one line of code here whereas
it took several steps before using our
DDS we can just say movie data set group
by movie ID and that basically does a
reduction by movie ID you know collects
all the ratings together for each unique
movie and then all we have to do is say
dot AVG to average those ratings
together into a final number so what we
end up with here is a new data set
called average ratings that contains a
movie ID column
a VG rating a column that contains the
average of all the ratings for each
movie ID so you can see that's a lot
easier to use than the previous RDD
interface now just to make things a
little bit more interesting
you remember we got some kind of dodgy
results when we did this with our D DS
because we had a lot of one-star movies
that were probably only rated by one
person should those really count well I
don't know but let's get a handle on how
big of a problem that really is by also
outputting the count of how many ratings
exist for each movie so now we can see
side-by-side next to that 1.0 average
rating how many people actually rated it
one star to deserve that so to do that
similarly to how we got the average
rating we can group the original movie
data set by movie ID again and this time
call count to get the count of how many
ratings for each movie ID occurred so
now we have two data sets one that
contains movie IDs and average ratings
and another one that contains movie IDs
and counts and we can join those
together into a single data set using
join just like you would in sequel and
what we're going to do is say we're
going to take the cows data set and join
it with the average ratings data set
using the movie ID column to join them
together and then we can just pull out
the top ten results and while we're at
it sort them by the average rating
column that came out of our average
ratings data set and after calling take
what we get back is a Python list that
we can dip then in Terraria through
again print out the movie names based on
the movie IDs and the average ratings
and and count of a number of ratings as
well stop the session and we're done so
you can see using data sets can be a
little bit simpler a little bit easier
to follow from a coding standpoint you
have to think a little bit less and it
also gives you a lot more flexibility
because you know in addition to
explicitly calling these functions like
group by and order by I could also just
issue sequel commands if I wanted to
which is like super easy and since this
is more structured information SPARC can
actually deal with it more efficiently
so you get all sorts of benefits by
going in this direction and these data
frames that we end up with are
interchangeable with other components of
SPARC like
m/l lib and sparks streaming which is
also really cool so with that let's
actually run this bad boy shall we so to
do that well again log into our cluster
I already have mine running as you can
see here so I'm gonna log back in as
Maria dev and the password again is
still Maria underscore dev now I already
downloaded everything we need to do this
from the previous activity so again you
should have the lowest rated movie data
frame dot py script that's the one we're
gonna run right now and if I do a less
lowest rated movie data frame dot py you
can see that is the same script we were
just looking at and it too will depend
on the ml - 100k / u dot item file to
get that dictionary so that should be in
there and we are also assuming that you
already have the movie lines data set
installed on HDFS on this little pseudo
cluster under user Maria underscore dev
/ ml - 100 K so that's all the stuff
this script expects and if you ran the
previous activity successfully that
should already be in place now as of
this recording Hortonworks actually
includes both spark 1 and spark 2 on its
distribution so you have to tell it
explicitly if you want to use spark 2 so
to do that we're all set a little
environment variable just like this
export all caps spark underscore major
underscore version equals 2 and that
will tell Hortonworks that we want to
use spark 2 instead of spark 1 and that
will allow us to use things like the
sparks session object and now that we
have that in place we can just say like
before spark - submit lowest rated movie
if only I could type rated movie and
data frame py and off it goes
we can ignore these errors and since I'm
doing a joint it did a little bit more
work but hey there's our results so top
10 results here not only can we see the
average ratings of one star but now we
can also see the total counts so we can
see that yeah a further gesture only had
a 1 star average rating but only one
person was behind that so a single
person rating at one star not sure how
much confidence I really placed in that
however a meet evil doll house had three
people who rated it and all three of
them gave it one star probably means
it's probably not the best movie
same thing with Amityville a new
generation well I've heard of The
Amityville Horror but I never heard of a
match to build a new generation and I
think there's a very good reason for
that because of the five people who
actually watched it in this data set all
five gave it a one-star rating which is
pretty terrible now these results don't
exactly match up with what we got before
using rdd's because we have so many tied
results the ordering was ambiguous right
so that's okay but when we get to our
exercise later on we'll make more sense
of that but there you have it same
exercise using data sets and spark - so
exciting stuff again this is the way of
the future and I hope it's a little bit
demystified for you now up next let's do
something even fancier</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>