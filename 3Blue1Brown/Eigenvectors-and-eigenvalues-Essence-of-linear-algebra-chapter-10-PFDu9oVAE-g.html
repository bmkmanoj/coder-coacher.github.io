<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Eigenvectors and eigenvalues | Essence of linear algebra, chapter 10 | Coder Coacher - Coaching Coders</title><meta content="Eigenvectors and eigenvalues | Essence of linear algebra, chapter 10 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/3Blue1Brown/">3Blue1Brown</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Eigenvectors and eigenvalues | Essence of linear algebra, chapter 10</b></h2><h5 class="post__date">2016-09-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PFDu9oVAE-g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">you
i convector z' and eigenvalues is one of
those topics that a lot of students find
particularly unintuitive questions like
why are we doing this and what does this
actually mean are too often left just
floating away in an unanswered sea of
computations and as I've put out the
videos of the series a lot of you have
commented about looking forward to
visualizing this topic in particular I
suspect that the reason for this is not
so much that I ghen things are
particularly complicated or poorly
explained in fact it's comparatively
straightforward and I think most books
do a fine job explaining it the issue is
that it only really makes sense if you
have a solid visual understanding for
many of the topics that precede it most
important here is that you know how to
think about matrices as linear
transformations but you also need to be
comfortable with things like
determinants linear systems of equations
and change of basis confusion about
eigen stuffs usually has more to do with
the shaky foundation in one of these
topics than it does with eigen vectors
and eigen values themselves to start
consider some linear transformation in
two dimensions like the one shown here
it moves the basis vector I hat
to the coordinates 3 0 and J hat to 1/2
so it's represented with a matrix whose
columns are 3 0 and 1 to focus in on
what it does to one particular vector
and think about the span of that vector
the line passing through its origin and
its tip most vectors are going to get
knocked off their span during the
transformation I mean it would seem
pretty coincidental if the place where
the vector landed also happened to be
somewhere on that line
but some special vectors do remain on
their own span meaning the effect that
the matrix has on such a vector is just
to stretch it or squish it like a scaler
for this specific example the basis
vector I hat is one such special vector
the span of I hat is the x-axis and from
the first column of the matrix we can
see that I hat moves over to 3 times
itself still on that x-axis what's more
because of the way linear
transformations work any other vector on
the x-axis is also just stretched by a
factor of 3 and hence remains on its own
span
a slightly sneakier vector that remains
on its own span during this
transformation is negative 1 1 it ends
up getting stretched by a factor of 2
and again linearity is going to imply
that any other vector on the diagonal
line spanned by this guy is just going
to get stretched out by a factor of 2
and for this transformation those are
all the vectors with this special
property of staying on their span those
on the x-axis getting stretched out by a
factor of three and those on this
diagonal line getting stretched by a
factor of two and the other vector is
going to get rotated somewhat during the
transformation knocked off the line that
it spans
as you might have guessed by now these
special vectors are called the
eigenvectors of the transformation and
each eigenvector has associated with it
what's called an eigenvalue which is
just the factor by which it's stretched
or squished during the transformation
of course there's nothing special about
stretching versus squishing or the fact
that these eigenvalues happen to be
positive in another example you could
have an eigenvector with eigenvalue
negative 1/2 meaning that the vector
gets flipped and squished by a factor of
1/2
but the important part here is that it
stays on the line that it spans out
without getting rotated off of it
for a glimpse of why this might be a
useful thing to think about consider
some three dimensional rotation
if you can find an eigenvector for that
rotation a vector that remains on its
own span what you have found is the axis
of rotation
and it's much easier to think about a 3d
rotation in terms of some axis of
rotation and an angle by which it's
rotating rather than thinking about the
full 3x3 matrix associated with that
transformation
in this case by the way the
corresponding eigenvalue would have to
be 1 since rotations never stretch or
squish anything so the length of the
vector would remain the same
this pattern shows up a lot in linear
algebra with any linear transformation
described by a matrix you could
understand what it's doing by reading
off the columns of this matrix as the
landing spots for basis vectors but
often a better way to get at the heart
of what the linear transformation
actually does less dependent on your
particular coordinate system is to find
the eigenvectors and eigenvalues
I won't cover the full details on
methods for computing eigenvectors and
eigenvalues here but I'll try to give an
overview of the computational ideas that
are most important for a conceptual
understanding symbolically here's what
the idea of an eigenvector looks like a
is the matrix representing some
transformation with V as the eigenvector
and lambda is a number namely the
corresponding eigenvalue what this
expression is saying is that the matrix
vector product a times V gives the same
result as just scaling the eigenvector v
by some value lambda so finding the
eigenvectors and their eigenvalues of a
matrix a comes down to finding the
values of V and lambda that make this
expression true
it's a little awkward to work with it
first because that left-hand side
represents matrix vector multiplication
but the right-hand side here is scalar
vector multiplication so let's start by
rewriting that right-hand side as some
kind of matrix vector multiplication
using a matrix which has the effect of
scaling any vector by a factor of lambda
the columns of such a matrix will
represent what happens to each basis
vector and each basis vector is simply
multiplied by lambda so this matrix will
have the number lambda down the diagonal
with zeros everywhere else
the common way to write this guy is to
factor that lambda out and write it as
lambda times I where I is the identity
matrix with ones down the diagonal with
both sides looking like matrix vector
multiplications we can subtract off that
right hand side and factor out the V
so what we now have is a new matrix a
minus lambda times the identity and
we're looking for a vector V such that
this new matrix times V gives the zero
vector
now this will always be true if V itself
is the zero vector but that's boring
what we want is a nonzero eigen vector
and if you watch chapter 5 &amp;amp; 6 you'll
know that the only way it's possible for
the product of a matrix with a nonzero
vector to become 0 is if the
transformation associated with that
matrix squishes space into a lower
dimension
and that squish off' occation
corresponds to a zero determinant for
the matrix to be concrete let's say your
matrix a has columns two one and two
three and think about subtracting off a
variable amount lambda from each
diagonal entry
now imagine tweaking lambda turning a
knob to change its value as that value
of lambda changes the matrix itself
changes and so the determinant of the
matrix changes the goal here is to find
a value of lambda that will make this
determinant zero meaning the tweaked
transformation squishes space into a
lower dimension in this case the sweet
spot comes when lambda equals one of
course if we have chosen some other
matrix the eigen value might not
necessarily be one the sweet spot might
be hit at some other value of lambda
so this is kind of a lot but let's
unravel what this is saying when lambda
equals 1 the matrix a minus lambda times
the identity squishes space onto a line
that means there's a nonzero vector V
such that a minus lambda times the
identity times V equals the zero vector
and remember the reason we care about
that is because it means a times V
equals lambda times V
saying that the vector v is an
eigenvector of a staying on its own span
during the transformation a in this
example the corresponding eigenvalue is
1 so V would actually just stay fixed in
place
pause and ponder if you need to make
sure that that line of reasoning feels
good
this is the kind of thing I mentioned in
the introduction if you didn't have a
solid grasp of determinants and why they
relate to linear systems of equations
having nonzero solutions an expression
like this would feel completely out of
the blue
to see this in action let's revisit the
example from the start with a matrix
whose columns are three zero and one two
to find if a value lambda is an
eigenvalue subtract it from the
diagonals of this matrix and compute the
determinant
doing this we get a certain quadratic
polynomial in lambda 3 minus lambda
times 2 minus lambda since lambda can
only be an eigenvalue if this
determinant happens to be 0 you can
conclude that the only possible
eigenvalues are lambda equals 2 and
lambda equals 3 to figure out what the
eigenvectors are that actually have one
of these eigenvalues say lambda equals 2
plug in that value of lambda to the
matrix and then solve for which vectors
this diagonally altered matrix sends to
0 if you computed this the way you would
any other linear system you'd see that
the solutions are all the vectors on the
diagonal line spanned by negative 1 1
this corresponds to the fact that the
unaltered matrix 3 0 1 2 has the effect
of stretching all those vectors by a
factor of 2
now a 2d transformation doesn't have to
have icon vectors for example consider a
rotation by 90 degrees this doesn't have
any eigenvectors since it rotates every
vector off of its own span
if you actually try computing the
eigenvalues of a rotation like this
notice what happens its matrix has
columns 0 1 and negative 1 0 subtract
off lambda from the diagonal elements
and look for when the determinant is 0
in this case you get the polynomial
lambda squared plus 1
the only roots of that polynomial are
the imaginary numbers I and negative I
the fact that there are no real number
solutions indicates that there are no
eigenvectors
another pretty interesting example worth
holding in the back of your mind is a
shear this fixes I hat in place and
moves J hat one over so it's matrix has
columns 1 0 &amp;amp; 1 1 all of the vectors on
the x axis are eigenvectors with
eigenvalue 1 since they remain fixed in
place in fact these are the only vector
x' when you subtract off lambda from the
diagonals and compute the determinant
what you get is 1 minus lambda squared
and the only root of this expression is
lambda equals one this lines up with
what we see geometrically that all of
the eigenvectors have I gained value one
keep in mind though it's also possible
to have just one eigenvalue but with
more than just a line full of
eigenvectors
a simple example is a matrix that scales
everything by two
the only eigenvalue is 2 but every
vector in the plane
gets to be an eigenvector with that
eigenvalue now is another good time to
pause and ponder some of this before I
move on to the last topic
you
I want to finish off here with the idea
of an eigenbasis which relies heavily on
ideas from the last video take a look at
what happens if our basis vectors just
so happen to be eigenvectors for example
maybe i hat is scaled by negative one
and J hat is scaled by two writing their
new coordinates as the columns of a
matrix notice that those scalar
multiples negative 1 &amp;amp; 2 which are the
eigenvalues of I hat and J hat sit on
the diagonal of our matrix and every
other entry is a 0 anytime a matrix has
zeros everywhere other than the diagonal
it's called reasonably enough a diagonal
matrix and the way to interpret this is
that all the basis vectors are
eigenvectors with the diagonal entries
of this matrix being there i ghen values
there are a lot of things that make
diagonal matrices much nicer to work
with one big one is that it's easier to
compute what will happen if you multiply
this matrix by itself a whole bunch of
times since all one of these matrices
does is scale each basis vector by some
eigenvalue applying that matrix many
times say 100 times is just going to
correspond to scaling each basis vector
by the 100th power of the corresponding
eigenvalue in contrast try computing the
hundredth power of a non diagonal matrix
really try it for a moment it's a
nightmare
of course you'll rarely be so lucky as
to have your basis vectors also be
eigenvectors but if your transformation
has a lot of eigenvectors like the one
from the start of this video enough so
that you can choose a set that spans the
full space then you could change your
coordinate system so that these
eigenvectors are your basis vectors I
talked about change-of-basis last video
but I'll go through a super quick
reminder here of how to express a
transformation currently written in our
coordinate system into a different
system take the coordinates of the
vectors that you want to use as a new
basis which in this case means our two
eigen vectors that make those
coordinates the columns of a matrix
known as the change of basis matrix when
you sandwich the original transformation
putting the change of basis matrix on
its right and the inverse of the change
of basis matrix on its left the result
will be a matrix representing that same
transformation but from the perspective
of the new basis vectors coordinate
system the whole point of doing this
with eigen vectors is that this new
matrix is guaranteed to be diagonal with
its corresponding eigen values down that
diagonal this is because it represents
working in a coordinate system where
what happens to the basis vectors is
that they get scaled during the
transformation a set of basis vectors
which are also eigen vectors is called
again reasonably enough an eigenbasis so
if for example you needed to compute the
100th power of this matrix it would be
much easier to change to an eigenbasis
compute the 100th power in that system
then convert back to our standard system
you can't do this with all
transformations Asheer for example
doesn't have enough eigenvectors to span
the full space but if you can find an
eigenbasis it makes matrix operations
really lovely for those of you willing
to work through a pretty neat puzzle to
see what this looks like in action and
how it can be used to produce some
surprising results I'll leave up a
prompt here on the screen it takes a bit
of work but I think you'll enjoy it
the next and final video of this series
is going to be on abstract vector spaces
see them
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>