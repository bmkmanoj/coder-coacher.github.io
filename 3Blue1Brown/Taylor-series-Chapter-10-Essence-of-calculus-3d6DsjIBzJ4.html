<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Taylor series | Chapter 10, Essence of calculus | Coder Coacher - Coaching Coders</title><meta content="Taylor series | Chapter 10, Essence of calculus - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/3Blue1Brown/">3Blue1Brown</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Taylor series | Chapter 10, Essence of calculus</b></h2><h5 class="post__date">2017-05-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3d6DsjIBzJ4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">when I first learned about Taylor series
I definitely didn't appreciate just how
important they are but time and time
again they come up in math and physics
and many fields of engineering because
they're one of the most powerful tools
that math has to offer for approximating
functions I think one of the first times
this clicked for me as a student was not
in a calculus class but a physics class
we were studying a certain problem that
had to do with the potential energy of a
pendulum and for that you need an
expression for how high the weight of
the pendulum is above its lowest point
and when you work that out it comes out
to be proportional to one minus the
cosine of the angle between the pendulum
and the vertical now the specifics of
the problem we were trying to solve are
beyond the point here but what I'll see
is that this cosine function made the
problem awkward and unwieldy and it made
it less clear how pendulums relate to
other oscillating phenomena but if you
approximate cosine of theta as one minus
theta squared over two of all things
everything just fell into place much
more easily now if you've never seen
anything like this before an
approximation like that might seem
completely out of left field I mean if
you graph cosine of theta along with
this function one minus theta squared
over two they do seem rather close to
each other at least for small angles
near zero but how would you even think
to make this approximation and how would
you find that particular quadratic
the study of Taylor series is largely
about taking non polynomial functions
and finding polynomials that approximate
them near some input and the motive here
is that polynomials tend to be much
easier to deal with than other functions
they are easier to compute easier to
take derivatives easier to integrate
just all-around more friendly so let's
take a look at that function cosine of X
and really take a moment to think about
how you might construct a quadratic
approximation near x equals 0
that is among all of the possible
polynomials that look like C 0 plus C 1
times X plus C 2 times x squared for
some choice of these constants C 0 C 1
and C 2 find the one that most resembles
cosine of X near x equals 0 whose graph
kind of spoons with the graph of cosine
X at that point
well first of all at the input zero the
value of cosine of X is 1 so if our
approximation is going to be any good at
all
it should also equal 1 at the input x
equals 0 plugging in 0 just results in
whatever c0 is so we can set that equal
to 1 this leaves us free to choose
constants c1 and c2 to make this
approximation as good as we can but
nothing we do with them is going to
change the fact that the polynomial
equals 1 at x equals 0 now it would also
be good if our approximation had the
same tangent slope as cosine X at this
point of interest otherwise the
approximation drifts away from the
cosine graph much faster than it needs
to the derivative of cosine is negative
sine and at x equals 0 that equals 0
meaning the tangent line is perfectly
flat
on the other hand when you work out the
derivative of our quadratic you get C 1
plus 2 times C 2 times X at x equals 0
this just equals whatever we choose for
C 1 so this constant C 1 has complete
control over the derivative of our
approximation around x equals 0 setting
it equal to 0
ensures that our approximation also has
a flat tangent line at this point and
this leaves us free to change C 2 but
the value and the slope of our
polynomial at x equals 0 are locked in
place to match that of cosine
the final thing to take advantage of is
the fact that the cosine graph curves
downward above x equals zero it has a
negative second derivative or in other
words even though the rate of change is
zero at that point the rate of change
itself is decreasing around that point
specifically since its derivative is
negative sine of X its second derivative
is negative cosine of X and at x equals
zero that equals negative one now in the
same way that we wanted the derivative
of our approximation to match that of
the cosine so that their values wouldn't
drift apart needlessly quickly making
sure that their second derivatives match
will ensure that they curve at the same
rate that the slope of our polynomial
doesn't drift away from the slope of
cosine X any more quickly than it needs
to pulling up the same derivative we had
before and then taking its derivative we
see that the second derivative of this
polynomial is exactly two times c2 so to
make sure that this second derivative
also equals negative one at x equals 0
to x c2
has to be negative one meaning c2 itself
to be negative 1/2 and this gives us the
approximation one plus zero X minus one
half x squared and to get a feel for how
good it is if you estimate say cosine of
0.1 using this polynomial you'd estimate
it to be zero point nine nine five and
this is the true value of cosine of zero
point one it's a really good
approximation
take a moment to reflect on what just
happened you had three degrees of
freedom with this quadratic
approximation the constant C 0 C 1 and C
2 C 0 was responsible for making sure
that the output of the approximation
matches that of cosine X at x equals 0 C
1 was in charge of making sure that the
derivatives match at that point
nc2 was responsible for making sure that
the second derivatives match up this
ensures that the way your approximation
changes as you move away from x equals
zero and the way that the rate of change
itself changes is as similar as possible
to the behavior of cosine x given the
amount of control that you have you
could give yourself more control by
allowing more terms in your polynomial
and matching higher-order derivatives
for example let's say you added on the
term c3 times X cubed for some constant
c3 well in that case if you take the
third derivative of a cubic polynomial
anything that's quadratic or smaller
goes to zero and as for that last term
after three iterations of the power rule
looks like 1 times 2 times 3 times
whatever C 3 is
on the other hand the third derivative
of cosine X comes out to sine of X which
equals 0 at x equals 0 so to make sure
that the third derivatives match the
constants III should be 0 or in other
words not only is 1 minus 1/2 x squared
the best possible quadratic
approximation of cosine it's also the
best possible cubic approximation
you can actually make an improvement by
adding on a fourth order term see four
times X to the fourth the fourth
derivative of cosine is actually itself
which equals one at x equals zero and
what's the fourth derivative of our
polynomial with this new term well when
you keep applying the power rule over
and over with those exponents all
hopping down in front you end up with 1
times 2 times 3 times 4 times C 4 which
is 24 times C 4 so if we want this to
match the fourth derivative of cosine X
which is 1 C 4 has to be 1 over 24 and
indeed the polynomial 1 minus 1/2 x
squared plus one twenty-fourth times X
to the fourth which looks like this is a
very close approximation for cosine X
around x equals 0 in any physics problem
involving the cosine of a small angle
for example predictions would be almost
unnoticeably different if you
substituted this polynomial for cosine
of X
now take a step back and notice a few
things happening with this process
first of all factorial terms come up
very naturally in this process when you
take n successive derivatives of the
function X to the N letting the power
rule just keep cascading on down what
you'll be left with is 1 times 2 times 3
on and on and on up to whatever n is so
you don't simply set the coefficients of
the polynomial equal to whatever
derivative you want you have to divide
by the appropriate factorial to cancel
out this effect for example that X to
the fourth coefficient was the fourth
derivative of cosign 1 but divided by 4
factorial 24
the second thing to notice is that
adding on new terms like this see four
times X to the fourth doesn't mess up
what the old terms should be and that's
really important for example the second
derivative of this polynomial at x
equals zero is still equal to two times
the second coefficient even after you
introduce higher-order terms and it's
because we're plugging in x equals zero
so the second derivative of any higher
order term which all include an X we'll
just wash away and the same goes for any
other derivative which is why each
derivative of a polynomial at x equals
zero is controlled by one and only one
of the coefficients if instead you were
approximating near an input other than
zero like maybe x equals pi in order to
get the same effect you would have to
write your polynomial in terms of powers
of X minus pi or whatever input you're
looking at this makes it look noticeably
more complicated but all we're doing is
just making sure that the point PI looks
and behaves like zero so that plugging
in x equals PI is going to result in a
lot of nice cancellation that leaves
only one constant
and finally on a more philosophical
level notice how what we're doing here
is basically taking information about
higher-order derivatives of a function
at a single point and then translating
that into information about the value of
the function near that point
you can take as many derivatives of
cosine as you want it follows this nice
cyclic pattern cosine of X negative sine
of X negative cosine sine and then
repeat and the value of each one of
these is easy to compute at x equals
zero it gives this cyclic pattern one
zero negative one zero and then repeat
and knowing the values of all of those
higher-order derivatives is a lot of
information about cosine of X even
though it only involves plugging in a
single number x equals zero so what
we're doing is leveraging that
information to get an approximation
around this input and you do it by
creating a polynomial whose higher-order
derivatives are designed to match up
with those at cosine following the same
1 0 negative 1 0 cyclic pattern and to
do that you just make each coefficient
of the polynomial follow that same
pattern but you have to divide each one
by the appropriate factorial like I
mentioned before this is what cancels
out the cascading effect of many power
rule applications
the polynomials you get by stopping this
process at any point are called Taylor
polynomials for cosine of X more
generally and hence more abstractly if
we were dealing with some other function
other than cosine you would compute its
derivative its second derivative and so
on getting as many terms as you'd like
and you would evaluate each one of them
at x equals 0 then for the polynomial
approximation the coefficient of each X
to the N term should be the value of the
nth derivative of the function evaluated
at 0 but divided by n factorial and this
whole rather abstract formula is
something that you'll likely see in any
text or any course that touches on
Taylor polynomials and when you see it I
want you to think to yourself that that
constant term ensures that the value of
the polynomial matches with the value of
F the next term ensures that the slope
of the polynomial matches the slope of
the function at x equals 0 the next term
ensures that the rate at which the slope
changes is the same at that point and so
on depending on how many terms you want
and the more terms you choose the closer
the approximation but the trade-off is
that the polynomial you'd get would be
more complicated
and to make things even more general if
you wanted to approximate near some
input other than zero which we'll call a
you would write this polynomial in terms
of powers of X minus a and you would
evaluate all the derivatives of F at
that input a this is what Taylor
polynomials look like in their fullest
generality changing the value of a
changes where this approximation is
hugging the original function where it's
higher-order derivatives will be equal
to those of the original function
one of the simplest meaningful examples
of this is the function e to the X
around the input x equals 0 computing
the derivatives is super nice as nice as
it gets because the derivative of e to
the X is itself so the second derivative
is also e to the X as is its third and
so on so at the point x equals 0 all of
these are equal to 1 and what that means
is our polynomial approximation should
look like 1 plus 1 times X
plus one over two times x squared
plus one over three factorial times X
cubed and so on depending on how many
terms you want these are the Taylor
polynomials for e to the X
okay so with that as a foundation in the
spirit of showing you just how connected
all the topics of calculus are let me
turn to something kind of fun a
completely different way to understand
this second-order term of the Taylor
polynomials but geometrically it's
related to the fundamental theorem of
calculus which I talked about in
chapters 1 and chapters 8 if you need a
quick refresher like we did in those
videos consider a function that gives
the area under some graph between a
fixed left point and a variable right
point what we're going to do here is
think about how to approximate this area
function not the function for the graph
itself like we've been doing before
focusing on that area is what's going to
make the second order term kind of pop
out
remember the fundamental theorem of
calculus is that this graph itself
represents the derivative of the area
function and it's because a slight nudge
DX to the right bound of the area gives
a new bit of area that's approximately
equal to the height of the graph times
DX and that approximation is
increasingly accurate for smaller and
smaller choices of DX but if you want it
to be more accurate about this change in
area given some change in X that isn't
meant to approach zero you would have to
take into account this portion right
here which is approximately a triangle
let's name the starting input a and the
nudged input above it X so that that
change is X minus a the base of that
little triangle is that change X minus a
and its height is the slope of the graph
times X minus a since this graph is the
derivative of the area function its
slope is the second derivative of the
area function evaluated at the input a
so the area of this triangle one-half
base times height is one half times the
second derivative of this area function
evaluated at a multiplied by X minus a
squared and this is exactly what you
would see with a Taylor polynomial if
you knew the various derivative
information about this area function at
the point a how would you approximate
the area at the point X
well you have to include all that area
up to a F of a plus the area of this
rectangle here which is the first
derivative times X minus a plus the area
of that little triangle which is 1/2
times the second derivative times X
minus a squared I really like this
because even though it looks a bit messy
all written out each one of the terms
has a very clear meaning that you can
just point to on the diagram
if you wanted we could call it an end
here and you would have a phenomenally
useful tool for approximations with
these Taylor polynomials but if you're
thinking like a mathematician one
question you might ask is whether or not
it makes sense to never stop and just
add infinitely many terms in math an
infinite sum is called a series so even
though one of these approximations with
finitely many terms is called a Taylor
polynomial adding all infinitely many
terms gives what's called a Taylor
series you have to be really careful
with the idea of an infinite series
because it doesn't actually make sense
to add infinitely many things you can
only hit the plus button on the
calculator so many times
but if you have a series we're adding
more and more of the terms which makes
sense at each step gets you increasingly
close to some specific value what you
say is that the series converges to that
value or if you're comfortable extending
the definition of equality to include
this kind of series convergence you'd
say that the series as a whole this
infinite sum equals the value that it's
converging to
for example look at the Taylor
polynomial for e to the X and plug in
some input like x equals 1 as you add
more and more polynomial terms the total
sum gets closer and closer to the value
e so you say that this infinite series
converges to the number e or what saying
the same thing that it equals the number
e in fact it turns out that if you plug
in any other value of x like x equals 2
and look at the value of the higher and
higher order Taylor polynomials at this
value they will converge towards e to
the X which in this case is e squared
and this is true for any input no matter
how far away from zero it is even though
these Taylor polynomials are constructed
only from derivative information
gathered at the input 0
in a case like this we say that e to the
X equals its own Taylor series at all
inputs X which is kind of a magical
thing to have happen and even though
this is also true for a couple other
important functions things like sine and
cosine
sometimes these series only converge
within a certain range around the input
whose derivative information you're
using if you work out the Taylor series
for the natural log of X around the
input x equals 1 which is built by
evaluating the higher-order derivatives
of the natural log of x at x equals 1
this is what it would look like when you
plug in an input between 0 &amp;amp; 2
adding more and more terms of this
series will indeed get you closer and
closer to the natural log of that input
but outside of that range even by just a
little bit the series fails to approach
anything as you add on more and more
terms the sum just kind of bounces up
back and forth wildly it does not as you
might expect approach the natural log of
that value even though the natural log
of X is perfectly well-defined for
inputs that are above 2 in some sense
the derivative information of Ln of X at
x equals 1 doesn't propagate out that
far
in a case like this we're adding more
terms of the series doesn't approach
anything you say that the series
diverges and that maximum distance
between the input you're approximating
near and points where the outputs of
these polynomials actually do converge
is called the radius of convergence for
the Taylor series that remains more to
learn about Taylor series
there are many use cases tactics for
placing bounds on the error of these
approximations tests for understanding
when series do and don't converge and
for that matter there remains more to
learn about calculus as a whole and the
countless topics not touched by this
series the goal with these videos is to
give you the fundamental intuitions that
make you feel confident and efficient in
learning more on your own and
potentially even rediscovering more of
the topic for yourself in the case of
Taylor series the fundamental intuition
to keep in mind as you explore more of
what there is is that they translate
derivative information at a single point
to approximation information around that
point
thank you once again to everybody who
supported this series the next series
like it will be on probability and if
you want early access as those videos
are made you know where to go</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>