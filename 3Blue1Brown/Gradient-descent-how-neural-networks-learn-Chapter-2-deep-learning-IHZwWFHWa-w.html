<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Gradient descent, how neural networks learn | Chapter 2, deep learning | Coder Coacher - Coaching Coders</title><meta content="Gradient descent, how neural networks learn | Chapter 2, deep learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/3Blue1Brown/">3Blue1Brown</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Gradient descent, how neural networks learn | Chapter 2, deep learning</b></h2><h5 class="post__date">2017-10-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IHZwWFHWa-w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">last video I laid out the structure of a
neural network I'll give a quick recap
here just so that it's fresh in our
minds and then I have two main goals for
this video the first is to introduce the
idea of gradient descent which underlies
not only how neural networks learn but
how a lot of other machine learning
works as well then after that we're
going to dig in a little more to how
this particular network performs and
what those hidden layers of neurons end
up actually looking for as a reminder
our goal here is the classic example of
handwritten digit recognition the hello
world of neural networks these digits
are rendered on a 28 by 28 pixel grid
each pixel with some grayscale value
between 0 &amp;amp; 1
those are what determine the activations
of 784 neurons in the input layer of the
network and then the activation for each
neuron in the following layers is based
on a weighted sum of all the activations
in the previous layer plus some special
number called a bias then you compose
that sum with some other function like
the sigmoid squish off' occation or
array Lu the way that I walked through
last video in total given the somewhat
arbitrary choice of two hidden layers
here with 16 neurons each the network
has about 13,000 weights and biases that
we can adjust and it's these values that
determine what exactly the network you
know actually does then what we mean
when we say that this network classifies
a given digit is that the brightest of
those 10 neurons in the final layer
corresponds to that digit and remember
the motivation that we had in mind here
for the layered structure was that maybe
the second layer could pick up on the
edges and the third layer might pick up
on patterns like loops and lines and the
last one could just piece together those
patterns to recognize digits so here we
learn how the network learns what we
want is an algorithm where you can show
this network a whole bunch of training
data which comes in the form of a bunch
of different images of handwritten
digits along with labels for what
they're supposed to be and it'll adjust
those 13000 weights and biases so as to
improve its performance on the training
data hopefully this layered structure
will mean that what it learns
generalizes to images beyond that
training data and the way we test that
is that after you train the network you
show it more labeled theta that it's
never seen before and you see how
accurately it classifies those new
images fortunately for us and what makes
this such a common example to start with
is that the good people behind the emne
Stata base have put together a
collection of tens of thousands of
handwritten digit images each one
labeled with the numbers that they're
supposed to be and it's provocative as
it is to describe a machine as learning
once you actually see how it works it
feels a lot less like some crazy sci-fi
premise and a lot more like while a
calculus exercise I mean basically it
comes down to finding the minimum of a
certain function remember conceptually
we're thinking of each neuron as being
connected to all of the neurons in the
previous layer and the weights in the
weighted sum defining its activation are
kind of like the strengths of those
connections and the bias is some
indication of whether that neuron tends
to be active or inactive and to start
things off we're just gonna initialize
all of those weights and biases totally
randomly needless to say this network is
going to perform pretty horribly on a
given training example since it's just
doing something random for example you
feed in this image of a 3 and the output
layer it just looks like a mess so what
you do is you define a cost function a
way of telling the computer no bad
computer that output should have
activations which are zero for most
neurons but one for this neuron what you
gave me is utter trash to say that a
little more mathematically what you do
is add up the squares of the differences
between each of those trash output
activations and the value that you want
them to have and this is what we'll call
the cost of a single training example
notice this sum is small when the
network confidently classifies the image
correctly but it's large when the
network seems like it doesn't really
know what it's doing
so then what you do is consider the
average cost over all of the tens of
thousands of training examples at your
disposal
this average cost is our measure for how
lousy the network is and how bad the
computer should feel and that's a
complicated thing remember how the
network itself was basically a function
one that takes in 784 numbers as inputs
the pixel values and spits out ten
numbers as its output and in a sense
it's parameterised by all these weights
and biases while the cost function is a
layer of complexity on top of that it
takes as its input those thirteen
thousand or so weights and biases and it
spits out a single number describing how
bad those weights and biases are and the
way it's defined depends on the
network's behavior over all the tens of
thousands of pieces of training data
that's a lot to think about but just
telling the computer what a crappy job
it's doing isn't very helpful you want
to tell it how to change those weights
and biases so that it gets better to
make it easier rather than struggling to
imagine a function with 13,000 inputs
just imagine a simple function that has
one number as an input and one number as
an output how do you find an input that
minimizes the value of this function
calculus students will know that you can
sometimes figure out that minimum
explicitly but that's not always
feasible for really complicated
functions certainly not in the thirteen
thousand input version of this situation
for our crazy complicated neural network
cost function a more flexible tactic is
to start at any old input and figure out
which direction you should step to make
that output lower specifically if you
can figure out the slope of the function
where you are then shift to the left if
that slope is positive and shift the
input to the right if that slope is
negative
if you do this repeatedly at each point
checking the new slope and taking the
appropriate step you're gonna approach
some local minimum of the function and
the image you might have in mind here is
a ball rolling down a hill and notice
even for this really simplified single
input function there are many possible
valleys that you might land in depending
on which brand omim put you start at and
there's no guarantee that the local
minimum you land in is going to be the
smallest possible value of the cost
function that's going to carry over to
our neural network case as well
and I also want you to notice how if you
make your step sizes proportional to the
slope then when the slope is flattening
out towards the minimum your steps get
smaller and smaller and that kind of
helps you from overshooting bumping up
the complexity a bit imagine instead a
function with two inputs and one output
you might think of the input space as
the XY plane and the cost function as
being graphed as a surface above it now
instead of asking about the slope of the
function you have to ask which direction
should you step in this input space so
as to decrease the output of the
function most quickly in other words
what's the downhill direction and again
it's helpful to think of a ball rolling
down that hill those of you familiar
with multivariable calculus will know
that the gradient of a function gives
you the direction of steepest ascent
basically which direction should you
step to increase the function most
quickly naturally enough taking the
negative of that gradient gives you the
direction to step that decreases the
function most quickly and even more than
that the length of this gradient vector
is actually an indication for just how
steep that steepest slope is now if
you're unfamiliar with multivariable
calculus and you want to learn more
check out some of the work that I did
for Khan Academy on the topic honestly
though all that matters for you and me
right now is that in principle there
exists a way to compute this vector this
vector that tells you what the downhill
direction is and how steep it is you'll
be okay if that's all you know and
you're not rock solid on the details
because if you can get that the
algorithm from minimizing the function
is to compute this gradient direction
then take a small step downhill and just
repeat that over and over it's the same
basic idea for a function that has
13,000 inputs instead of two inputs
imagine organizing all 13,000 weights
and biases of our network into a giant
column vector the negative gradient of
the cost function is just a vector it's
some Direction inside this insanely huge
input space that tells you which nudges
to all of those numbers is going to
cause the most rapid decrease to the
cost function and of course with our
specially designed cost function
changing the weights and biases to
decrease it means making the output of
the network on each piece of training
data look less like a random array of
ten values and more like an actual
decision that we want it to make it's
important to remember this cost function
involves an average over all of the
training data so if you minimize it
it means it's a better performance on
all of those samples the algorithm for
computing this gradient efficiently
which is effectively the heart of how a
neural network learns is called back
propagation and it's what I'm going to
be talking about next video there I
really want to take the time to walk
through what exactly happens to each
weight and each bias for a given piece
of training data trying to give an
intuitive feel for what's happening
beyond the pile of relevant calculus and
formulas right here right now the main
thing I want you to know independent of
implementation details is that what we
mean when we talk about a network
learning is that it's just minimizing a
cost function and notice one consequence
of that is that it's important for this
cost function to have a nice smooth
output so that we can find a local
minimum by taking little steps downhill
this is why by the way artificial
neurons have continuously ranging
activations
rather than simply being active or
inactive in a binary way if the way that
biological neurons are this process of
repeatedly nudging an input of a
function by some multiple of the
negative gradient is called gradient
descent it's a way to converge towards
some local minimum of a
function basically a valley in this
graph I'm still showing the picture of a
function with two inputs of course
because nudges in a thirteen thousand
dimensional Intuit space are a little
hard to wrap your mind around but there
is actually a nice non spatial way to
think about this each component of the
negative gradient tells us two things
the sign of course tells us whether the
corresponding component of the input
vector should be noted up or down but
importantly the relative magnitudes of
all these components kind of tells you
which changes matter more easy in our
network an adjustment to one of the
weights might have a much greater impact
on the cost function than the adjustment
to some other weight some of these
connections just matter more for our
training data so a way that you can
think about this gradient vector of our
mind-warping lee massive cost function
is that it encodes the relative
importance of each weight and bias that
is which of these changes is going to
carry the most bang for your buck this
really is just another way of thinking
about direction to take a simpler
example if you have some function with
two variables as an input and you
compute that its gradient at some
particular point comes out as 3/1 then
on the one hand you can interpret that
as saying that when you're standing at
that input moving along this direction
increases the function most quickly that
when you graph the function above the
plane of input points that vector is
what's giving you the straight uphill
direction but another way to read that
is to say that changes to this first
variable have three times the importance
as changes to the second variable that
at least in the neighborhood of the
relevant input nudging the x value
carries a lot more bang for your buck
all right let's zoom out and sum up
where we are so far the network itself
is this function with 784 inputs and 10
outputs defined in terms of all of these
weighted sums the cost function is a
layer of complexity on top of that it
takes the 13,000 weights and biases as
inputs and spits out a single measure of
lousy nough spaced on the training
example
and the gradient of the cost function is
one more layer of complexity still it
tells us what nudges to all of these
weights and biases cuz the fastest
change to the value of the cost function
which you might interpret is saying
which changes to which weights matter
the most so when you initialize the
network with random weights and biases
and adjust them many times based on this
gradient descent process how well does
it actually perform on images that it's
never seen before well the one that I've
described here with the two hidden
layers of sixteen neurons each chosen
mostly for aesthetic reasons well it's
not bad it classifies about 96 percent
of the new images that it sees correctly
and honestly if you look at some of the
examples that it messes up on you kind
of feel compelled to cut it a little
slack now if you play around with the
hidden layer structure and make a couple
tweaks you can get this up to 98% and
that's pretty good it's not the best you
can certainly get better performance by
getting more sophisticated than this
plain vanilla Network but given how
daunting the initial task is I just
think there's something incredible about
any network doing this well on images
that it's never seen before given that
we never specifically told it what
patterns to look for originally the way
that I motivated this structure was by
describing a hope that we might have
that the second layer might pick up on
little edges that the third layer would
piece together those edges to recognize
loops and longer lines and that those
might be pieced together to recognize
digits so is this what our network is
actually doing well for this one at
least not at all remember how last video
we looked at how the weights of the
connections from all of the neurons in
the first layer to a given neuron in the
second layer can be visualized as a
given pixel pattern that that second
layer neuron is picking up on well when
we actually do that for the weights
associated with these transitions from
the first layer to the next instead of
picking up on isolated little edges here
and there they look well almost random
just put some very loose patterns in the
middle there
it would seem that in the unfathomably
large 13,000 dimensional space of
possible weights and biases our network
found itself a happy little local
minimum that despite successfully
classifying most images doesn't exactly
pick up on the patterns that we might
have hoped for and to really drive this
point home watch what happens when you
input a random image if the system was
smart you might expect it to either feel
uncertain maybe not really activating
any of those 10 output neurons or
activating them all evenly but instead
it confidently gives you some nonsense
answer as if it feels as sure that this
random noise is a 5 as it does that an
actual image of a 5 is a 5 phrase
differently even if this network can
recognize digits pretty well it has no
idea how to draw them a lot of this is
because it's such a tightly constrained
training setup I mean put yourself in
the network's shoes here from its point
of view the entire universe consists of
nothing but clearly defined unmoving
digits centered in a tiny grid and its
cost function just never gave it any
incentive to be anything but utterly
confident in its decisions so if this is
the image of what those second layer
neurons are really doing you might
wonder why I would introduce this
network with the motivation of picking
up on edges and patterns I mean that's
just not at all what it ends up doing
well this is not meant to be our end
goal but instead a starting point
frankly this is old technology the kind
researched in the 80s and 90s and you do
need to understand it before you can
understand more detailed modern variants
and it clearly is capable of solving
some interesting problems but the more
you dig in to what those hidden layers
are really doing the less intelligent it
seems
shifting the focus for a moment from how
networks learn to how you learn that'll
only happen if you engage actively with
the material here somehow one pretty
simple thing that I want you to do is
just pause right now and think deeply
for a moment about what changes you
might make to this system and how it
perceives images if you wanted it to
better pick up on things like edges and
patterns but better than that to
actually engage with the material I
highly recommend the book by Michael
Nielsen on deep learning and neural
networks in it you can find the code and
the data to download and play with for
this exact example and the book will
walk you through step by step what that
code is doing what's awesome is that
this book is free and publicly available
so if you do get something out of it
consider joining me in making a donation
towards Nielsen's efforts I've also
linked a couple other resources that I
like a lot in the description including
the phenomenal and beautiful blog post
by Chris Ola and the articles in distill
to close things off here for the last
few minutes I want to jump back into a
snippet of the interview that I had with
Leisha Lee you might remember her from
the last video she did her PhD work in
deep learning and in this little snippet
she talks about two recent papers that
really dig into how some of the more
modern image recognition networks are
actually learning just to set up where
we were in the conversation the first
paper took one of these particularly
deep neural networks that's really good
at image recognition and instead of
training it on a properly labeled data
set it shuffled all of the labels around
before training obviously the testing
accuracy here was going to be no better
than random since everything's just
randomly labeled but it was still able
to achieve the same training accuracy as
you would on a properly labeled dataset
basically the millions of weights for
this particular network were enough for
it to just memorize the random data
which kind of raises the question for
whether minimizing this cost function
actually corresponds to any sort of
structure in the image or is it just you
know memorize the entire data set of
what the correct classification is and
so a couple of you know half a year
later at ICML this year there was not
exactly rebuttal paper paper that
addressed some asked
like hey actually these networks are
doing something a little bit smarter
than that if you look at that accuracy
curve if you were just training on a
random data set that curve sort of went
down very you know very slowly in almost
kind of a linear fashion so you're
really struggling to find that local
minima of possible you know the right
weights that would get you that accuracy
whereas if you're actually training on a
structured data set one that has the
right labels you know you fiddle around
a little bit in the beginning but then
you kind of dropped very fast to get to
that accuracy level and so in some sense
it was easier to find that local maxima
and so it was also interesting about
that is it caught brings into light
another paper from actually a couple of
years ago which has a lot more
simplifications about the network layers
but one of the results was saying how if
you look at the optimization landscape
the local minima that these networks
tend to learn are actually of equal
quality so in some sense if your data
set is structure and you should be able
to find that much more easily my thanks
as always to those of you supporting on
patreon I've said before just what a
game-changer patreon is but these videos
really would not be possible without you
I also want to give a special thanks to
the VC firm amplifi partners in their
support of these initial videos in the
series they focus on very early stage
machine learning and AI companies and I
feel pretty confident in the
probabilities that some of you watching
this and even more likely some of the
people that you know are right now in
the early stages of getting such a
company off the ground and the amplifi
folks would love to hear from any such
founders and they even set up an email
address just for this video that you can
reach out to them through three blue one
brown at amplify partners com</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>