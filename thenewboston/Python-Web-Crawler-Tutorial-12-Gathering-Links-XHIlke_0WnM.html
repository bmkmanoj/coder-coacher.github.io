<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Python Web Crawler Tutorial - 12 - Gathering Links | Coder Coacher - Coaching Coders</title><meta content="Python Web Crawler Tutorial - 12 - Gathering Links - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/thenewboston/">thenewboston</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Python Web Crawler Tutorial - 12 - Gathering Links</b></h2><h5 class="post__date">2016-02-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XHIlke_0WnM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all righty my mateys welcome back in in
this fabulous tutorial what we're going
to do is we're going to make this gather
link and I'm actually a name that gather
links since there is more likely more
than one link on a web page and all this
function is going to do is we're going
to pass it in a URL of a page and it's
going to get the HTML and then it just
says hey you know that link Finder
engine you built earlier I'm going to
throw in on my HTML and I just want the
links back so again we did like 90% of
the work right here but of course as
always we have one little issue it's not
really a problem it's actually pretty
interesting but I'm this link finder
that we built right here it takes human
readable string HTML and by that I mean
as we already looked at in the demo if
you just have you know some tags like
this HTML HTML and you have some links
in there we can throw this in and it's
going to parse it perfectly this thing
works fine but fabulous fantastic Python
it always has to do things differently
whenever we use this module to connect
to a website and get the HTML it doesn't
give us the HTML back in a human
readable string it actually returns the
results in bites ones and zeros so
before we can just go ahead and pass the
server response to this link finder what
we need to first do is we need to
convert those bytes to characters human
readable characters and it's a little
bit goofy but I'll show you guys and
talk you guys through all of it so we're
going to go ahead and make a static
method and it was named too lazy to type
that gather links now again the only
thing that we need to pass in here is
just the URL of the page and again it's
just going to crawl that page and return
a set of links now what we need is a
variable for the HTML string so again
whenever we first connect to the server
and get the response back that's going
to be in bytes ones and zeros
thing that only the computer can
understand however this variable we're
going to store the actual string after
we convert it so that's what that's
going to be now any time I want to say
like 99% of time whenever you're doing
like in networking operations or server
operations in Python it's always a good
idea to throw it inside a try except
statement so that way if you have any
weird server errors then it doesn't
crash your entire program and you can
handle the exceptions however you want
so first I'll show you how to just
connect to this webpage so whenever you
connect to a webpage you use the
function URL open so we give a URL and
we'll just say I know maybe it's this
URL or something it's going to go ahead
and make a connection and store the
response right here so just like you're
in a browser and you get some data back
it's going to store it right in there
now unlike a browser this is actually
white data so what we need to do first
is we need to make sure that it's actual
HTML data and you actually probably
don't need to do this but it's always a
good idea because that way if you curl
like a PDF file or maybe someone has
like an executable that they link to we
really don't want to be worrying about
those so again in order to check if it's
an actual HTML file and if your code if
your website is in like node or PHP or
anything like that this is all HTML it
gets parsed before so don't worry that
Oh my PHP slide isn't going to work it
will um alright so response get header
why did that not autocomplete scaring me
now content type
text slash HTML and again this just says
okay
first make sure that we're connecting to
actual webpage and not like some
executable or weird PDF or anything like
that
once we ensure that then we're going to
want to go ahead and get the response of
that and we are going to read it in so
this right here it just reads in the raw
response as it's coming over the
Ethernet cable so again all of this
right here these HTML bytes those are
just ones and zeroes however we need to
convert it to an actual string so what
we do is for the string variable right
here set this equal to sigmah bytes dot
decode and utf-8 now there's a bunch of
different types of character encoding
utf-8 is use 99% of time this just means
um like English human readable
characters so nothing goofy it's pretty
much the standard but you need to
specify it anyways so basically take
those ones and zeros that were coming
across that got sent back to us from the
New Boston server convert them to a HTML
string and this is what we can actually
pass on to link finder so now what we
can do is create a link finder object
and whenever we create a link finder
object we need to pass it two things the
base URL which is the home page URL and
the URL that we are currently crawling
and again this is just for link
formatting and this is for gathering the
actual links so what are my variables
spider dot base URL and the page URL so
after that after we initialize that
remember I don't even know if you guys
remember from like the second video but
then you call the feed function so this
is where you actually pass in the HTML
data and it's going to go ahead and
parse it so
then we can return all of the links and
all of that's taken care of as soon as
you call that you don't have to actually
call those methods manually pretty
awesome so that's what should happen but
now we have to say okay well what about
if you know maybe you're trying to
connect to a link on a page that doesn't
exist anymore or maybe the server that
you're trying to hack their website in
the booty off well what's going to
happen well let's just go ahead and
instead of crashing our program we'll
just say
air and not crawl Paige cannot crawl a
page oh and what this function is going
to do is it's going to return a set
hopefully of links but if we get an
error
we still need a return a set or else the
calling function for this is going to
generate an error because then return
the proper thing so if we get an error
then we're just going to go ahead and
return an empty set and this is
basically saying hey that page you want
to meet a crawl there weren't any links
on it so there you go so after this
hopefully we didn't get this we're going
to have some nice juicy links in a set
so how do we get those well we just call
this function page links and it just
returns the page link so return finder
which is an object that just parsed
their site page links and there you go
so again just to recap one last time
what this function does is it connects
to the site it takes the HTML converts
it to a proper string HTML format it
passes it on to link finder link finder
parses through it gets a set of all of
the links on it all of the URLs and as
long as you didn't get any issues it
just returns it for you so now we got a
cool function where we can say hey
gather links and as soon as you call it
you get back all the URLs makes things a
whole lot easier</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>