<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Python Web Crawler Tutorial - 2 - Queue and Crawled Files | Coder Coacher - Coaching Coders</title><meta content="Python Web Crawler Tutorial - 2 - Queue and Crawled Files - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/thenewboston/">thenewboston</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Python Web Crawler Tutorial - 2 - Queue and Crawled Files</b></h2><h5 class="post__date">2016-02-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/z_vIWoTZm2E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">are already hostas welcome back and in
this little tutorial I'm going to show
you guys how to create the data files in
other words the files that are going to
go inside our projects directory now for
each website you crawl you actually only
have two files and that is the Q and the
crawled files and inside these files
there's just a list of links so it's
actually really easy to understand so
basically whenever our fight
they say fighter our spider first starts
out what it's going to do is it already
has the home page this is the only link
that we ever give it because of course
it needs a starting point so from here
what it's going to do is it's going to
go ahead and start gathering all of
these URLs Beauty business HTML computer
science the HTML again I know that's not
the actual URL but this is just an
example so it's going to go start on
this page and each link it finds this
one this one this one this watch now on
this visit Android forum one is going to
add all of those URLs to a waiting list
now whenever it's done crawling this
page and gathering up all the links it's
going to go ahead and take this URL and
add it to the crowd file why do we do
that well because then when we're on
another page and we find this link right
here which links back to the home page
we don't want to go ahead and crawl that
again we already crowded it so we need
to keep track of what pages we already
crawled and what links we found that are
on the waiting list that we didn't crawl
yet so the Q or the waiting list and the
crowd files that's all we need so go
ahead and do this and all right
and by the way another cool thing I want
to point out you see how just one thread
is all you need to curl this home page
you just need one little bot to do it
whenever we have a bunch of links so 1 2
3 4 5 6 7 8 9 10 let's say that we have
10 links in the waiting list well it'll
take let's say that it takes one second
to crawl a page that means that it would
take 10 seconds to curl all of these
pages who has time for that I'm actually
going to set up this program and make it
multi-threaded which means we're going
to make a program that
crow all of these pages simultaneously
so we're going to be able to curl like
eight or sixteen pages at once so it's
going to speed it up a bit but again I'm
getting kind of heading myself that's
essentially what we're doing so for now
I'll just say create queue and crawled
files now again just like the last time
you don't want to create these files if
they already exist because if they
already exist that means that your bot
has already been running and you know
you gather like I don't know like 20% of
the website already so you don't want to
repeat yourself you need to keep this
efficient so we'll just say uh def
create data files alright now again
there are two pieces of information we
need the project name in the project
name is usually just going to be it can
be whatever the user types in but 99%
I'm just going to be the domain name
without the top-level domain so that com
dot edu or anything so in this example
I'll just keep the project name the new
Boston and this is going to be the home
page URL so again the project in
directory name are the same thing so I
just say project name that's the first
thing we need and the second one is the
base URL and we need that because we
need to give it a starting point so the
queue I'll just say project name plus Q
text all right so I just want to go
ahead and make a variable for the file
path just so I don't have to type this
every time so basically in this project
right here we're going to have a file
called Q dot txt and all this file is
going to be is a list of links on the
waiting list waiting to be crawled so
same thing with crawled do I want to
need to copy that so crowd is just going
to be project name plus crawl dot text
so there you go now again we need to
check if these files exist before we
actually go ahead and create them in
case you're like running this for a
second time so if not OS dot path dot is
file and I'll destroy queue so we're
basically saying does this file exist
already
if not then we can just go ahead and
create it now I'm going to make a quick
little function in like just a second
and this is going to take like two
minutes to write all it does is it
writes a new or creates a new text file
so I'll say write file and whenever you
write a file
what you need is essentially the file
path which is the new Boston Q dot X so
that's the path or the file name and the
second thing is what data is going to go
in the file so you can say like hey now
my name is Bucky but that would be
pointless for this tutorial so again
whenever you create your queue or
waiting list for the very first time you
don't want to empty because then
whenever your program actually starts is
going to look alright what files do I
need to curl none I'm done and it will
be the shortest web crawler ever so what
we're going to actually do is we're
going to pass it this in so when you
program first boots up it has one arm
URL in the waiting list and that's your
homepage so then it says ok let me go
ahead and grab that and start crawling
from there so pretty cool and we're
going to do the same thing with crawled
so if not OS path is file
all right now if you don't have that one
created then just go ahead and write
this file crowd now again whenever we
make this crowd file we want to make it
is an empty file nothing inside it and
that's because if we just go ahead and
make it and throw URL in there then our
program is going to think that we
crawled this already but we just created
the file so we obviously didn't curl
this page yet and again later what's
going to happen is we're going to go
ahead and look at the waiting list crawl
the homepage and after we're done
crawling this then we move it to crawled
so not now and yeah there we go so
that's how that works
and now let me just go ahead and create
this write file again nothing new here
I'll say uh create a new file give
myself some room here a def write file
and again we'll just say path or the
file name and actual data you want to
write to it and in order to create a new
file I'm just write a pickles open and
give the path in W stands for right so
whenever you do this this is the file
name and this is in write mode so you
can actually write data to it and in
order to actually write the data to it
you just take that file handle and call
write and then you can pass in whatever
data you want so we're creating the file
writing data to it and it's also a good
housekeeping to close the file when
you're done and all this does is it
frees up memory resources and make sure
you don't have any slow data leaks or
any weird errors or anything like that
so there you go look in sweet looking
beautiful</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>