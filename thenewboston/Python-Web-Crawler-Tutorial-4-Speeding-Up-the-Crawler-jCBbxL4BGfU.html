<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Python Web Crawler Tutorial - 4 - Speeding Up the Crawler | Coder Coacher - Coaching Coders</title><meta content="Python Web Crawler Tutorial - 4 - Speeding Up the Crawler - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/thenewboston/">thenewboston</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Python Web Crawler Tutorial - 4 - Speeding Up the Crawler</b></h2><h5 class="post__date">2016-02-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jCBbxL4BGfU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">are ready mofo so up to this point we
start creating the basic structure of a
crawler and we can kind of figure out or
kind of starting to see how it's going
to work basically it's going to go
through some website get out of this
homepage then look into that thing for a
while and it's going to go ahead and
gather a bunch of links now every time
it finds a link it's going to go ahead
and add it to the waiting list right
here and then once it goes to that page
and curls it it's going to add it to
this crowd file right there simple
enough but we already have a problem and
that is that this method that I just
explained it's going to be slow the
reason for that is because whenever you
write to a file in a Python program
writing to file operations are slow and
I I say slow relatively because nothing
is really slow from our computer it's
going to take like a couple of thousands
of a second we're not really that slow
but whenever you're crawling tens and
thousands of pages it's going to be
noticeably slower than if we were to
just use variables so why don't we just
use variables well the bad thing about
using variables is if our program ever
stops or if we encounter bug our
computer shuts off all of that data is
going to be lost so let's think about
this we can either use files and then
it's a little bit slower but our data is
always saved and you know we don't have
any problems if our computer shuts down
or we can use variables which is going
to get the job done a lot quicker and
you know but we don't have the security
of data loss but you know what eff that
let's go ahead and get the best of both
worlds so this is how we're going to set
up our web crawler what we're going to
do is whenever we are working with this
data we're going to store it in a set
now a set is kind of like a list but
with a list
you can have items like bacon bacon tuna
apples bacon essentially duplicate items
so you can have bacon three times now
what we want to do is we're just working
with a bunch of URLs so we want to store
them in a
set now a set can only have unique
elements so let's say that we have a set
and it has bacon in tune in it if you go
ahead and try to add bacon again it's
not going to add it it's just going to
keep it bacon and tuna so the reason we
want unique elements is because let's
say that we were adding items to the
waiting list well we came across Beauty
and then we're on another page and came
across Beauty again we don't need to add
Beauty this URL to the waiting list
twice it's already in there same thing
with the crowd once you crawl a page you
don't need to add it to the crowd file
every single time you crawled it once
it's in there it's good now another cool
thing is that once we have two unique
sets we ensure that we don't have any
crossover so in other words Beauty is
never going to be in the waiting list
and the crowd list it's going to be in
the waiting list and then once it's
crowded it's going to be moved through
crawl this so anyways like I'm saying
sets solve everything
so while we're working with the data
we're going to go ahead and use sets
because it's a lot quicker and then
periodically we're going to go ahead and
take that data and save it to files so
pretty cool so what I'm going to do is
create two more functions the first one
is how to convert the links and file to
a set and the second one is how to
convert the items in a set and save them
to a file really simple stuff so the
first one I'll make is um I'll just do
file to set so I'll say read a file and
convert each line to set items so again
later on we're going to have our project
file and we'll say we have the waiting
list now the waiting list is going to
have I don't know 30 different URLs in
it waiting to be crawled so this
function is going to do is it's going to
read that file and convert it to a set
in that way we can use in our Python
program a lot quicker so the file to set
and of course all we need is a file name
let me go ahead and add that underscore
alright so whether I write resu LTS the
heck was it giving me a red line stupid
little play charm probably type to wrong
heart so the first thing we're going to
do is we're just going to create an
empty set and then we're going to loop
through that file line-by-line so with
open filename RT that means read text
file as f so again we just refer to the
file that we're reading as f what we
want to do with that file is loop
through each line in that so this
actually looks pretty easy for line and
f and this is just going to iterate
through a file one line at a time what
we want to do is for our set which is
empty right now it's named results we
just want to take each line and add it
to the set so not Len all right
so that's going to give us our line
right there now there is I mean this
looks pretty good right now and this is
going to work but there is one little
issue
you remember how whenever we wrote each
item or whenever we're sticking each
link in the file we added this newline
character and that was just so whenever
you're looking at the files all of the
URLs are bunched up together on the same
line it just made it easier to read for
humans
well now the thing is we got the URL but
we also have that newline character on
there so whenever we read it into a set
we don't want that newline character we
just want the URL so what we can do is
this we need to go ahead and replace in
a replace is a built-in function it
takes two parameters first one is what
character are you trying to replace
we're trying to pre replace that newline
character and the second one is what do
you want to replace it with so we're
just going to go ahead and replace it
with nothing and this is essentially
just saying hey you know that URL plus
newline that we're reading in just go
ahead and delete that newline
of it that's all we're doing talking
into talking to this Python program like
it's one of our friends hey man you know
that new line that you got over there
just get rid of it what am I talking
about
all right now looking good and all we
have to do here is return the results so
again we're going to give it a file say
loop through each line read the URL and
return them as a set looking sweet now
this last one I'm just going to convert
a set to a file so I'll say it through a
set each item in the set will be a new
line in the file and DEF set to file and
since we're working with a web crawler
instead of set and I can't even use the
word set anyways because it's built in
keyword in Python
I'm just going to name it links because
that's all our sets going to be just a
bunch of links and so these are the
links that we want to save to this file
so the first thing I wanted to do is I'm
going to go ahead and call delete file
contents now remember what we can do is
we can go ahead and delete whatever file
because this is the old data all of the
data that is up-to-date is going to be
stored in this set right here so we can
go ahead and get rid of that and we
don't want to append it on there because
this is the good stuff right there all
the juicy links and now what I can do is
just start looping through this set so
say for link and links so loop through
links and treat each item is linked and
actually let me do this sort it link
all right so you don't really have to do
this but what this is going to do is
instead of just crawling and getting a
bunch of URLs and just spewing them in
text file or writing them to a text file
in random order we can go ahead and sort
them before and then if we ever need to
look at our data file all the links are
in alphabetical order which is pretty
cool so go ahead and sort through all
the links that we gathered and for each
one all we want to do is we want to add
them to the end of whatever file that we
want either queue or curl so add them to
the end of the file and again this
function that we made earlier takes two
parameters what file and what do you
want to add on to the end of it
well whatever file we told you and the
link that you are currently iterating
through so now let me clean this up and
check it out we now have two functions
where we can actually read in a file so
if we were you know going to lunch and
we just came back in and we want to run
this program again we can look at that
file and read it into a set and also
once you're running this for a while and
our mom says hey you want to go out for
ice cream mom of course I do I love ice
cream but first let me see let me save
my data being go ahead and take that set
and save it to a file so boom roasted
I'm going to go eat some ice cream now I
will see you guys later</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>