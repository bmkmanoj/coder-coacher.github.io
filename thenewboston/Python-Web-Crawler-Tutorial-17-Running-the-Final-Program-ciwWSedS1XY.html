<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Python Web Crawler Tutorial - 17 - Running the Final Program | Coder Coacher - Coaching Coders</title><meta content="Python Web Crawler Tutorial - 17 - Running the Final Program - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/thenewboston/">thenewboston</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Python Web Crawler Tutorial - 17 - Running the Final Program</b></h2><h5 class="post__date">2016-03-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ciwWSedS1XY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the really cool thing about working with
threads and Python is you already
understand the concept of it you create
a bunch of workers and give them a bunch
of jobs but the awesome thing is you
don't need to specify hey this thread
needs to do that job that thread needs
to do this job all you do is you create
like X amount of workers let's say we're
going to have eight and then as long as
they have jobs they're going to start
doing those jobs automatically it's
freaking fantastic and I'll show you
guys how awesome it is
so we'll say create worker threads and
these threads by the way they're going
to die I'll say will die when main exits
so again you know how whenever you're
running your program it's going to take
like one or two minutes depending on the
size of your website if you're like all
right well something went wrong and I'm
just going to close out of this but crap
now I got a bunch of threads run in the
background after I probably going to
command line and stop those well the
cool thing is I'm going to set this up
where if you ever want to terminate your
main program then it's going to call all
the spiders as well so pretty cool so
dev create workers and you can name this
create spiders or create threads or
whatever you want to do and the reason I
like naming it create workers is because
I like just giving a generic name
because then whenever I'm making other
programs I can just copy all of these
and pretty much just paste them in and
change like one line of code so you know
pretty much is for laziness so all we're
doing in this is we're creating eight
generic spiders so I'll say four hundred
score and range number of threads so if
you want to create eight write eight if
you want to create 16 write 16 since
we're going to let the user specify
earlier I'm just going to write number
of threads right there if you guys are
like whoa whoa whoa
why did you write your variable as a
underscore instead of like X or num or
anything like that well usually you have
a variable right here like X whenever
you want to do something with this value
so this is just going to be equal to
zero one two three four five six seven
however I'm not using that number like a
formula or an algorithm or anything I
just want to loop through eight times
and create my threads I really don't
care what that is so the convention is
whenever you need to do that just write
an underscore and it kind of disregards
that value and love to you know continue
using a range for loop whatever you want
to call it so we're going to loop
through this eight times and what do we
want to do each time well we just want
to create a thread so I'm going to make
a variable called T and how do you
create a thread just call the threading
module thread and whenever you call this
third class the one thing that you need
to pass in is a target so you're making
a worker
what is this worker supposed to do and
that's the last function that we're
going to create and that's just going to
say work now again we didn't create it
yet so that's why it's given us error
but all we're doing is we're going to
create eight workers and the only thing
that they can do is this function right
here which is work so whatever we put in
there is actually going to be the job of
the thread so pretty cool now this is
optional but I actually want to make it
a daemon and again that is just ensuring
that it runs as a daemon process and
it's going to die whenever the main
exits and the last thing you need to
call for your thread whenever you create
it is start so now we have one more
function to create which is actually
work because right now what essentially
we did is we created like eight workers
and they're just standing in the factory
and are like uh what am I supposed to do
now and of course we have the to-do list
in our hand but you know we need back
hey do these things so that's what we're
going to do so all this work function is
going to do is do the next job in the
queue
one day I'll learn how to type Q without
having any typos so work is going to be
really simple
we're just going to set this equal to
true and what do you want each spider to
do well this is actually really easy
just go ahead and grab one of these
items from the list and that's actually
in your thread queue now so URL equals Q
dot git so it's going to get the next
item from the thread queue which is
essentially one of these links and now
once you have those links that need to
be crawled just called spider dot crawl
page and as you see whenever I sit um
call crawl page it takes two pieces of
information that the first one is just
the name of the thread and that's just
so whenever running this program we can
display the user thread five is curling
the forum thread eight is curling the
buckey's profile page whatever and the
second one is just the URL so how do we
get the name of the thread well it's
actually a cool piece of Python that's
already built in to the threading module
threading current thread name so each
thread already comes with a name and you
can name it something special if you
want but just keep the default name and
the URL is the page that we want you to
crawl in the last thing the last line of
code for this entire program is queue
tasks done now again whenever you have a
spider you give it some work and
whenever it's done with that job it just
needs to say hey I'm done working on it
and again that just tells your operating
systems that says hey I'm ready for the
next job now and it helps free up some
memory if there are any more jobs and
again all of this is taken care of
behind the scenes with threading and
queue modules but there you go
essentially all this program does
multi-threading wise is it creates a
bunch of workers and then it creates a
bunch of jobs and these are the jobs and
as long as there are more items in the
list then your workers or threads are
going to keep crawling all right now
check it out
actually said that that was the last
line but we have two more lines because
if we just run this right now we created
all these functions but we didn't even
call them yet so we just need to rate
first what do you need to do we need to
create workers because who the heck is
gonna crawl who the heck is going to do
our work if we don't have any workers
and crawl which pretty much means create
your jobs or make your to-do list
whatever and you can even even keep this
right now but let me just go ahead and
delete it so check it out right now I
don't have any project directory I don't
have any of those files nothing like
that however when I run main that first
fighter let me go ahead and stop this
right now so that's what that first
spider did it cron my home page right
here and I said hey all right I got 20
lengths in the queue and then we created
a bunch of little threads and as long as
we had jobs item in the items in the
queue they went and they did their thing
so each one got an item from the waiting
list crawled that page and added it to
the queue so look at that and now in
like what was that it was running for
like four seconds maybe all of these
pages have already been crawled so it
already crawled 15 pages on my website
and it already discovered all of these
links man I can't even scroll in my
middle wheel all right 861 more pages to
look at so this bad boy is fast and it
looks like it is working beautifully and
also I posted all of this source code on
my github page so if you guys just go to
github.com slash bucky roberts make sure
you follow me oh no it's gonna be
somewhere around here it's not there for
me because I'm already logged in can't
really follow myself but if you click
spider right here or you can just type
in this URL then what you're going to
see is here is all the source code let
me bump that down a little bit guys
might be able to see a little bit better
and yeah so I already have a bunch of
people working um if you guys are like
you know what this can be improved that
can be improved
then submit a pull request and I'll
review it in you know it will help out
so it's a pretty cool community project
it's actually getting really popular I
did I had no idea that you know all web
crawlers so many people would actually
use this and it would be this popular
but I just posted this a few days ago
and I already has like six to eight
stars 23 Forks bunch of people working
on it so yeah thank you for everyone
who's contributed thank you for everyone
that will contribute in the future and
yeah let's go ahead and make the best
spider ever</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>