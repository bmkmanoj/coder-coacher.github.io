<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Python Programming Tutorial - 27 - How to Build a Web Crawler (3/3) | Coder Coacher - Coaching Coders</title><meta content="Python Programming Tutorial - 27 - How to Build a Web Crawler (3/3) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/thenewboston/">thenewboston</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Python Programming Tutorial - 27 - How to Build a Web Crawler (3/3)</b></h2><h5 class="post__date">2014-09-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pLHejmLB16o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright guys so now we know how to make
a very simple web crawler at least a web
crawler for my cell right here Bucky's
room Oryx last trade I'm guessing that
you're probably not going to only want
to crawl my web page I mean it is pretty
cool but you're probably going to want
to let your web crawler do things like
crawl other web pages start crawling the
entire internet and gathering
information so instead of just going
through page by page by page I'm going
to show you guys how to make a dynamic
web crawler that can pretty much gather
links from a web page and crawl those as
well so spread out start crawling the
entire web so what I'm going to do is
actually build a new function and what
this web crawler is going to do this
example is it's pretty much not only
going to go through page by page but for
every item it's in actually go to that
link where the item is and you can
actually do a couple different things
you can either just gather information
so say that you're making I don't know
maybe a GUI to trade your items well
what we do want to do is gather maybe
the item title maybe some information
about the owner so you can either just
gather information from the web page or
what most web crawlers do is they go to
that web page gather all of the links
from that web page and store them so
they can either crawl them later or um
start spreading out with Karla men's
instantly whatever you want to do so I'm
going to show you guys both of those
techniques and by the end you're going
to learn how to make some pretty sweet
web crawlers so anyways let's go back in
here and I'll get myself a little bit
more space so again like I said what
this is going to do is go to each item
one by one and do something cool with it
so I'm going to name this function get
single item data in the parameter that
we pass it in is the URL of the item now
the cool thing about this is we actually
pulled the URL from item and it's stored
in this href variable
so we're going to be passing it in as
item URL so how awesome is that like
half the work is already done because we
already um got the URL of each item in
the last Oriole pretty freakin sweet so
now with this again the first thing that
we always want to do is request the data
from the website convert it in plain
text or in other words just get the
source code and then make it into a
beautiful soup object well check this
out as well oh my god this is like the
easiest soro ever because everything is
basically done for us so again in this
function as well tired of looking at
fantasies say uh I don't know here we go
alright
so again whenever we're on this page
what we need to do is we need to make a
new beautiful soup object because the
old one was for the search page so in
this one we requested that URL which is
now named item URL we got the source
code from it and then we convert it to a
beautiful soup object so now we can
crawl it and like pull out links from it
titles whatever we will want so the
first thing I want to do is I want to
show you guys just say you want to
gather information from this page so for
each item will say that you wanted to
get like I don't know will get the item
name and will just get the item name for
now I'll show you guys a real keep it
really basic so for item name in soup
find all actually I can just copy this
right here
what I want to do is get the item name
from each individual each individual
page now my guess is since it's a new
page it's not probably going to have the
same class because you know it has a
different design so let's go ahead and
find out what information we can use for
this all right so the three things you
need is what HTML element it is it's a
div what attribute are you looking for
at the class and what's the value of it
well I'm just copy this so I don't
forget I name so div class I name those
are the three things you need whenever
you use it so again it's not a link
before but the title is now in a div and
the class of it is I name which stands
for item name easy to remember so now on
each page let's just go ahead and print
out the item name so I'd name about
string remember it takes whatever this
HTML element is and pulls a string from
it or just the text from inside so now
what I can do is actually just run this
in let's see we actually need to call
this right here so copy this and
remember right here is basically going
through the search page so for each item
actually comment these out because we
don't want a bunch of stuff cluttering
up our results so basically for each
item this href was the URL so go to each
items individual page and right now
we're just printing out the title of it
and actually just so I can test this to
make sure it's working instead of
crawling just one page let's curl three
so now whenever I run this hopefully
it'll work so again what this is doing
right now is it's not just going to this
main search page in printing out the
titles it's actually going to each of
these pages one by one and printing out
the titles of each
so check that out and it looks like it
is indeed working for more than one page
so again since I know that's working no
need to keep going looks good and also I
probably should mention this whenever
you want to stop your program like maybe
you accidentally said crawl 100 pages
you just hit this little icon right here
stop and also whenever you want to clear
your console you just hit this trashcan
clear oh all right so now our web page
orcses me our web crawler cannot only
crawl explicitly the pages we told it to
it can find links then dynamically and
crawl those as well now another cool
thing that you can do is say ok start
here find some pages crawl those pages
well what if you wanted to go to a
certain page and find all the links on
this page and crawl those as well well
this is actually one of the most common
things in web crawlers so might as well
show you guys I'd do right now so what
we can do is put for link and I guess we
might as well type this out trying to be
lazy in link in soup fine all now in
this case this is actually going to be
the easiest thing ever we don't need to
give it any explicit data we're just
going to say find all a find all the
links on this page so it's going to find
this one this one this one this one this
one this one any link that's on the page
it's going to gather so um you know it's
actually it's kind of weird that the
more work your program does the easier
it is the code mmm funny how things are
so anyways now that we have the link
what we can do is just build a complete
version of it because if we just look at
it like any of these links right here we
have the same problem before it just
links to the arm extension of it and not
the base of it because you know let's
just help people develop websites so of
course if we just try to go to this in
our browser
it would try to do it on the localhost
and not the webpage so build the full
version of it and just to make sure it's
working we'll print that out
so print href and actually let me
comment this out right here so again
what our program is going to do right
now is actually don't want to mess up my
loops so what our program is going to do
right now is say ok this is your
starting point go page by page go to
each items page in on that page gather
all the links hopefully that's what it
does if I didn't mess anything up so
that is what it's doing right here and I
know this is kind of a it's kind of a
weird example because on each page the
links are very similar like I don't know
like send a message on view profile so
they're going to look the same but what
you can do is if you don't want to crawl
the same link over and over again if you
guys remember a couple tutorials ago I
tell you guys about something called
sets now in the example I showed you
guys just a very simple grocery list and
you're like ok when the heck am I ever
going to make a grocery list piece of
software this is the dumbest thing ever
well this is one of the instances where
you can use a set because it's basically
a list of items but it has all unique
values every item it can't repeat like
you can end a regular list so that way
you can throw all of your URLs in a set
and it will only crow each URL once
instead of you know crawling on index
each time so that's one cool thing or
one cool way that you can use sets to
make a very powerful web crawler so
anyways that is the very basics of not
only how you can pull information from a
single page but also gather the links
from those pages then of course you'd
say ok now go to this page gather all
the links before you know it you have a
million links and you have a next Google
so that is the basics of making a very
simple web crawler if you guys have any
questions at all please ask me I'm going
to forum also what I
is on my forum I posted all the source
code so if you go to the forum and in
the Python section it is right here 25
how to make a web crawler so maybe you
have some trouble or you're messing
something up then I just copy that and
you'll be good to go and also another
thing I want to point out is try to
gather this information right here with
a web crawler if you can make a web
crawler to go to this website and get
the source code of how to make a itself
that will be the coolest thing ever so
try to do that that will be your next
challenge
but for now then guys for watching don't
forget subscribe and well
see you next time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>