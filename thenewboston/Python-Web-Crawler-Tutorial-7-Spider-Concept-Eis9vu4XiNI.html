<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Python Web Crawler Tutorial - 7 - Spider Concept | Coder Coacher - Coaching Coders</title><meta content="Python Web Crawler Tutorial - 7 - Spider Concept - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/thenewboston/">thenewboston</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Python Web Crawler Tutorial - 7 - Spider Concept</b></h2><h5 class="post__date">2016-02-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Eis9vu4XiNI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">already hostas welcome back and in this
video we are going to be making the most
important class in this entire program
and that is the actual spider
so we already made this class right here
and all this class does is essentially
whenever we connect to a page and gather
that HTML we can throw in here and it's
going to give us all of the links from
that HTML but the thing is uh we don't
even have any HTML yet I mean we took a
look at a quick demo but you know we
can't even connect the pages yet so
that's what we're going to be doing
building the actual heart of our program
so go ahead and right click new Python
file just name a spider and the class is
just going to be called spider
so before we start coding let me go
ahead and explain exactly what the
spider is going to do so what's going to
happen is we're going to have a bunch of
links in the waiting list so what's
going to happen is the spider is going
to go ahead grab one of those links and
then once it has one it's going to
connect to that page so once it's
connected it's going to go ahead and
essentially grab all of this HTML right
here and then once it has the HTML it's
going to go ahead and throw into link
finder so link finder is going to do its
thing parse through it and return all of
the links we already took a look at how
that was done
so once the spider has all the links
from that page it's going to go ahead
and add those to the waiting list
because if I scroll down whenever it
sees a link it's going to check the
waiting list and make sure it's not in
it and it's also going to make sure it
didn't crawl it already and it's going
to add those links to the waiting list
now again another thing it's going to do
once it's done crawling this page is
it's going to take it from the waiting
list and move it to the crowd file
because that way we make sure that we
aren't crawling the same page twice
simple enough and it actually is pretty
simple once we start getting into the
code but before we start you know just
start making some functions methods
whatever I want to talk to you guys
about a prop
and that is this so here's a little
thingy I made in Photoshop real quick
with my awesome graphic design skills
and we already see that all right so
this is a spider it has a waiting list
right there
it basically is full of links it's going
to take one of those links crawl the
page get all the links and then plop
that page into crawled to make sure it
doesn't crawl it again and all the links
that it found it's going to add it to
the waiting list that they weren't there
already and do its thing until all of
the pages on your website or crawled
so look what happens whenever we try to
make another spider because I mean we
want this program to be fast so we I
mean we can just develop it so we have
one spider running at a time but imagine
if Google only could curl one web page
at a time I mean no BOTS do this in it's
terribly efficient even if you're just
using this for your own website alright
so we made the spider class now let's go
ahead and make another instance of it so
we can run this on our website again and
look at this so we're going to have all
these spiders running and everything is
going to work three times as fast wait a
minute
you see the thing is these spiders they
each have their own individual class
variables they have their own waiting
list and their own crawled list so how
the heck is this spider the second one
supposed to know what's in spiders one
waiting list are they going to talk to
each other are they going Mike hey
spider - did you crawl this page he's
like hey I do it over here that one's
actually my waiting list he's like what
who said that oh so if we design it like
this it's going to be a pain in the butt
so we are not going to make a typical
class in the sense that all of these
classes are going to have their own set
of variables what we need to do is
something like this we need to take the
waiting list and the crowd file and we
need to share those among a bunch of
individual spiders so then let me show
you what happens let me make a bunch of
these bad boys
all right so we can have is
many spiders as our operating system can
handle and they are all looking at the
same waiting list so in other words once
one of these spiders takes the item off
the waiting list and plops it in the
crowd all of these spiders can see it so
that way they don't have copies of these
files they're all looking at the same
file it's going to make everything
perfect super fast and super efficient
so that's the concept now you have to
figure out how to heck to do it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>