<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Python Web Crawler Tutorial - 13 - Adding Links to the Queue | Coder Coacher - Coaching Coders</title><meta content="Python Web Crawler Tutorial - 13 - Adding Links to the Queue - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/thenewboston/">thenewboston</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Python Web Crawler Tutorial - 13 - Adding Links to the Queue</b></h2><h5 class="post__date">2016-02-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rxGUiLcW0cI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">our RIT hostages so in the last video
what we did is we made a quick function
to connect to a web page and it gathered
up all of the links boo-boo-boo-boo boom
let's say gathered up a hundred links
well then what it needs to do is it
needs to go ahead and add them to the
queue so those are new links and we want
to crawl those pages as well simple
enough so that's what we're gonna be
doing and of course it's gonna be a
static method and I'll just say what did
I name it add links to queue alright so
this function it just takes a set of
links and it adds it to the already
existing waiting list or all the other
links that we already have on the
waiting list so we can just go ahead and
loop through the set so say for each URL
in that link set that we just passed in
all those links that we just gathered
from that web page and instead of just
plopping them all in there we first need
to check a couple things first we need
to make sure that they aren't already in
the waiting list because if they already
are in there then we don't want to add
them again another thing that we need to
do is we need to make sure they aren't
in the crawl list because that would
mean that we already crawled that site
so there's no use crawling it again so
I'll say if URL in spider dot Q continue
and if euro in spider crawl continue so
essentially this is gonna loop through
each link one by one and if it finds out
that it's in the waiting list or the
crowd it's gonna say oops actually never
mind adding it just go ahead and go to
the next item in the list now there's
actually one more check that we need to
do tonight let me actually go ahead and
type this in I'll explain what's going
on so a spider domain name not in url
continue as well so what does this mean
well the domain name that we are going
to pass in is
equal to this right here the new
boston.com why do we care about that
well whenever I ran this program this is
embarrassing but whenever I ran this
program for the first time I wanted to
see if it would curl all the pages on my
site so I left it running and then I
went off to hang out with my friends and
I came back and it curled like a few
million pages I'm like what happened and
it turned out once it got to someone's
profile what it did is it found these
links as well YouTube Facebook Twitter
so then added those to the waiting list
and from there it tried to call YouTube
and Facebook and essentially the entire
internet so I didn't even know that this
was a pretty fast multi-threaded program
I don't think that you know I can make
one that efficient at least that runs on
my computer that I'm sitting at right
now so what we need to do for every link
we discover we want to make sure that
the new boston.com is in that URL so no
matter if i'm on my account page check
this out the new boston come no matter
if i go to the home page the new boston
com
if i go to the videos pages the new
boston com so as long as this is in the
URL that means that this URL must belong
to my site pretty sweet so that's what
that does essentially saying hey don't
crawl the entire internet just Carl our
site right there so if it passes all of
those tests then what we can do is we
can just add that link to the waiting
list so it add URL boom roasted now the
last thing we need to do is only like
two lines of code static method and that
is once you pretty much did all
everything you need to do you just need
to update those files and all this
function is gonna do is you know how
we've been working with these sets it's
just gonna say hey save that data to a
file so you know I already explained
like a hundred times why we're doing
that so def what did I name it update
files
and we already created this function to
take a set and convert it to a file so
it saves us a bunch of time so sit to
file and what are two pieces of
information that it needs the first one
is what set are you trying to convert
and that is just the shared soup Q first
Q and what file what do you want me to
save this to Q file right there so hey
here is the set that we've been working
with and this is where we want you to
save it to and now we just need to do
the same thing with the crowd
so spider crawled spider crawled file
and check this out this is actually
complete so there you go that is your
spider class right there and man look
how beautiful that is probably a
thousand bugs now I think we did good so
yeah I guess I'll see you guys in the
next video</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>