<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Python Web Crawler Tutorial - 16 - Creating Jobs | Coder Coacher - Coaching Coders</title><meta content="Python Web Crawler Tutorial - 16 - Creating Jobs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/thenewboston/">thenewboston</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Python Web Crawler Tutorial - 16 - Creating Jobs</b></h2><h5 class="post__date">2016-03-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zfBhpmhXUqM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">already houses welcome back and in the
last video what we did is we made that
very first spider and all that that guy
did is he went to my homepage gobbled up
all these links and knows like alright
there are all the links and I'm also
going to take that URL and move it to
crawled so you don't accidentally curl
the homepage again and we're like thank
you mate for that now what we can do is
we can go ahead and make eight spiders
multi-threaded and all of these can run
at the same time working to crawl the
rest of the website so what we're going
to do is we're gonna say hey you know
those links in the key right here this
is pretty much our to-do list so I'm
gonna create eight workers eat spiders
whatever you want to call them and once
I have a bunch of jobs and a bunch of
workers they can just go ahead and do
their thing so let me just go ahead and
make a function called crawl since this
is gonna be kind of the heart of it so
all right what things fun function is
gonna do is it's gonna check if there
are items in the to-do list and if so
then it's gonna crawl them so check if
there are items and queue if so crawl
them all right simple enough so now cute
links just set these equal to file to
sets cue file alright so again like
before we want to be working with a set
so we're gonna say hey whatever links
are in here convert those to a set
because it's a whole lot faster and then
just store it in queued links right
there so the next thing I want to do is
how do you tell if there are items that
need to be crawled well you just test
for the length of it so if length of Q
blinks
is greater than zero then go ahead and
continue so essentially if there are
zero items in here then our program is
done and we don't have to do anything
but as long as there is at least one
link in there then we have to keep
crawling so the first thing I'm actually
going to do is just print a little
indicator and this is just so the user
knows that something's going on and I'll
say I'll give it indicator of like how
many links are left in the to do list so
since it's going to be number and he
converts to a string and I just need to
say for length cute links and then I'll
say that many links in the queue queue
eue looking good so after this I'm
actually going to break it up into great
jobs and I didn't make this function yet
but I will in just a second so again all
we're doing right here is checking if
there are links that need to be crawled
now after this like I said we're going
to need to create jobs in actually you
feel like sticking that above here
because all my threading functions I
kind of want to group together so all of
these links that you see right here we
pretty much need to stick it in the
thread queue because that's what threads
need they can't just work directly with
a set or with a file and I don't know
you have to complain about the people
who created Python instead that but it's
really easy we pretty much this loop
through the set and then put each item
in the thread queue it's incredibly easy
so I'll just write each queued link is a
new job
alright so define create jobs and again
this is going to be called as long as
there are links to be crawled so how do
we do that let me just copy some code
right there all we do is say for a link
in file to set Q then for the thread Q
put link so again all we're doing is
we're taking these
and sticking them in the thread queue
nothing hard or anything like that so
the next thing you need to call on here
is queue join and that just ensures that
whenever you have a bunch of threads
that they don't bump into each other and
they kind of lock their operations until
the next one is done so most of the time
you want them working at the same time
but whenever they're trying to find out
if there are more jobs
you actually want them just say hey
don't try to do this all at once wait
your turn
be nice workers so there you go and then
after it's done creating a job and doing
this thing we want to call this again
just to get the updated version of it
all right so now we have a bunch of jobs
now we have a bunch of things that our
threads can do but we don't even have
any workers yet we don't even have any
threads so again right here all we did
is we created a bunch of jobs added a
bunch of items to the to-do list and
made it compatible for threads so now
let's actually go ahead and create those
threads</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>