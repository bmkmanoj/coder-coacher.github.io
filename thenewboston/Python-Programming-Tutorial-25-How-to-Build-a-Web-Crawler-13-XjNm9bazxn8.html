<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Python Programming Tutorial - 25 - How to Build a Web Crawler (1/3) | Coder Coacher - Coaching Coders</title><meta content="Python Programming Tutorial - 25 - How to Build a Web Crawler (1/3) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/thenewboston/">thenewboston</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Python Programming Tutorial - 25 - How to Build a Web Crawler (1/3)</b></h2><h5 class="post__date">2014-09-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XjNm9bazxn8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright guys welcome back to another
video and this is definitely going to be
the coolest tutorial yet because I'm
going to be showing you guys how to make
a web crawler some people may call them
spiders but they're basically programs
that you can give it any URL you want
and they can go to that web page and
they can either it can actually do a
couple things you can either just gather
information from that web page or it
could find all the links and go on to
those web pages pretty much crawl the
web the most popular web crawler is
probably Google's which goes through a
bunch of pages gathers information for a
search engine so of course we're not
going to be building one that big but
this one is actually going to be pretty
cool and also the reason I had this page
up is because I'm not sure what websites
actually allow you to curl them I'm
pretty sure like um eBay or Craigslist
it's against their Terms of Service like
I don't know that for sure but I'm like
they probably don't want you doing it so
I figured just to test this out just so
I wouldn't get in trouble I'm just doing
it on one of my own web pages so if you
go to Bucky's room org slash trade slash
index dot PHP this is a section on my
website where I let users like trade
items so this is the home page right
here and it's actually kind of important
to see the structure of it now if you go
to search this is the page that we're
going to be curling so of course this
lists all the items in all the
categories and if you notice a couple
things we want to take note of it has a
couple pages that the items are on so
then only less 30 items per page and
we're going to be using this parameter
right here the page number and we're
going to be giving our spider that
information so it isn't just stuck on
one page the whole time so what our web
crawler is going to do is go page by
page and it can either just gather the
links or maybe you can just gather the
titles whatever we want it to do but
anyways that is how my website is laid
out so let's go ahead and open pycharm
and start coding this bad boy
now there are couple modules that we
need to import first the first is
requests and that's because remember
this is whenever you're connecting to
the Internet in Python this is well it's
my favorite way to there's actually a
couple different ways but this is to
request information from a web page and
another thing that we're going to need
is my favorite module of all time so go
to settings and this is when I was
changing the font size for the video all
right where am I get out of that so go
to project interpreter and that's where
you can see all your modules and the
package or module that we want to import
is called beautiful soup 4 so again if
you don't have this added which you
probably don't just add it like before I
showed you in the I don't know like
couple tutorials ago look for a
beautiful soup for and what this is is
it's your module that lets you pretty
much go to a website and in sort through
data so right now that request is kind
of limited because it can connect to a
webpage but it's kind of dumb so what
beautiful soup for allows you to do is
pretty much look at all the page source
and you can say things like ok get all
the links from this page or get all the
you know headers or titles or anything
like that so you guys are going to be
seeing how cool it is later on but
anyways whenever you have it installed
what you can say is from bs4 import
beautiful soup and that's going to
import all the tools that we need and
again if you're ever looking at
documentation to use this online it's
either you can type in beautiful soup
bs4 arm beautiful suit for and the web
page is going to pop up so you know if
you ever want more information than just
giving you guys a heads up so now we
imported all the crap we need and the
first thing I actually want to do is
go back to your main search so by
default
we're just on this page right now and
this is the page that we're going to
start from the main search page so make
sure you're on this right here not on
any not like on page three or anything
like that so the first thing I want to
do is I wanted to make the core spider
just the main spider that can sift
through and we'll just do something
really simple at first before we get to
advanced so since this is a spider
that's going to be limited to the
trading section on my website I just
want to name it trade spider
so after this what I want to do is I
actually want to give it a parameter of
max pages because whenever we're testing
things out say we're just testing it for
the first time make sure it can gather
links or something well I don't want it
to crawl all these pages what if there's
a hundred pages well if something isn't
working in our program it's going to
take a while to you know go through and
find the bug so whenever I have this
parameter max pages by default we can
just set it equal to one or two at first
and then once everything's working we'll
just set equal to like 50 or however
many pages we want to curl so by default
I'm going to set page equal to one so
this is going to change every time it
loop through a page so basically it's
going to do this crawl this page and
then in our spider we're going to set
that variable equal to page two and that
way it knows to go to page two right
there because of course this is a
program it can't actually like click
buttons on your web page so that's why
we need to throw in everything in
parameters so once we have that what we
need to do is we need to make a wild
loop so we're going to say while the
page number is less than max pages so
again we're going to go through the
pages one by one and it's going to stop
whenever we get to max pages so if we
pass in 10 right here it's going to say
okay
I'm pretty much going to crawl 10 pages
simple enough so the first thing I want
to do is actually the first thing that
we need to do is we need to build the
basic URL because it's saying okay I am
a web crawler but you kind of need to
give me a web page of the curl
well the web page is this if you copy
that URL again around page 2 but we'll
fix that right now again of course the
easier thing to do is just throw all of
this in a variable so I'm going to name
it URL and this is the URL that it's
going to curl so set this equal to this
right here now if we just left this as
is then every single time it would just
crow page number two and that's
obviously not what we want so what we
want to do is actually delete this last
number right here and the page that we
want it to crow is whatever page this
variable is equal to so get this base
URL and then on the end of it
Tech on STR page so again if this
variable changes every time the URL the
first time is going to be page 1 then
page 2 and page 3 and it's going to go
all the way until it gets the however
many pages we told it to curl so that's
how you can have your URL change every
single time so now we have a URL what do
we need to do now well what we need to
do is actually request the data from
that website or connect to that website
through Python so the results that we
get back I'm going to be storing in a
variable called source code and set this
equal to request dot get URL so again
each time this loops it's going to
connect to that web page and store the
results in a variable car called source
code
so basically what we have now is as you
Alvie page source it'll be easier
basically what we have now is all of
this crap stored in that variable
however whenever you connect to a
website it's a little bit more complex
than just getting the page source it
also comes with other stuff like headers
I don't know I don't even know if you
guys know what headers are but they're
the extra stuff that like your um well
it's like extra crap that the user
doesn't need to know about it's probably
the easiest example but all we really
want is that um source code so what I
can do now is I can put plain underscore
text or you can name your variable
anything you want and if you put</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>