<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Python Programming Tutorial - 26 - How to Build a Web Crawler (2/3) | Coder Coacher - Coaching Coders</title><meta content="Python Programming Tutorial - 26 - How to Build a Web Crawler (2/3) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/thenewboston/">thenewboston</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Python Programming Tutorial - 26 - How to Build a Web Crawler (2/3)</b></h2><h5 class="post__date">2014-09-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sVNJOiTBi_8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">source-code dot text what this is gonna
do is it's pretty much just going to
take the text of this request all the
crap that we need to crawl and we're
gonna store it in a variable called
plain text so again we're not we don't
really care about the extra crap that
happens behind the scenes we just want
the text the links the images the good
meat of a website so right now all my
good stuff is stored in plain text
that's what we can sift through so the
only other kind of transition I need to
make now is is pretty much this right
now I have all the information from a
webpage however since we're using this
class right here beautifulsoup
we need to take that and convert it to a
beautiful soup object and that's because
this class right here it needs its data
formatted in a special way to be able to
crawl through it and sit through and
sort through like all the links to stuff
easily so my beautiful soup object I'm
gonna store it you know object named
soup now in order to create an object
it's actually really easy just put the
class name and the one parameter that
you pass into it is that text so
basically plain and simple this is all
the source code from the website
whatever webpage we said to Crowe now
when we have this soup object this soup
object is the thing that we can sort
through say okay find all the links in
soup find all the titles in soup so I
know it's kind of confusing so far but
you guys are going to see something
really cool right now so since we have
all that data in soup we can do
something like this say we wanted to
know what's an easy thing to do
alright we'll say that we'll let's
gather just the the titles of each item
so maybe we just wanted to make a web
crawler to gather the titles of each
item just so we can sort through them
easier well we just can't be like okay
get the title of this how do we type it
in because even though that you know if
we were talking to our friend then he
might see what the titles are Arkham
Peter doesn't understand that language
so what we need to do is if we
right-click this inspect element we need
to find something in this source code
that is unique so we'll say okay
this is the title right here so we can't
just say links because you know my name
is a link
the categories are links what trait is
specific to this title that we can tell
beautifulsoup to use
well actually this class right here item
name no other links except the titles
have that class so we can just copy that
and basically what we're going to say is
okay gather all the links on this page
but only if they have the class item
name and whenever it does that it only
gets the titles so in order to do that
put for link in su find all now this
parameter is gonna go through that
source code and find all of a specific
item depending on on the information to
give it and I'll show you guys what I
mean right now so the first thing it
says okay what element do you want me to
find well we want to find the links and
of course links are named a for anchors
in HTML so find all the links now if you
just ran this it'll return all of the
links go to return you know the category
names the ratings names we don't want
that so we need to give it one more
parameter and that's this add a comma
and in between curly braces
remember the item name was a class so it
says okay you can either put the
attribute such as ID class arm so on and
so forth there so it says okay what
attribute do you want me to look for and
after this you add a colon and then you
give it the value such as item name said
that kind of item name a legume low so
what this is going to do
is it's gonna loop through all of your
source code and it's gonna pick out the
links with a class of item name so again
this is how we tell beautifulsoup to get
all the titles I know it's kind of weird
but hey it's a computer not so smart as
us or maybe they're smarter than us
no haven't really figured that out yet
so right now it's plucking out all of
the titles well okay I have like a
million things to teach you well we were
refreshes so I can so whenever we
inspect these elements this is what it's
gathering basically whatever is
highlighted right here but now saying
okay well what do you want from here do
you want the href pretty much your o do
you want the class name do you want the
crap that's inside here I mean this is a
lot of crap do you want me just to give
it give everything to you back well
let's say that we actually only wanted
the URL so the link address of each one
so what we can do is we can say okay
we're gonna make a new variable called
href and it's gonna be equal to link
because remember everything gives back
this treated is linked and we'll put get
href so again what this does is it says
okay I know you're looking at this
entire thing right here but we don't
want all this extra crap we don't like
need the anchor tags and stuff like this
the only thing we want is the text
that's inside this href attribute so for
each item what this is gonna do is it's
gonna give you this and actually let me
print this out real quick and you guys
are going to understand what I mean so
now if I print href let me run this real
quick all right so of course if we just
run this right now nothing's going to
happen because we didn't call it so
let's call it right there and just the
test I'm going to throw in Mac's pages
is one
so now let me right click and run Bucky
and check it out and why is this not oh
I see all right so what we did is we
accidentally put the Paigey cattle 1 and
we put this so since 1 is not less than
1 it never ran and also forgot to do one
other thing we also need to increment
the pages each time even though we're
only searching one page right now so
again let's run this and check it out so
what this basically did is it went to
that page and for every single um title
link it pulled out the href or the URL
and then it printed it out on the screen
and then of course I went to the next
page but that's when our condition broke
right here so of course if we're
actually crawling and gathering this
data it wouldn't be very helpful because
this URL is great for you know whenever
I was designing the website but if I try
to go to one of these in my browser you
know it's not gonna it's not gonna
happen
so well is a better thing to do is if
you add actually let me just do the
shortcut of this actually do throw it
away I guess so basically you have to
look okay this URL ends it trade the
trade directory so what we need to do is
if we copy all of this right here
we can build a complete URL by writing
href put this equal to HTTPS Bucky's
room dot org and then add the ending
part of it and now whenever you run this
we now have a full URL so again that URL
works this one works
this one works what we did is let me
close these to the right
we now successfully built a web crawler
that goes through every single item and
pulls out the URL of it so again that is
pretty much the very basics of how
Google um pretty much pulls links from
your website so another thing that I
want to do is I want to show you guys
well what if you don't want the URL what
if you just want the title or something
well what you can do in that case is
this and even if you don't want the
title this is actually a pretty good to
learn so for everything you pull out of
a link it's good to store it in a new
variable so set the title equal to this
remember all of that linker information
is still in that link variable so you
always add link now to get the
information inside it I just added edit
this er equipment now the part inside
your link right here the string without
any of the HTML that is actually a
parameter called string just gonna say
okay this lick is the entire um HTML but
we only want the text inside it so
that's when you use string so now if we
were to do something like print title
and run this what it's going to do is of
course we printed the href as well but
now we printed the title of it
underneath it so at this this is the URL
for this item this is the URL for this
item so on and so forth so check this
out we now have a beginning of a very
simple but very cool web crawler spider
whatever you want to call it and in the
next video what I'm going to show you
guys how to do is actually make this
more dynamic because right now we give a
certain number of pages basically giving
it a list of URLs and it can go to that
and when it's done it's done but what if
we wanted this web crawler to go on
forever such as we want to say okay go
to a new page get those all let go
all of those links crawl those and on
those pages gather those links crawl
those and it can just going keep going
on and on forever well that's what I'm
gonna be showing you guys in the next
tutorial but for now same guys for
watching don't forget subscribe and I
will see them</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>