<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Python Website Scanner Tutorial - 5 - robots.txt | Coder Coacher - Coaching Coders</title><meta content="Python Website Scanner Tutorial - 5 - robots.txt - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/thenewboston/">thenewboston</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Python Website Scanner Tutorial - 5 - robots.txt</b></h2><h5 class="post__date">2015-10-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8ZZSd0cdymo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright guys welcome back in in this
video I am gonna show you how to build a
Python tool to scan for a robots.txt
file now if you don't know what the
robots that text file is it's this so
you know whenever you make a website a
bunch of search engines like Google and
Yahoo they're gonna crawl your website
and that's how with the crawler they're
gonna go through page by page in you
know store in their search engine so
then whenever people type in the new
Boston comm all the results pop up for
everyone's you know profiles page is a
forum post whatever now the problem with
this is whenever you're developing a
website there are some pages that you
don't want Google to crawl so examples
of these pages would be like the admin
login page you know maybe some sensitive
areas maybe some moderator panels so a
lot of the private areas of the website
you want to make sure that Google
doesn't crawl those so what you can do
is you can make a special file called
robots.txt and you can upload this to
your server and usually what web
developers do is they list all the files
that they don't want Google to crawl and
then Google ignores them same thing with
Yahoo now the thing is and the cool
thing is whenever you're analyzing the
website for security issues one of the
one of the first files that you always
go to is that robots.txt file why is
that well if the developer said hey
Google don't crawl these because you
know people shouldn't be looking at them
well we can just look at it and say oh
so these are all the areas that are you
know kind of sensitive so it's pretty
cool and yeah what we need to do is we
actually need to use a new package to
download this file and store the results
so let me make a new Python file and
I'll just name it uh robots underscore
text dot py
all right so I'm sure you guys used URL
Lib before so all this does is it allows
you to basically make a request to a URL
like a get request then I don't know
simple enough so basically download
files from the internet now we also need
to import package called IO and this is
just for encoding so we can make sure
that we have you know we're getting our
data in a readable format that we can
work with so robots.txt so what we're
gonna do is we're gonna pass in a URL
and here's the thing so this is actually
code I just want to show you guys
something so we're gonna pass in a URL
like this reddit comm and the robots.txt
file is on the top domain so robots.txt
now the trouble with this is whenever
your users are using this tool I don't
know if they're gonna format their URL
like this or they're gonna format it
like this so some people have that
little forward slash at the end some
people don't so I just want to check for
that first so you know this has nothing
to do with the tool just kind of user
experience if you will so just write if
URL that they pass then ends with a
forward slash then what we want to do is
we want to take that path and just keep
it as the URL all right so if it ends
with a forward slash then we're just
going to use that with a forward slash
at the end else if it doesn't end with
the forward slash then we're just gonna
add one to the end so by the time we get
done with this they're always gonna have
four slash at the end nice and
consistent so URL plus the forward slash
so again if it already ends with the
ford slash then just use that URL else
if it doesn't then we're going to tack
on the URL simple enough easy wheezy
alright so now we have to do is we
actually have to make the request to
that file so what we're gonna do is
we're gonna request a file from the
net so you are a lip dot request and now
it says okay what file are you
requesting I need the URL of it well
it's just path right here plus sure you
need to surround this in robots dot text
and the second one I just read data
equal to none and here I actually need
to write URL open all right so this is
the function URL open so it's pretty
much gonna open this file right here
which is going to be reddit calm slash
robots that text the New Boston comp
robots that text ebay.com slash robots
that text or whatever and it's going to
store the request or the results right
in here and now we just need to make
sure that our data is encoded properly
so you know we can work with it in
Python without a freaking out so IO text
IO wrapper so this is our request and we
just want to set the encoding to utf-8
standard encoding and now let's just
return whatever those results were so
data dot read format this and there you
go so again all we're doing is we're
gonna pass in URL some website we're
gonna get the robots that text file and
then we're gonna return the results the
data now just so we can prove that this
works we print get robot stuff text and
we'll do it for reddit calm and let me
run that right there all right so this
is read its thought reddit calms robot
text so there you go looking beautiful
and you guys can play around of this
throw in a few more websites but I done
with that test right there art so we got
the robots the text file we got the IP
address we did an maps
looking good this tool is looking sweet
and in the next video I'm going to show
you how to get the who is of a website
see you next time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>